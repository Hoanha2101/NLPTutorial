{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGEX - REGular EXpression\n",
    "\n",
    "Link youtube gi·ªõi thi·ªáu v√† h∆∞·ªõng d·∫´n kh√° hay v·ªÅ l√≠ thuy·∫øt: https://www.youtube.com/watch?v=YGpWSC8Kxvk&list=LL&index=7&t=1s\n",
    "\n",
    "Link trang web th·ª±c h√†nh: https://regex101.com/\n",
    "\n",
    "Trang web python:\n",
    "https://www.geeksforgeeks.org/regular-expression-python-examples/\n",
    "\n",
    "### T√≥m t·∫Øt\n",
    "\n",
    "Regex bao g·ªìm 2 ph·∫ßn ch√≠nh:\n",
    "\n",
    "/ab + c/g      (- NgƒÉn c√°ch gi·ªØa 2 ph·∫ßn c·ªßa regex l√† 2 k√≠ t·ª± g·∫°ch ch√©o)\n",
    "\n",
    "Trong ƒë√≥: \n",
    "+ ab + c: pattern\n",
    "+ g: flag\n",
    "\n",
    "    V·ªõi flag:\n",
    "    \n",
    "        . /g: Cho ph√©p t√¨m ki·∫øm ·ªü ph·∫°m vi to√†n c·ª•c (global search)\n",
    "        . /i: cho ph√©p t√¨m ki·∫øm m√† kh√¥ng c·∫ßn ph√¢n bi·ªát ch·ªØ hoa hay ch·ªØ th∆∞·ªùng. (case-insensitive search)\n",
    "        . /m: cho ph√©p t√¨m ki·∫øm tr√™n nhi·ªÅu d√≤ng (multi-line search)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ngu·ªìn c·ªßa c√°c b√†i c·ªßa Regex v√≠ d·ª•:** https://www.geeksforgeeks.org/regular-expression-python-examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(34, 40), match='portal'>\n",
      "Start Index: 34\n",
      "End Index: 40\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "s = 'GeeksforGeeks: A computer science portal for geeks'\n",
    "\n",
    "# t√¨m ki·∫øm t·ª´ \"portal\"\n",
    "\n",
    "match = re.search(r'portal', s) \n",
    "print(match)\n",
    "print('Start Index:', match.start()) \n",
    "print('End Index:', match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='g'>\n",
      "<re.Match object; span=(5, 6), match='.'>\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "  \n",
    "s = 'geeks.forgeeks'\n",
    "  \n",
    "# without using \\ \n",
    "match = re.search(r'.', s) \n",
    "print(match)\n",
    "\n",
    "# using \\ \n",
    "match = re.search(r'\\.', s) \n",
    "print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  [] ‚Äì D·∫•u ngo·∫∑c vu√¥ng\n",
    "\n",
    "+ D·∫•u ngo·∫∑c vu√¥ng ([]) ƒë·∫°i di·ªán cho m·ªôt l·ªõp k√Ω t·ª± bao g·ªìm m·ªôt t·∫≠p h·ª£p c√°c k√Ω t·ª± m√† ch√∫ng ta mu·ªën kh·ªõp. V√≠ d·ª•: l·ªõp k√Ω t·ª± [abc] s·∫Ω kh·ªõp v·ªõi b·∫•t k·ª≥ a, b ho·∫∑c c n√†o. \n",
    "\n",
    "+ Ch√∫ng ta c≈©ng c√≥ th·ªÉ ch·ªâ ƒë·ªãnh m·ªôt ph·∫°m vi k√Ω t·ª± b·∫±ng c√°ch s·ª≠ d·ª•ng ‚Äì b√™n trong d·∫•u ngo·∫∑c vu√¥ng. V√≠ d·ª•, \n",
    "\n",
    "[0, 3] l√† m·∫´u d∆∞·ªõi d·∫°ng [0123]\n",
    "[ac] gi·ªëng nh∆∞ [abc]\n",
    "+ Ch√∫ng ta c≈©ng c√≥ th·ªÉ ƒë·∫£o ng∆∞·ª£c l·ªõp k√Ω t·ª± b·∫±ng c√°ch s·ª≠ d·ª•ng k√Ω hi·ªáu d·∫•u m≈©(^). V√≠ d·ª•, \n",
    "\n",
    "[^0-3] c√≥ nghƒ©a l√† b·∫•t k·ª≥ s·ªë n√†o ngo·∫°i tr·ª´ 0, 1, 2 ho·∫∑c 3\n",
    "[^ac] c√≥ nghƒ©a l√† b·∫•t k·ª≥ k√Ω t·ª± n√†o ngo·∫°i tr·ª´ a, b ho·∫∑c c\n",
    "+ V√≠ d·ª•:\n",
    "    Trong m√£ n√†y, b·∫°n ƒëang s·ª≠ d·ª•ng bi·ªÉu th·ª©c ch√≠nh quy ƒë·ªÉ t√¨m t·∫•t c·∫£ c√°c k√Ω t·ª± trong chu·ªói n·∫±m trong ph·∫°m vi t·ª´ 'a' ƒë·∫øn 'm'. H√†m re.findall()tr·∫£ v·ªÅ danh s√°ch t·∫•t c·∫£ c√°c k√Ω t·ª± nh∆∞ v·∫≠y. Trong chu·ªói ƒë√£ cho, c√°c k√Ω t·ª± kh·ªõp v·ªõi m·∫´u n√†y l√†: 'c', 'k', 'b', 'f', 'j', 'e', ‚Äã‚Äã'h', 'l', 'd', ' g'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(1, 2), match='h'>\n",
      "\n",
      "['h', 'e', 'i', 'c', 'k', 'b', 'f', 'j', 'm', 'e', 'h', 'e', 'l', 'a', 'd', 'g']\n",
      "['T', ' ', 'q', 'u', ' ', 'r', 'o', 'w', 'n', ' ', 'o', 'x', ' ', 'u', 'p', 's', ' ', 'o', 'v', 'r', ' ', 't', ' ', 'z', 'y', ' ', 'o']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "  \n",
    "string = \"The quick brown fox jumps over the lazy dog\"\n",
    "pattern = \"[a-m]\"\n",
    "result = re.search(pattern, string) # search: trong tr∆∞·ªùng h·ª£p n√†y s·∫Ω tr·∫£ l·∫°i index k·∫øt qu·∫£ t√¨m ki·∫øm c·ªßa k√≠ t·ª± ƒë·∫ßu ti√™n c·ªßa k·∫øt qu·∫£ findall\n",
    "print(result)\n",
    "print()\n",
    "result = re.findall(pattern, string) # tr·∫£ ra t·∫•t c·∫£ c√°c k√≠ t·ª± n·∫±m trong kho·∫£ng [a - m]\n",
    "print(result)\n",
    "\n",
    "pattern_sub= \"[^a-m]\" # D√πng k√≠ t·ª± ^ trong pattern\n",
    "result = re.findall(pattern_sub, string) # tr·∫£ ra k·∫øt qu·∫£ n·∫±m ngo√†i [a-m]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ^ ‚Äì D·∫•u m≈©\n",
    "Bi·ªÉu t∆∞·ª£ng d·∫•u m≈© (^) kh·ªõp v·ªõi ph·∫ßn ƒë·∫ßu c·ªßa chu·ªói, t·ª©c l√† ki·ªÉm tra xem chu·ªói c√≥ b·∫Øt ƒë·∫ßu b·∫±ng (c√°c) k√Ω t·ª± ƒë√£ cho hay kh√¥ng. V√≠ d·ª• -  \n",
    "\n",
    "+ ^g s·∫Ω ki·ªÉm tra xem chu·ªói c√≥ b·∫Øt ƒë·∫ßu b·∫±ng g hay kh√¥ng, ch·∫≥ng h·∫°n nh∆∞ geeks, Globe, girl, g, v.v.\n",
    "\n",
    "+ ^ge s·∫Ω ki·ªÉm tra xem chu·ªói c√≥ b·∫Øt ƒë·∫ßu b·∫±ng ge hay kh√¥ng, ch·∫≥ng h·∫°n nh∆∞ geeks, geeksforgeeks, v.v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: The quick brown fox\n",
      "Matched: The lazy dog\n",
      "Not matched: A quick brown fox\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "regex = r'^The'\n",
    "strings = ['The quick brown fox', 'The lazy dog', 'A quick brown fox'] \n",
    "for string in strings: \n",
    "    if re.match(regex, string):  # S·ª≠ d·ª•ng match trong re ƒë·ªÉ th·ª±c hi·ªán vi·ªác so kh·ªõp\n",
    "        print(f'Matched: {string}') \n",
    "    else: \n",
    "        print(f'Not matched: {string}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ ‚Äì ƒê√¥ la\n",
    "K√Ω hi·ªáu Dollar($) kh·ªõp v·ªõi ph·∫ßn cu·ªëi c·ªßa chu·ªói, t·ª©c l√† ki·ªÉm tra xem chu·ªói c√≥ k·∫øt th√∫c b·∫±ng (c√°c) k√Ω t·ª± ƒë√£ cho hay kh√¥ng. V√≠ d·ª•-\n",
    "\n",
    "+ s$ s·∫Ω ki·ªÉm tra chu·ªói k·∫øt th√∫c b·∫±ng a ch·∫≥ng h·∫°n nh∆∞ computer geek, end, s, v.v.\n",
    "\n",
    "+ ks$ s·∫Ω ki·ªÉm tra chu·ªói k·∫øt th√∫c b·∫±ng ks nh∆∞ geeks, geeksforgeeks, ks, v.v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(6, 12), match='World!'>\n",
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "# Kh√¥ng d√πng match cho $, d√πng search\n",
    "\n",
    "string = \"Hello World!\"\n",
    "pattern = r\"World!$\"\n",
    "  \n",
    "match = re.search(pattern, string) \n",
    "print(match)\n",
    "if match: \n",
    "    print(\"Match found!\") \n",
    "else: \n",
    "    print(\"Match not found.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## . ‚Äì Ch·∫•m\n",
    "\n",
    "Bi·ªÉu t∆∞·ª£ng d·∫•u ch·∫•m(.) ch·ªâ kh·ªõp v·ªõi m·ªôt k√Ω t·ª± duy nh·∫•t ngo·∫°i tr·ª´ k√Ω t·ª± d√≤ng m·ªõi (\\n). V√≠ d·ª• -  \n",
    "\n",
    "+ ab s·∫Ω ki·ªÉm tra chu·ªói c√≥ ch·ª©a b·∫•t k·ª≥ k√Ω t·ª± n√†o ·ªü v·ªã tr√≠ d·∫•u ch·∫•m nh∆∞ acb, acbd, abbb, v.v.\n",
    "\n",
    "+ .. s·∫Ω ki·ªÉm tra xem chu·ªói c√≥ ch·ª©a √≠t nh·∫•t 2 k√Ω t·ª± kh√¥ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "  \n",
    "string = \"The quick brown fox jumps over the lazy dog.\"\n",
    "pattern = r\"brown.fox\"\n",
    "\n",
    "# Gi·∫£i th√≠ch: \n",
    "\"\"\"\n",
    "    ...brown fox...\n",
    "    ...brown0fox...\n",
    "    ...brown-fox...\n",
    "        ...\n",
    "    \n",
    "    Bi·ªÉu th·ª©c pattern = r\"brown.fox\" c√≥ d·∫•u ch·∫•m, t·ª©c n√≥ ch·∫•p nh·∫≠n b·∫•t k√¨\n",
    "                            l√≠ t·ª± n√†o n·∫±m gi·ªØ brown v√† fox ngo·∫°i t·ª´ k√≠ t·ª± xu·ªëng h√†ng /n (d·∫•u xu·ªëng d√≤ng)\n",
    "\"\"\"\n",
    "\n",
    "match = re.search(pattern, string) \n",
    "if match: \n",
    "    print(\"Match found!\") \n",
    "else: \n",
    "    print(\"Match not found.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## | - Ho·∫∑c\n",
    "\n",
    "K√Ω hi·ªáu ho·∫∑c ho·∫°t ƒë·ªông nh∆∞ to√°n t·ª≠ ho·∫∑c nghƒ©a l√† n√≥ ki·ªÉm tra xem m·∫´u tr∆∞·ªõc hay sau k√Ω hi·ªáu ho·∫∑c c√≥ trong chu·ªói hay kh√¥ng. V√≠ d·ª• -  \n",
    "\n",
    "+ a|b s·∫Ω kh·ªõp v·ªõi b·∫•t k·ª≥ chu·ªói n√†o ch·ª©a a ho·∫∑c b, ch·∫≥ng h·∫°n nh∆∞ acd, bcd, abcd, v.v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "  \n",
    "string = \"The quick brown fox jumps over the lazy dog.\"\n",
    "pattern = r\"brown|mmmm\" \n",
    "\n",
    "match = re.search(pattern, string) \n",
    "if match: \n",
    "    print(\"Match found!\") \n",
    "else: \n",
    "    print(\"Match not found.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ? - D·∫•u ch·∫•m h·ªèi\n",
    "\n",
    "D·∫•u ch·∫•m h·ªèi (?) l√† m·ªôt b·ªô ƒë·ªãnh l∆∞·ª£ng trong c√°c bi·ªÉu th·ª©c ch√≠nh quy cho bi·∫øt ph·∫ßn t·ª≠ tr∆∞·ªõc ƒë√≥ ph·∫£i ƒë∆∞·ª£c so kh·ªõp b·∫±ng 0 ho·∫∑c m·ªôt l·∫ßn. N√≥ cho ph√©p b·∫°n ch·ªâ ƒë·ªãnh r·∫±ng ph·∫ßn t·ª≠ n√†y l√† t√πy ch·ªçn, nghƒ©a l√† n√≥ c√≥ th·ªÉ x·∫£y ra m·ªôt l·∫ßn ho·∫∑c kh√¥ng x·∫£y ra l·∫ßn n√†o. V√≠ d·ª•,\n",
    "\n",
    "+ ab?c s·∫Ω kh·ªõp v·ªõi chu·ªói ac, acb, dabc nh∆∞ng s·∫Ω kh√¥ng kh·ªõp v·ªõi chu·ªói abbc v√¨ c√≥ hai chu·ªói b. T∆∞∆°ng t·ª±, n√≥ s·∫Ω kh√¥ng kh·ªõp v·ªõi abdc v√¨ b kh√¥ng ƒë∆∞·ª£c theo sau b·ªüi c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match not found.\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "string = \"The quick brown fox jumps over the lazy dog.\"\n",
    "pattern = r\"br?wn\" \n",
    "\n",
    "match = re.search(pattern, string) \n",
    "\n",
    "if match: \n",
    "    print(\"Match found!\") \n",
    "else: \n",
    "    print(\"Match not found.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gi·∫£i th√≠ch nguy√™n nh√¢n kh√¥ng t√¨m th·∫•y:\n",
    "\n",
    "### Ph√¢n t√≠ch m·∫´u regex r\"br?wn\"\n",
    "    b: K√Ω t·ª± b.\n",
    "    r?: K√Ω t·ª± r l√† t√πy ch·ªçn (c√≥ th·ªÉ xu·∫•t hi·ªán ho·∫∑c kh√¥ng xu·∫•t hi·ªán).\n",
    "    wn: K√Ω t·ª± w ti·∫øp theo l√† n.\n",
    "\n",
    "### C√°ch regex ho·∫°t ƒë·ªông\n",
    "+ K√Ω t·ª± ? trong regex c√≥ nghƒ©a l√† k√Ω t·ª± tr∆∞·ªõc n√≥ (trong tr∆∞·ªùng h·ª£p n√†y l√† r) l√† t√πy ch·ªçn, nghƒ©a l√† c√≥ th·ªÉ xu·∫•t hi·ªán ho·∫∑c kh√¥ng xu·∫•t hi·ªán. V√¨ v·∫≠y, m·∫´u r\"br?wn\" s·∫Ω kh·ªõp v·ªõi hai m·∫´u con:\n",
    "\n",
    "    bwn (n·∫øu r kh√¥ng xu·∫•t hi·ªán)\n",
    "    brwn (n·∫øu r xu·∫•t hi·ªán)\n",
    "\n",
    "### Chu·ªói \"The quick brown fox jumps over the lazy dog.\"\n",
    "+ Trong chu·ªói n√†y, ch√∫ng ta c√≥ t·ª´ \"brown\", nh∆∞ng t·ª´ n√†y kh√¥ng kh·ªõp v·ªõi m·∫´u r\"br?wn\":\n",
    "\n",
    "    brown: C√≥ r, kh√¥ng kh·ªõp v·ªõi bwn\n",
    "    brown: Kh·ªõp v·ªõi brwn, nh∆∞ng ch√∫ng ta ƒëang ki·ªÉm tra s·ª± xu·∫•t hi·ªán c·ªßa r.\n",
    "\n",
    ">=> Nh∆∞ v·∫≠y, t·ª´ \"brown\" kh√¥ng kh·ªõp v·ªõi m·∫´u r\"br?wn\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "string = \"The quick brown fox jumps over the lazy dog.\"\n",
    "pattern = r\"r?w\" # c√≥ r ho·∫∑c kh√¥ng c√≥ r\n",
    "\n",
    "match = re.search(pattern, string) \n",
    "\n",
    "if match: \n",
    "    print(\"Match found!\") \n",
    "else: \n",
    "    print(\"Match not found.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * ‚Äì Ng√¥i sao\n",
    "\n",
    "Bi·ªÉu t∆∞·ª£ng d·∫•u sao (*) kh·ªõp v·ªõi 0 ho·∫∑c nhi·ªÅu l·∫ßn xu·∫•t hi·ªán c·ªßa bi·ªÉu th·ª©c ch√≠nh quy tr∆∞·ªõc k√Ω hi·ªáu *. V√≠ d·ª• -  \n",
    "\n",
    "+ ab*c s·∫Ω ƒë∆∞·ª£c so kh·ªõp v·ªõi chu·ªói ac, abc, abbbc, dabc, v.v. nh∆∞ng s·∫Ω kh√¥ng kh·ªõp v·ªõi chu·ªói abdc v√¨ b kh√¥ng ƒë∆∞·ª£c theo sau b·ªüi c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "string = \"The quick brown fox jumps over the lazy dog.\"\n",
    "pattern = r\"r*wn\" # s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa r l√† 0 ho·∫∑c l·ªõn h∆°n 0 v√† theo sau b·ªüi wn\n",
    "\n",
    "match = re.search(pattern, string) \n",
    "\n",
    "if match: \n",
    "    print(\"Match found!\") \n",
    "else: \n",
    "    print(\"Match not found.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + ‚Äì C·ªông\n",
    "Bi·ªÉu t∆∞·ª£ng d·∫•u c·ªông (+) kh·ªõp v·ªõi m·ªôt ho·∫∑c nhi·ªÅu l·∫ßn xu·∫•t hi·ªán c·ªßa bi·ªÉu th·ª©c ch√≠nh quy tr∆∞·ªõc bi·ªÉu t∆∞·ª£ng +. V√≠ d·ª• -  \n",
    "\n",
    "+ ab+c s·∫Ω kh·ªõp v·ªõi chu·ªói abc, abbc, dabc, nh∆∞ng s·∫Ω kh√¥ng kh·ªõp v·ªõi chu·ªói ac, abdc, v√¨ kh√¥ng c√≥ b trong ac v√† b, kh√¥ng c√≥ c trong abdc theo sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "string = \"The quick brown fox jummmmmps over the lazy dog.\"\n",
    "pattern = r\"um+p\" # s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa r l√† 0 ho·∫∑c l·ªõn h∆°n 0 v√† theo sau b·ªüi wn\n",
    "\n",
    "match = re.search(pattern, string) \n",
    "\n",
    "if match: \n",
    "    print(\"Match found!\") \n",
    "else: \n",
    "    print(\"Match not found.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## {m, n} ‚Äì Braces\n",
    "\n",
    "C√°c d·∫•u ngo·∫∑c nh·ªçn kh·ªõp v·ªõi m·ªçi l·∫ßn l·∫∑p l·∫°i tr∆∞·ªõc bi·ªÉu th·ª©c ch√≠nh quy t·ª´ m ƒë·∫øn n, bao g·ªìm c·∫£ hai. V√≠ d·ª• -  \n",
    "\n",
    "+ a{2, 4} s·∫Ω kh·ªõp v·ªõi chu·ªói aaab, baaaac, gaad, nh∆∞ng s·∫Ω kh√¥ng kh·ªõp v·ªõi c√°c chu·ªói nh∆∞ abc, bc v√¨ ch·ªâ c√≥ m·ªôt a ho·∫∑c kh√¥ng c√≥ a trong c·∫£ hai tr∆∞·ªùng h·ª£p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(1, 9), match='oooooooo'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "string = \"goooooooogle in Vietnam\"\n",
    "\n",
    "pattern = r\"o{2,10}\"\n",
    "\n",
    "FIND = re.search(pattern, string)\n",
    "print(FIND)\n",
    "\n",
    "pattern = r\"V{2,10}\"\n",
    "\n",
    "FIND = re.search(pattern, string)\n",
    "print(FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (\\<regex>) ‚Äì Nh√≥m\n",
    "K√Ω hi·ªáu nh√≥m ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ nh√≥m c√°c m·∫´u ph·ª•. V√≠ d·ª• -  \n",
    "\n",
    "+ (a|b)cd s·∫Ω kh·ªõp v·ªõi c√°c chu·ªói nh∆∞ acd, abcd, gacd, v.v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(16, 19), match='Vie'>\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "string = \"goooooooogle in Vietnam\"\n",
    "\n",
    "pattern = r\"(V|W)ie\"\n",
    "\n",
    "FIND = re.search(pattern, string)\n",
    "print(FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\A - Matches if the string begins with the given character\n",
    "Kh·ªõp n·∫øu chu·ªói b·∫Øt ƒë·∫ßu b·∫±ng k√Ω t·ª± ƒë√£ cho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n",
      "Match not found.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"\\Afor\"\n",
    "text1 = \"for geeks\"\n",
    "text2 = \"geeks for\"\n",
    "\n",
    "match1 = re.search(pattern, text1)\n",
    "match2 = re.search(pattern, text2)\n",
    "\n",
    "print(\"Match found!\" if match1 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match2 else \"Match not found.\")  # Match not found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\b - Matches if the word begins or ends with the given character\n",
    "\n",
    "Kh·ªõp n·∫øu t·ª´ b·∫Øt ƒë·∫ßu ho·∫∑c k·∫øt th√∫c b·∫±ng k√Ω t·ª± ƒë√£ cho. \n",
    "+ \\b(string) s·∫Ω ki·ªÉm tra ph·∫ßn ƒë·∫ßu c·ªßa t·ª´.\n",
    "+ (string)\\b s·∫Ω ki·ªÉm tra ph·∫ßn cu·ªëi c·ªßa t·ª´."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 2), match='ge'>\n",
      "<re.Match object; span=(0, 2), match='ge'>\n",
      "<re.Match object; span=(3, 5), match='ge'>\n",
      "Match found!\n",
      "Match found!\n",
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern1 = r\"\\bge\"\n",
    "pattern2 = r\"ge\\b\"\n",
    "text1 = \"geeks\"\n",
    "text2 = \"get\"\n",
    "text3 = \"forge\"\n",
    "\n",
    "match1 = re.search(pattern1, text1)\n",
    "print(match1)\n",
    "match2 = re.search(pattern1, text2)\n",
    "print(match2)\n",
    "match3 = re.search(pattern2, text3)\n",
    "print(match3)\n",
    "\n",
    "print(\"Match found!\" if match1 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match2 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match3 else \"Match not found.\")  # Match found!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\B - Opposite of \\b, should not start or end with the given regex\n",
    "N√≥ ng∆∞·ª£c l·∫°i v·ªõi \\b t·ª©c l√† chu·ªói kh√¥ng ƒë∆∞·ª£c b·∫Øt ƒë·∫ßu ho·∫∑c k·∫øt th√∫c b·∫±ng bi·ªÉu th·ª©c ch√≠nh quy ƒë√£ cho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n",
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"\\Bge\"\n",
    "text1 = \"together\"\n",
    "text2 = \"forge\"\n",
    "\n",
    "match1 = re.search(pattern, text1)\n",
    "match2 = re.search(pattern, text2)\n",
    "\n",
    "print(\"Match found!\" if match1 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match2 else \"Match not found.\")  # Match not found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\d - Matches any decimal digit\n",
    "Kh·ªõp v·ªõi b·∫•t k·ª≥ ch·ªØ s·ªë th·∫≠p ph√¢n n√†o, ƒëi·ªÅu n√†y t∆∞∆°ng ƒë∆∞∆°ng v·ªõi l·ªõp ƒë√£ ƒë·∫∑t [0-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n",
      "Match found!\n",
      "Match not found.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"\\d\"\n",
    "text1 = \"123\"\n",
    "text2 = \"gee1\"\n",
    "text3 = \"gee\"\n",
    "\n",
    "match1 = re.search(pattern, text1)\n",
    "match2 = re.search(pattern, text2)\n",
    "match3 = re.search(pattern, text3)\n",
    "\n",
    "print(\"Match found!\" if match1 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match2 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match3 else \"Match not found.\")  # Match not found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\D - Matches any non-digit character\n",
    "Kh·ªõp v·ªõi b·∫•t k·ª≥ k√Ω t·ª± kh√¥ng c√≥ ch·ªØ s·ªë n√†o, ƒëi·ªÅu n√†y t∆∞∆°ng ƒë∆∞∆°ng v·ªõi l·ªõp ƒë√£ ƒë·∫∑t [^0-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n",
      "Match found!\n",
      "Match not found.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"\\D\"\n",
    "text1 = \"geeks\"\n",
    "text2 = \"geek1131\"\n",
    "text3 = \"1131\"\n",
    "\n",
    "match1 = re.search(pattern, text1)\n",
    "match2 = re.search(pattern, text2)\n",
    "match3 = re.search(pattern, text3)\n",
    "\n",
    "print(\"Match found!\" if match1 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match2 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match3 else \"Match not found.\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\s - Matches any whitespace character\n",
    "Kh·ªõp v·ªõi b·∫•t k·ª≥ k√Ω t·ª± kho·∫£ng tr·∫Øng n√†o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n",
      "Match found!\n",
      "Match not found.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"\\s\"\n",
    "text1 = \"gee ks\"\n",
    "text2 = \"a bc a\"\n",
    "text3 = \"msmsmdm\"\n",
    "\n",
    "match1 = re.search(pattern, text1)\n",
    "match2 = re.search(pattern, text2)\n",
    "match3 = re.search(pattern, text3)\n",
    "\n",
    "print(\"Match found!\" if match1 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match2 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match3 else \"Match not found.\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\S - Matches any non-whitespace character\n",
    "Kh·ªõp v·ªõi b·∫•t k·ª≥ k√Ω t·ª± kh√¥ng ph·∫£i kho·∫£ng tr·∫Øng n√†o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n",
      "Match found!\n",
      "Match not found.\n",
      "Match not found.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"\\S\"\n",
    "text1 = \"a bd\"\n",
    "text2 = \"abcd\"\n",
    "text3 = \"\"\n",
    "text4 = \" \"\n",
    "\n",
    "match1 = re.search(pattern, text1)\n",
    "match2 = re.search(pattern, text2)\n",
    "match3 = re.search(pattern, text3)\n",
    "match4 = re.search(pattern, text4)\n",
    "\n",
    "print(\"Match found!\" if match1 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match2 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match3 else \"Match not found.\")\n",
    "print(\"Match found!\" if match4 else \"Match not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\w - Matches any alphanumeric character\n",
    "Kh·ªõp v·ªõi b·∫•t k·ª≥ k√Ω t·ª± ch·ªØ v√† s·ªë n√†o, k√Ω t·ª± n√†y t∆∞∆°ng ƒë∆∞∆°ng v·ªõi l·ªõp [a-zA-Z0-9_].\n",
    "\n",
    "T·ª©c l√† chu·ªói c√≥ th·ªÉ bao g·ªìm `ch·ªØ th∆∞·ªùng`, `ch·ªØ hoa`, v√† `s·ªë`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n",
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"\\w\"\n",
    "text1 = \"123\"\n",
    "text2 = \"geeKs4\"\n",
    "\n",
    "match1 = re.search(pattern, text1)\n",
    "match2 = re.search(pattern, text2)\n",
    "\n",
    "print(\"Match found!\" if match1 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match2 else \"Match not found.\")  # Match found!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\W - Matches any non-alphanumeric character\n",
    "Kh·ªõp v·ªõi b·∫•t k·ª≥ k√Ω t·ª± kh√¥ng ph·∫£i ch·ªØ v√† s·ªë n√†o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n",
      "Match found!\n",
      "Match not found.\n",
      "Match not found.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"\\W\"\n",
    "text1 = \">$\"\n",
    "text2 = \"gee<>\"\n",
    "text3 = \"123\"\n",
    "text4 = \"gee\"\n",
    "\n",
    "match1 = re.search(pattern, text1)\n",
    "match2 = re.search(pattern, text2)\n",
    "match3 = re.search(pattern, text3)\n",
    "match4 = re.search(pattern, text4)\n",
    "\n",
    "print(\"Match found!\" if match1 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match2 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match3 else \"Match not found.\")\n",
    "print(\"Match found!\" if match4 else \"Match not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\Z - Matches if the string ends with the given regex\n",
    "Kh·ªõp n·∫øu chu·ªói k·∫øt th√∫c b·∫±ng bi·ªÉu th·ª©c ch√≠nh quy ƒë√£ cho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n",
      "Match found!\n",
      "Match not found.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"ab\\Z\"\n",
    "text1 = \"abcdab\"\n",
    "text2 = \"abababab\"\n",
    "text3 = \"abababcm\"\n",
    "\n",
    "match1 = re.search(pattern, text1)\n",
    "match2 = re.search(pattern, text2)\n",
    "match3 = re.search(pattern, text3)\n",
    "\n",
    "print(\"Match found!\" if match1 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match2 else \"Match not found.\")  # Match not found.\n",
    "print(\"Match found!\" if match3 else \"Match not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re.findall()\n",
    "Tr·∫£ v·ªÅ t·∫•t c·∫£ c√°c m·∫´u kh·ªõp kh√¥ng tr√πng nhau trong chu·ªói d∆∞·ªõi d·∫°ng danh s√°ch c√°c chu·ªói. Chu·ªói ƒë∆∞·ª£c qu√©t t·ª´ tr√°i sang ph·∫£i v√† k·∫øt qu·∫£ kh·ªõp ƒë∆∞·ª£c tr·∫£ v·ªÅ theo th·ª© t·ª± t√¨m th·∫•y.\n",
    "\n",
    "T√¨m t·∫•t c·∫£ c√°c l·∫ßn xu·∫•t hi·ªán c·ªßa m·ªôt m·∫´u \n",
    "\n",
    "M√£ n√†y s·ª≠ d·ª•ng regular expression (\\d+) ƒë·ªÉ t√¨m t·∫•t c·∫£ c√°c chu·ªói c·ªßa m·ªôt ho·∫∑c nhi·ªÅu ch·ªØ s·ªë trong chu·ªói ƒë√£ cho. N√≥ t√¨m ki·∫øm c√°c gi√° tr·ªã s·ªë v√† l∆∞u tr·ªØ ch√∫ng trong m·ªôt danh s√°ch. Trong v√≠ d·ª• n√†y, n√≥ t√¨m v√† in c√°c s·ªë ‚Äú123456789‚Äù v√† ‚Äú987654321‚Äù t·ª´ chu·ªói ƒë·∫ßu v√†o.\\d+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['123456789', '987654321']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re \n",
    "string = \"\"\"Hello my Number is 123456789 and \n",
    "            my friend's number is 987654321\"\"\"\n",
    "regex = '\\d+'\n",
    "  \n",
    "match = re.findall(regex, string) \n",
    "print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re.compile()\n",
    "\n",
    "Bi·ªÉu th·ª©c ch√≠nh quy ƒë∆∞·ª£c bi√™n d·ªãch th√†nh c√°c ƒë·ªëi t∆∞·ª£ng m·∫´u, c√≥ c√°c ph∆∞∆°ng th·ª©c cho nhi·ªÅu thao t√°c kh√°c nhau nh∆∞ t√¨m ki·∫øm k·∫øt qu·∫£ kh·ªõp m·∫´u ho·∫∑c th·ª±c hi·ªán thay th·∫ø chu·ªói. \n",
    "\n",
    "V√≠ d·ª• 1:\n",
    "\n",
    "M√£ n√†y s·ª≠ d·ª•ng m·∫´u bi·ªÉu th·ª©c ch√≠nh quy ƒë·ªÉ t√¨m v√† li·ªát k√™ t·∫•t c·∫£ c√°c ch·ªØ c√°i vi·∫øt th∆∞·ªùng t·ª´ 'a' ƒë·∫øn 'e' trong chu·ªói ƒë·∫ßu v√†o ‚Äúƒê√∫ng v·∫≠y, √¥ng Gibenson Stark n√≥i‚Äù. ƒê·∫ßu ra s·∫Ω l√† , l√† c√°c k√Ω t·ª± ph√π h·ª£p. [a-e]['e', 'a', 'd', 'b', 'e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'a', 'd', 'b', 'e', 'a']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "p = re.compile('[a-e]') \n",
    "\n",
    "print(p.findall(\"Aye, said Mr. Gibenson Stark\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi·ªÉu ƒë·∫ßu ra: \n",
    "\n",
    "+ L·∫ßn xu·∫•t hi·ªán ƒë·∫ßu ti√™n l√† 'e' trong \"Aye\" ch·ª© kh√¥ng ph·∫£i 'A', v√¨ n√≥ ph√¢n bi·ªát ch·ªØ hoa ch·ªØ th∆∞·ªùng.\n",
    "\n",
    "+ L·∫ßn xu·∫•t hi·ªán ti·∫øp theo l√† 'a' trong \"said\", sau ƒë√≥ l√† 'd' trong \"said\", ti·∫øp theo l√† 'b' v√† 'e' trong \"Gibenson\", 'a' cu·ªëi c√πng tr√πng kh·ªõp v·ªõi \"Stark\".\n",
    "\n",
    "+ D·∫•u g·∫°ch ch√©o ng∆∞·ª£c si√™u k√Ω t·ª± '\\' c√≥ vai tr√≤ r·∫•t quan tr·ªçng v√¨ n√≥ b√°o hi·ªáu c√°c chu·ªói kh√°c nhau. N·∫øu d·∫•u g·∫°ch ch√©o ng∆∞·ª£c ƒë∆∞·ª£c s·ª≠ d·ª•ng m√† kh√¥ng c√≥ √Ω nghƒ©a ƒë·∫∑c bi·ªát nh∆∞ si√™u k√Ω t·ª±, h√£y s·ª≠ d·ª•ng'\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V√≠ d·ª• 2: ƒê·∫∑t l·ªõp [\\s,.] s·∫Ω kh·ªõp v·ªõi b·∫•t k·ª≥ k√Ω t·ª± kho·∫£ng tr·∫Øng n√†o, ',' ho·∫∑c, '.' . \n",
    "\n",
    "+ M√£ n√†y s·ª≠ d·ª•ng c√°c bi·ªÉu th·ª©c ch√≠nh quy ƒë·ªÉ t√¨m v√† li·ªát k√™ t·∫•t c·∫£ c√°c ch·ªØ s·ªë ƒë∆°n v√† chu·ªói ch·ªØ s·ªë trong chu·ªói ƒë·∫ßu v√†o ƒë√£ cho. N√≥ t√¨m th·∫•y c√°c ch·ªØ s·ªë ƒë∆°n v·ªõi v√† chu·ªói c√°c ch·ªØ s·ªë v·ªõi .\\d \\d+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '1', '4', '1', '8', '8', '6']\n",
      "['11', '4', '1886']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "p = re.compile('\\d') \n",
    "print(p.findall(\"I went to him at 11 A.M. on 4th July 1886\")) \n",
    "  \n",
    "p = re.compile('\\d+') \n",
    "print(p.findall(\"I went to him at 11 A.M. on 4th July 1886\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V√≠ d·ª• 3:\n",
    "\n",
    "M√£ n√†y s·ª≠ d·ª•ng c√°c bi·ªÉu th·ª©c ch√≠nh quy ƒë·ªÉ t√¨m v√† li·ªát k√™ c√°c k√Ω t·ª± t·ª´, chu·ªói k√Ω t·ª± t·ª´ v√† k√Ω t·ª± kh√¥ng ph·∫£i t·ª´ trong chu·ªói ƒë·∫ßu v√†o. N√≥ cung c·∫•p danh s√°ch c√°c k√Ω t·ª± ho·∫∑c chu·ªói ph√π h·ª£p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 's', 'a', 'i', 'd', 'i', 'n', 's', 'o', 'm', 'e', '_', 'l', 'a', 'n', 'g']\n",
      "\n",
      "['I', 'went', 'to', 'him', 'at', '11', 'A', 'M', 'he', 'said', 'in', 'some_language']\n",
      "\n",
      "[' ', ' ', '*', '*', '*', ' ', ' ', '.']\n",
      "\n",
      "[' ', ' *** ', ' ', '.']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "  \n",
    "p = re.compile('\\w') \n",
    "print(p.findall(\"He said * in some_lang.\")) \n",
    "\n",
    "print()\n",
    "p = re.compile('\\w+') \n",
    "print(p.findall(\"I went to him at 11 A.M., he said *** in some_language.\")) \n",
    "\n",
    "print()\n",
    "p = re.compile('\\W') \n",
    "print(p.findall(\"he said *** in some_language.\")) \n",
    "\n",
    "print()\n",
    "p = re.compile('\\W+') \n",
    "print(p.findall(\"he said *** in some_language.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V√≠ d·ª• 4:\n",
    "\n",
    "M√£ n√†y s·ª≠ d·ª•ng m·∫´u bi·ªÉu th·ª©c ch√≠nh quy 'ab*' ƒë·ªÉ t√¨m v√† li·ªát k√™ t·∫•t c·∫£ c√°c l·∫ßn xu·∫•t hi·ªán c·ªßa 'ab', theo sau l√† 0 ho·∫∑c nhi·ªÅu k√Ω t·ª± 'b' trong chu·ªói ƒë·∫ßu v√†o ‚Äúababbaabbb‚Äù. N√≥ tr·∫£ v·ªÅ danh s√°ch tr√πng kh·ªõp sau: ['ab', 'abb', 'abbb']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'abb', 'a', 'abbb']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nHi·ªÉu ƒë·∫ßu ra: \\n\\nRE c·ªßa ch√∫ng t√¥i l√† ab*, m√† 'a' ƒëi k√®m v·ªõi b·∫•t k·ª≥ s·ªë n√†o. c·ªßa 'b', b·∫Øt ƒë·∫ßu t·ª´ 0.\\nƒê·∫ßu ra 'ab', h·ª£p l·ªá v√¨ c√≥ m·ªôt 'a' ƒëi k√®m v·ªõi m·ªôt 'b'.\\nƒê·∫ßu ra 'abb', h·ª£p l·ªá v√¨ c√≥ m·ªôt 'a' k√®m theo 2 'b'.\\nƒê·∫ßu ra 'a', h·ª£p l·ªá v√¨ c√≥ m·ªôt 'a' k√®m theo 0 'b'.\\nƒê·∫ßu ra 'abbb', h·ª£p l·ªá v√¨ c√≥ m·ªôt 'a' k√®m theo 3 '\\n\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "p = re.compile('ab*') \n",
    "print(p.findall(\"ababbaabbb\")) \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Hi·ªÉu ƒë·∫ßu ra: \n",
    "\n",
    "RE c·ªßa ch√∫ng t√¥i l√† ab*, m√† 'a' ƒëi k√®m v·ªõi b·∫•t k·ª≥ s·ªë n√†o. c·ªßa 'b', b·∫Øt ƒë·∫ßu t·ª´ 0.\n",
    "ƒê·∫ßu ra 'ab', h·ª£p l·ªá v√¨ c√≥ m·ªôt 'a' ƒëi k√®m v·ªõi m·ªôt 'b'.\n",
    "ƒê·∫ßu ra 'abb', h·ª£p l·ªá v√¨ c√≥ m·ªôt 'a' k√®m theo 2 'b'.\n",
    "ƒê·∫ßu ra 'a', h·ª£p l·ªá v√¨ c√≥ m·ªôt 'a' k√®m theo 0 'b'.\n",
    "ƒê·∫ßu ra 'abbb', h·ª£p l·ªá v√¨ c√≥ m·ªôt 'a' k√®m theo 3 '\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re.split() \n",
    "Chia chu·ªói theo s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa m·ªôt k√Ω t·ª± ho·∫∑c m·∫´u, khi t√¨m th·∫•y m·∫´u ƒë√≥, c√°c k√Ω t·ª± c√≤n l·∫°i trong chu·ªói s·∫Ω ƒë∆∞·ª£c tr·∫£ v·ªÅ nh∆∞ m·ªôt ph·∫ßn c·ªßa danh s√°ch k·∫øt qu·∫£. \n",
    "\n",
    "+ re.split(m·∫´u, chu·ªói, maxsplit=0, flags=0)\n",
    "\n",
    "+ Tham s·ªë ƒë·∫ßu ti√™n, m·∫´u bi·ªÉu th·ªã bi·ªÉu th·ª©c ch√≠nh quy, chu·ªói l√† chu·ªói ƒë√£ cho trong ƒë√≥ m·∫´u s·∫Ω ƒë∆∞·ª£c t√¨m ki·∫øm v√† trong ƒë√≥ x·∫£y ra s·ª± ph√¢n t√°ch, maxsplit n·∫øu kh√¥ng ƒë∆∞·ª£c cung c·∫•p s·∫Ω ƒë∆∞·ª£c coi l√† b·∫±ng 0 '0' v√† n·∫øu b·∫•t k·ª≥ gi√° tr·ªã n√†o kh√°c 0 ƒë∆∞·ª£c cung c·∫•p th√¨ nhi·ªÅu nh·∫•t l√† c√≥ nhi·ªÅu s·ª± chia r·∫Ω x·∫£y ra. N·∫øu maxsplit = 1 th√¨ chu·ªói s·∫Ω ch·ªâ ph√¢n t√°ch m·ªôt l·∫ßn, d·∫´n ƒë·∫øn danh s√°ch c√≥ ƒë·ªô d√†i 2. C√°c c·ªù r·∫•t h·ªØu √≠ch v√† c√≥ th·ªÉ gi√∫p r√∫t ng·∫Øn m√£, ch√∫ng kh√¥ng ph·∫£i l√† tham s·ªë c·∫ßn thi·∫øt, v√≠ d·ª•: flags = re.IGNORECASE, trong ph·∫ßn ph√¢n chia n√†y , tr∆∞·ªùng h·ª£p, t·ª©c l√† ch·ªØ th∆∞·ªùng ho·∫∑c ch·ªØ hoa s·∫Ω b·ªã b·ªè qua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Words', 'words', 'Words']\n",
      "['Word', 's', 'words', 'Words']\n",
      "['On', '12th', 'Jan', '2016', 'at', '11', '02', 'AM']\n",
      "['On ', 'th Jan ', ', at ', ':', ' AM']\n"
     ]
    }
   ],
   "source": [
    "from re import split \n",
    "  \n",
    "print(split('\\W+', 'Words, words , Words')) \n",
    "print(split('\\W+', \"Word's words Words\")) \n",
    "print(split('\\W+', 'On 12th Jan 2016, at 11:02 AM')) \n",
    "print(split('\\d+', 'On 12th Jan 2016, at 11:02 AM')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V√≠ d·ª• 2:\n",
    "\n",
    "C√¢u l·ªánh ƒë·∫ßu ti√™n chia chu·ªói ·ªü l·∫ßn xu·∫•t hi·ªán ƒë·∫ßu ti√™n c·ªßa m·ªôt ho·∫∑c nhi·ªÅu ch·ªØ s·ªë: ['On ', 'th Jan 2016, at 11:02 AM']. th·ª© hai ph√¢n t√°ch chu·ªói b·∫±ng c√°ch s·ª≠ d·ª•ng c√°c ch·ªØ c√°i vi·∫øt th∆∞·ªùng t·ª´ a ƒë·∫øn f l√†m d·∫•u ph√¢n c√°ch, kh√¥ng ph√¢n bi·ªát ch·ªØ hoa ch·ªØ th∆∞·ªùng: . Th·ª© ba chia chu·ªói b·∫±ng c√°ch s·ª≠ d·ª•ng c√°c ch·ªØ c√°i vi·∫øt th∆∞·ªùng t·ª´ a ƒë·∫øn f l√†m d·∫•u ph√¢n c√°ch, ph√¢n bi·ªát ch·ªØ hoa ch·ªØ th∆∞·ªùng: . ['', 'y, ', 'oy oh ', 'oy, ', 'ome here']['', 'ey, Boy oh ', 'oy, ', 'ome here']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On ', 'th Jan 2016, at 11:02 AM']\n",
      "['', 'y, ', 'oy oh ', 'oy, ', 'om', ' h', 'r', '']\n",
      "['A', 'y, Boy oh ', 'oy, ', 'om', ' h', 'r', '']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "print(re.split('\\d+', 'On 12th Jan 2016, at 11:02 AM', 1)) \n",
    "print(re.split('[a-f]+', 'Aey, Boy oh boy, come here', flags=re.IGNORECASE)) \n",
    "print(re.split('[a-f]+', 'Aey, Boy oh boy, come here')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re.sub() \n",
    "‚Äòsub‚Äô trong h√†m l√† vi·∫øt t·∫Øt c·ªßa SubString, m·ªôt m·∫´u bi·ªÉu th·ª©c ch√≠nh quy nh·∫•t ƒë·ªãnh ƒë∆∞·ª£c t√¨m ki·∫øm trong chu·ªói ƒë√£ cho (tham s·ªë th·ª© 3) v√† khi t√¨m th·∫•y m·∫´u chu·ªói con ƒë∆∞·ª£c thay th·∫ø b·∫±ng repl (tham s·ªë th·ª© 2), ki·ªÉm tra s·ªë l·∫ßn v√† duy tr√¨ s·ªë l·∫ßn ƒëi·ªÅu n√†y x·∫£y ra. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V√≠ d·ª• 1:\n",
    "\n",
    "+ C√¢u l·ªánh ƒë·∫ßu ti√™n thay th·∫ø t·∫•t c·∫£ c√°c l·∫ßn xu·∫•t hi·ªán c·ªßa 'ub' b·∫±ng '~*' (kh√¥ng ph√¢n bi·ªát ch·ªØ hoa ch·ªØ th∆∞·ªùng): 'S~*ject has ~*er booked already'.\n",
    "+ C√¢u l·ªánh th·ª© hai thay th·∫ø t·∫•t c·∫£ c√°c l·∫ßn xu·∫•t hi·ªán c·ªßa 'ub' b·∫±ng '~*' (ph√¢n bi·ªát ch·ªØ hoa ch·ªØ th∆∞·ªùng): 'S~*ject has Uber booked already'.\n",
    "+ C√¢u l·ªánh th·ª© ba thay th·∫ø l·∫ßn xu·∫•t hi·ªán ƒë·∫ßu ti√™n c·ªßa 'ub' b·∫±ng '~*' (kh√¥ng ph√¢n bi·ªát ch·ªØ hoa ch·ªØ th∆∞·ªùng): 'S~*ject has Uber booked already'.\n",
    "+ Th·ª© t∆∞ thay th·∫ø 'AND' b·∫±ng ' & ' (kh√¥ng ph√¢n bi·ªát ch·ªØ hoa ch·ªØ th∆∞·ªùng): .'Baked Beans & Spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S~*ject has ~*er booked already\n",
      "S~*ject has Uber booked already\n",
      "S~*ject has Uber booked already\n",
      "Baked Beans & Spam\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "print(re.sub('ub', '~*', 'Subject has Uber booked already',  \n",
    "             flags=re.IGNORECASE)) \n",
    "print(re.sub('ub', '~*', 'Subject has Uber booked already')) \n",
    "print(re.sub('ub', '~*', 'Subject has Uber booked already', \n",
    "             count=1, flags=re.IGNORECASE)) \n",
    "print(re.sub(r'\\sAND\\s', ' & ', 'Baked Beans And Spam',  \n",
    "             flags=re.IGNORECASE)) # Trong c√¢u l·ªánh n√†y r'\\sAND\\s' c√≥ th·ªÉ ko c·∫ßn d√πng \\s, nh∆∞ng ' & ', ph·∫£i x√≥a c√°c d·∫•u tr·ªëng ƒëi, ƒë·ªÉ khi thay th·∫ø kh√¥ng b·ªã d∆∞ d·∫•u tr·ªëng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re.subn() \n",
    "subn() t∆∞∆°ng t·ª± nh∆∞ sub() v·ªÅ m·ªçi m·∫∑t, ngo·∫°i tr·ª´ c√°ch cung c·∫•p ƒë·∫ßu ra. N√≥ tr·∫£ v·ªÅ m·ªôt b·ªô d·ªØ li·ªáu c√≥ t·ªïng s·ªë thay th·∫ø v√† chu·ªói m·ªõi thay v√¨ ch·ªâ chu·ªói. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V√≠ d·ª•:\n",
    "\n",
    "re.subn() thay th·∫ø t·∫•t c·∫£ c√°c l·∫ßn xu·∫•t hi·ªán c·ªßa m·ªôt m·∫´u trong chu·ªói v√† tr·∫£ v·ªÅ m·ªôt b·ªô d·ªØ li·ªáu c√≥ chu·ªói ƒë√£ s·ª≠a ƒë·ªïi v√† s·ªë l·∫ßn thay th·∫ø ƒë∆∞·ª£c th·ª±c hi·ªán. N√≥ h·ªØu √≠ch cho c·∫£ s·ª± thay th·∫ø ph√¢n bi·ªát ch·ªØ hoa ch·ªØ th∆∞·ªùng v√† kh√¥ng ph√¢n bi·ªát ch·ªØ hoa ch·ªØ th∆∞·ªùng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('S~*ject has Uber booked already', 1)\n",
      "('S~*ject has ~*er booked already', 2)\n",
      "2\n",
      "S~*ject has ~*er booked already\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "  \n",
    "print(re.subn('ub', '~*', 'Subject has Uber booked already')) \n",
    "  \n",
    "t = re.subn('ub', '~*', 'Subject has Uber booked already', \n",
    "            flags=re.IGNORECASE) \n",
    "print(t) \n",
    "print(len(t)) \n",
    "print(t[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re.escape()\n",
    "Tr·∫£ v·ªÅ chu·ªói c√≥ t·∫•t c·∫£ c√°c k√Ω t·ª± kh√¥ng ph·∫£i ch·ªØ v√† s·ªë b·ªã g·∫°ch ch√©o ng∆∞·ª£c, ƒëi·ªÅu n√†y r·∫•t h·ªØu √≠ch n·∫øu b·∫°n mu·ªën kh·ªõp m·ªôt chu·ªói k√Ω t·ª± t√πy √Ω c√≥ th·ªÉ ch·ª©a c√°c si√™u k√Ω t·ª± bi·ªÉu th·ª©c ch√≠nh quy trong ƒë√≥."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V√≠ d·ª•:\n",
    "\n",
    "re.escape()ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ tho√°t c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát trong m·ªôt chu·ªói, gi√∫p an to√†n khi s·ª≠ d·ª•ng l√†m m·∫´u trong bi·ªÉu th·ª©c ch√≠nh quy. N√≥ ƒë·∫£m b·∫£o r·∫±ng b·∫•t k·ª≥ k√Ω t·ª± n√†o c√≥ √Ω nghƒ©a ƒë·∫∑c bi·ªát trong bi·ªÉu th·ª©c ch√≠nh quy ƒë·ªÅu ƒë∆∞·ª£c coi l√† k√Ω t·ª± ch·ªØ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\\ is\\ Awesome\\ even\\ 1\\ AM\n",
      "I\\ Asked\\ what\\ is\\ \\\n",
      "\\ this\\ \\[a\\-9\\],\\ he\\ said\\ \\\t\\ \\^WoW\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "print(re.escape(\"This is Awesome even 1 AM\")) \n",
    "print(re.escape(\"I Asked what is \\n this [a-9], he said \\t ^WoW\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re.search()\n",
    "Ph∆∞∆°ng th·ª©c n√†y tr·∫£ v·ªÅ Kh√¥ng c√≥ (n·∫øu m·∫´u kh√¥ng kh·ªõp) ho·∫∑c re.MatchObject ch·ª©a th√¥ng tin v·ªÅ ph·∫ßn kh·ªõp c·ªßa chu·ªói. Ph∆∞∆°ng ph√°p n√†y d·ª´ng sau l·∫ßn so kh·ªõp ƒë·∫ßu ti√™n, v√¨ v·∫≠y ph∆∞∆°ng ph√°p n√†y ph√π h·ª£p nh·∫•t ƒë·ªÉ ki·ªÉm tra bi·ªÉu th·ª©c ch√≠nh quy h∆°n l√† tr√≠ch xu·∫•t d·ªØ li·ªáu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V√≠ d·ª•: T√¨m ki·∫øm s·ª± xu·∫•t hi·ªán c·ªßa m·∫´u\n",
    "\n",
    "M√£ n√†y s·ª≠ d·ª•ng bi·ªÉu th·ª©c ch√≠nh quy ƒë·ªÉ t√¨m ki·∫øm m·∫´u trong chu·ªói ƒë√£ cho. N·∫øu t√¨m th·∫•y k·∫øt qu·∫£ kh·ªõp, n√≥ s·∫Ω tr√≠ch xu·∫•t v√† in c√°c ph·∫ßn ph√π h·ª£p c·ªßa chu·ªói.\n",
    "\n",
    "Trong v√≠ d·ª• c·ª• th·ªÉ n√†y, n√≥ t√¨m ki·∫øm m·ªôt m·∫´u bao g·ªìm m·ªôt th√°ng (ch·ªØ c√°i) theo sau l√† m·ªôt ng√†y (ch·ªØ s·ªë) trong chu·ªói ƒë·∫ßu v√†o ‚ÄúT√¥i sinh ng√†y 24 th√°ng 6‚Äù. N·∫øu t√¨m th·∫•y k·∫øt qu·∫£ kh·ªõp, n√≥ s·∫Ω in k·∫øt qu·∫£ kh·ªõp ƒë·∫ßy ƒë·ªß, th√°ng v√† ng√†y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match at index 14, 21\n",
      "Full match: June 24\n",
      "Month: June\n",
      "Day: 24\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "regex = r\"([a-zA-Z]+) (\\d+)\"\n",
    "  \n",
    "match = re.search(regex, \"I was born on June 24\") \n",
    "if match != None: \n",
    "    print (\"Match at index %s, %s\" % (match.start(), match.end())) \n",
    "    print (\"Full match: %s\" % (match.group(0))) \n",
    "    print (\"Month: %s\" % (match.group(1))) \n",
    "    print (\"Day: %s\" % (match.group(2))) \n",
    "  \n",
    "else: \n",
    "    print (\"The regex pattern does not match.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the string and the regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(11, 12), match='G'>\n",
      "re.compile('\\\\bG')\n",
      "Welcome to GeeksForGeeks\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "s = \"Welcome to GeeksForGeeks\"\n",
    "res = re.search(r\"\\bG\", s) \n",
    "\n",
    "print(res)\n",
    "print(res.re) \n",
    "print(res.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Getting index of matched object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "14\n",
      "(11, 14)\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "  \n",
    "s = \"Welcome to GeeksForGeeks\"\n",
    "  \n",
    "res = re.search(r\"\\bGee\", s) \n",
    "  \n",
    "print(res.start()) \n",
    "print(res.end()) \n",
    "print(res.span())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Getting matched substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(5, 9), match='me t'>\n",
      "me t\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "s = \"Welcome to GeeksForGeeks\"\n",
    "res = re.search(r\"\\D{2} t\", s) \n",
    "print(res)\n",
    "print(res.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "    Word Embedding is a language modeling technique for mapping words to vectors of real numbers. It represents words or phrases in vector space with several dimensions. Word embeddings can be generated using various methods like neural networks, co-occurrence matrices, probabilistic models, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'any communicable material that is used to describe, explain or instruct regarding some attributes of an object, system or procedure'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"<b>any communicable material that is used to describe, explain or instruct regarding some attributes of an object, system or procedure</b>\"\n",
    "# S·ª≠ d·ª•ng ch√≠nh quy h√≥a\n",
    "import re\n",
    "def stringhtml(data):\n",
    "    '''    \n",
    "        + r\"\": K√Ω t·ª± r tr∆∞·ªõc d·∫•u ngo·∫∑c k√©p bi·∫øn chu·ªói th√†nh \"raw string\" (chu·ªói th√¥), gi√∫p x·ª≠ l√Ω c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát b√™n trong chu·ªói m·ªôt c√°ch ch√≠nh x√°c m√† kh√¥ng c·∫ßn d√πng k√Ω t·ª± escape (\\).\n",
    "        + <: K√Ω t·ª± < t∆∞·ª£ng tr∆∞ng cho vi·ªác b·∫Øt ƒë·∫ßu m·ªôt th·∫ª HTML.\n",
    "        + .*?: D·∫•u ch·∫•m . ƒë·∫°i di·ªán cho b·∫•t k·ª≥ k√Ω t·ª± n√†o, v√† *? l√† m·ªôt bi·ªÉu th·ª©c l∆∞·ªùi bi·∫øng (non-greedy), c√≥ nghƒ©a l√† n√≥ s·∫Ω kh·ªõp v·ªõi c√†ng √≠t k√Ω t·ª± c√†ng t·ªët cho ƒë·∫øn khi g·∫∑p k√Ω t·ª± kh·ªõp ti·∫øp theo.\n",
    "        + >: K√Ω t·ª± > t∆∞·ª£ng tr∆∞ng cho vi·ªác k·∫øt th√∫c m·ªôt th·∫ª HTML.\n",
    "       =>  Do ƒë√≥, r\"<.*?>\" c√≥ √Ω nghƒ©a l√† t√¨m v√† kh·ªõp v·ªõi m·ªçi th·∫ª HTML trong chu·ªói, t·ª´ < ƒë·∫øn >, b·∫•t k·ªÉ n·ªôi dung b√™n trong th·∫ª l√† g√¨.\n",
    "    '''\n",
    "    \n",
    "    p = re.compile(r\"<.*?>\")\n",
    "    return p.sub(\"\",data)\n",
    "\n",
    "stringhtml(sample_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNIcode Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"\\nToday is a great day! \\xf0\\x9f\\x98\\x84\\nI finished my work early and can now write\\nRelax with a cup of coffee \\xe2\\x98\\x95\\xef\\xb8\\x8f. The weather is beautiful,\\nIt's sunny and warm \\xf0\\x9f\\x8c\\x9e. I'm so happy to be able to spend time with my family \\xf0\\x9f\\x91\\xa8\\xe2\\x80\\x8d\\xf0\\x9f\\x91\\xa9\\xe2\\x80\\x8d\\xf0\\x9f\\x91\\xa7\\xe2\\x80\\x8d\\xf0\\x9f\\x91\\xa6\\nand friends \\xf0\\x9f\\x91\\xaf\\xe2\\x80\\x8d\\xe2\\x99\\x82\\xef\\xb8\\x8f this weekend. Wishing everyone a wonderful day! \\xf0\\x9f\\x8c\\x9f\\n\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "Today is a great day! üòÑ\n",
    "I finished my work early and can now write\n",
    "Relax with a cup of coffee ‚òïÔ∏è. The weather is beautiful,\n",
    "It's sunny and warm üåû. I'm so happy to be able to spend time with my family üë®‚Äçüë©‚Äçüëß‚Äçüë¶\n",
    "and friends üëØ‚Äç‚ôÇÔ∏è this weekend. Wishing everyone a wonderful day! üåü\n",
    "\"\"\"\n",
    "\n",
    "sample_text.encode(\"utf-8\")\n",
    "\n",
    "# Nh·ªØng emoji trong vƒÉn b·∫£n b·∫£n ch·∫•t v·∫´n l√† nh·ªØng chu·ªói k√≠ t·ª± ƒë∆∞·ª£c m√£ h√≥a, c·∫ßn x·ª≠ l√≠ ph√π h·ª£p ·ªü c√°c tr∆∞·ªùng h·ª£p kh√°c nhau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TextBlob' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m TextBlob\u001b[38;5;241m.\u001b[39mcorrect()  \u001b[38;5;66;03m# TextBlob(\"how are you?\")\u001b[39;00m\n\u001b[0;32m     13\u001b[0m incorrect_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhhow ar yor?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 15\u001b[0m TextBlob \u001b[38;5;241m=\u001b[39m \u001b[43mTextBlob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mincorrect_text\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     16\u001b[0m TextBlob\u001b[38;5;241m.\u001b[39mcorrect() \u001b[38;5;66;03m# TextBlob(\"how ar for?\")\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'TextBlob' object is not callable"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "#V·ªõi nh·ªØng t·ª´ ti·∫øng Anh b·ªã l·∫∑p k√≠ t·ª± n·∫øu form ƒë√£ ho√†n ch·ªânh th√¨ c√≥ th·ªÉ d√πng textblob ƒë·∫ª kh·∫Øc ph·ª•c l·ªói,\n",
    "# Nh∆∞ng th∆∞ vi√™n n√†y c≈©ng c√≤n r·∫•t h·∫°n ch·∫ø, kh√¥ng th·ªÉ gi·∫£i quy·∫øt h·∫øt ƒë∆∞·ª£c, c√≥ th·ªÉ d√πng c√°ch s·ª≠a th·ªß c√¥ng\n",
    "# ho·∫∑c d√πng c√°c th∆∞ vi·ªán c√≥ b·ªô ng√¥n ng·ªØ t·ª´ v·ª±ng c·∫•u tr√∫c, c√¢u l·ªõn h∆°n ƒë·ªÉ s·ª≠a => c√≥ m·ªôt b·ªô data ƒë·∫πp, v√¨ vi·ªác \n",
    "# model h·ªçc c√°c c√¢u sai ·∫£nh h∆∞·ªüng l·ªõn ƒë·∫øn ch·∫•t l∆∞·ª£ng.\n",
    "\n",
    "incorrect_text = \"hhow are yoou?\"\n",
    "\n",
    "TextBlob = TextBlob(incorrect_text) \n",
    "TextBlob.correct()  # TextBlob(\"how are you?\")\n",
    "\n",
    "incorrect_text = \"hhow ar yor?\"\n",
    "\n",
    "TextBlob = TextBlob(incorrect_text) \n",
    "TextBlob.correct() # TextBlob(\"how ar for?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# nltk l√† m·ªôt th∆∞ vi·ªán x·ª≠ l√≠ ngon ng·ªØ ƒë∆∞·ª£c x·ª≠ d·ª•ng r·∫•t nhi·ªÅu,\n",
    "#n√≥ support h·∫ßu h·∫øt c√°c h√†m phan t√≠ch, x·ª≠ l√≠ data d·∫°ng vƒÉn b·∫£n.\n",
    "\n",
    "# sent_tokenize : T√°ch c√¢u th√†nh c√°c ph·∫ßn t·ª≠ c·ªßa list\n",
    "# word_tokenize : t√°ch ch·ªØ th√†nh c√°c ph√¢n t·ª≠ c·ªßa list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The most important person in the life of a person is his or her mother.',\n",
       " 'There is also a super character in our real lives and that is the mother.',\n",
       " 'So for me, that special character is my mom as she is a superwoman.',\n",
       " 'Regardless of day and night, my mother is always there for me.',\n",
       " 'Like everyone‚Äôs mother, my mother is a hard-working woman.',\n",
       " 'She worked day and night for my whole house.',\n",
       " 'I learn almost everything \\n    from my mother, as she‚Äôs my first teacher.',\n",
       " 'My mother taught me manners \\n    in everything, like manners of eating, manners of talking, and so on.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = \"\"\"The most important person in the life of a person is his or her mother. \n",
    "    There is also a super character in our real lives and that is the mother. \n",
    "    So for me, that special character is my mom as she is a superwoman. \n",
    "    Regardless of day and night, my mother is always there for me.\n",
    "\n",
    "    Like everyone‚Äôs mother, my mother is a hard-working woman. \n",
    "    She worked day and night for my whole house. I learn almost everything \n",
    "    from my mother, as she‚Äôs my first teacher. My mother taught me manners \n",
    "    in everything, like manners of eating, manners of talking, and so on.\n",
    "\"\"\"\n",
    "sents = sent_tokenize(dummy)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['most', 'important', 'person', 'in']\n",
      "['The', 'most', 'important', 'person', 'in', 'the', 'life', 'of', 'a', 'person', 'is', 'his', 'or', 'her', 'mother', '.']\n",
      "['There', 'is', 'also', 'a', 'super', 'character', 'in', 'our', 'real', 'lives', 'and', 'that', 'is', 'the', 'mother', '.']\n",
      "['So', 'for', 'me', ',', 'that', 'special', 'character', 'is', 'my', 'mom', 'as', 'she', 'is', 'a', 'superwoman', '.']\n",
      "['Regardless', 'of', 'day', 'and', 'night', ',', 'my', 'mother', 'is', 'always', 'there', 'for', 'me', '.']\n",
      "['Like', 'everyone', '‚Äô', 's', 'mother', ',', 'my', 'mother', 'is', 'a', 'hard-working', 'woman', '.']\n",
      "['She', 'worked', 'day', 'and', 'night', 'for', 'my', 'whole', 'house', '.']\n",
      "['I', 'learn', 'almost', 'everything', 'from', 'my', 'mother', ',', 'as', 'she', '‚Äô', 's', 'my', 'first', 'teacher', '.']\n",
      "['My', 'mother', 'taught', 'me', 'manners', 'in', 'everything', ',', 'like', 'manners', 'of', 'eating', ',', 'manners', 'of', 'talking', ',', 'and', 'so', 'on', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(dummy)\n",
    "print(words[1:5])  # ['most', 'important', 'person', 'in']\n",
    "\n",
    "#or \n",
    "for sent in sents:\n",
    "    print(word_tokenize(sent)) # ph√¢n t√°ch th√†nh t·ª´ng ch·ªØ ri√™ng l·∫ª trong t·ª´ng c√¢u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the most important person in the life of a person is his or her mother. \\n    there is also a super character in our real lives and that is the mother. \\n    so for me, that special character is my mom as she is a superwoman. \\n    regardless of day and night, my mother is always there for me.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ƒê∆∞a b·ªô data v·ªÅ chung m·ªôt ki·ªÉu vi·∫øt -> vi·∫øt th∆∞·ªùng lower\n",
    "\n",
    "sentence = \"\"\"The most important person in the life of a person is his or her mother. \n",
    "    There is also a super character in our real lives and that is the mother. \n",
    "    So for me, that special character is my mom as she is a superwoman. \n",
    "    Regardless of day and night, my mother is always there for me.\"\"\"\n",
    "    \n",
    "sentence.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check out my notebook '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_url(text):\n",
    "    \"\"\"\n",
    "    \n",
    "    r\"\": K√Ω t·ª± r tr∆∞·ªõc d·∫•u ngo·∫∑c k√©p bi·∫øn chu·ªói th√†nh \"raw string\" (chu·ªói th√¥), gi√∫p x·ª≠ l√Ω c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát b√™n trong chu·ªói m·ªôt c√°ch ch√≠nh x√°c m√† kh√¥ng c·∫ßn d√πng k√Ω t·ª± escape (\\).\n",
    "\n",
    "    https?://\\S+: ƒê√¢y l√† m·ªôt ph·∫ßn c·ªßa bi·ªÉu th·ª©c ch√≠nh quy v√† c√≥ √Ω nghƒ©a nh∆∞ sau:\n",
    "\n",
    "    http: Kh·ªõp ch√≠nh x√°c v·ªõi chu·ªói http.\n",
    "    s?: K√Ω t·ª± s c√≥ th·ªÉ xu·∫•t hi·ªán ho·∫∑c kh√¥ng (kh·ªõp v·ªõi c·∫£ http v√† https).\n",
    "    ://: Kh·ªõp ch√≠nh x√°c v·ªõi chu·ªói ://.\n",
    "    \\S+: Kh·ªõp v·ªõi m·ªôt ho·∫∑c nhi·ªÅu k√Ω t·ª± kh√¥ng ph·∫£i kho·∫£ng tr·∫Øng. \\S ƒë·∫°i di·ªán cho b·∫•t k·ª≥ k√Ω t·ª± n√†o kh√¥ng ph·∫£i l√† kho·∫£ng tr·∫Øng, v√† + ch·ªâ ra r·∫±ng ph·∫£i c√≥ √≠t nh·∫•t m·ªôt k√Ω t·ª± kh√¥ng ph·∫£i kho·∫£ng tr·∫Øng.\n",
    "    |: D·∫•u g·∫°ch ƒë·ª©ng (|) c√≥ nghƒ©a l√† \"ho·∫∑c\". N√≥ ph√¢n t√°ch hai ph·∫ßn c·ªßa bi·ªÉu th·ª©c ch√≠nh quy v√† cho ph√©p kh·ªõp v·ªõi m·ªôt trong hai ph·∫ßn.\n",
    "\n",
    "    www\\.\\S+: ƒê√¢y l√† ph·∫ßn c√≤n l·∫°i c·ªßa bi·ªÉu th·ª©c ch√≠nh quy v√† c√≥ √Ω nghƒ©a nh∆∞ sau:\n",
    "\n",
    "    www: Kh·ªõp ch√≠nh x√°c v·ªõi chu·ªói www.\n",
    "    \\.: Kh·ªõp v·ªõi k√Ω t·ª± d·∫•u ch·∫•m (.). D·∫•u ch·∫•m l√† k√Ω t·ª± ƒë·∫∑c bi·ªát trong regex, n√™n c·∫ßn ph·∫£i c√≥ k√Ω t·ª± escape (\\) ƒë·ªÉ kh·ªõp v·ªõi d·∫•u ch·∫•m th·ª±c t·∫ø.\n",
    "    \\S+: Kh·ªõp v·ªõi m·ªôt ho·∫∑c nhi·ªÅu k√Ω t·ª± kh√¥ng ph·∫£i kho·∫£ng tr·∫Øng, t∆∞∆°ng t·ª± nh∆∞ ph·∫ßn tr∆∞·ªõc.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r\"\", text)\n",
    "\n",
    "text = \"Check out my notebook https://englishinsane.com/paragraph-on-my-mother\"\n",
    "remove_url(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import time\n",
    "string.punctuation   # Danh s√°ch c√°c punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T·∫°o bi·∫øn ƒë·ªÉ d·ªÖ qu·∫£n l√≠\n",
    "exclude = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How are you'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#C√°ch 1\n",
    "\n",
    "def remove_punc(text):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        text : string l√∫c ch∆∞a b·ªè nh·ªØng punctuation\n",
    "\n",
    "    Returns:\n",
    "        text: string ƒë√£ x·ª≠ l√≠ punctuatio\n",
    "    \"\"\"\n",
    "    \n",
    "    for char in exclude:\n",
    "        text = text.replace(char,\"\")\n",
    "    return text\n",
    "\n",
    "text = \"How are you?\"\n",
    "\n",
    "remove_punc(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How are you'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C√°ch 2\n",
    "def remove_punc_v2(text):\n",
    "    \"\"\"\n",
    "    str.maketrans(\"\", \"\", exclude): T·∫°o m·ªôt b·∫£ng d·ªãch (translation table) m√† s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng b·ªüi ph∆∞∆°ng th·ª©c translate. B·∫£ng d·ªãch n√†y x√°c ƒë·ªãnh c√°c k√Ω t·ª± n√†o s·∫Ω b·ªã thay th·∫ø ho·∫∑c lo·∫°i b·ªè.\n",
    "    Ba ƒë·ªëi s·ªë trong str.maketrans l√†:\n",
    "    ƒê·ªëi s·ªë th·ª© nh·∫•t (\"\"): X√°c ƒë·ªãnh c√°c k√Ω t·ª± s·∫Ω ƒë∆∞·ª£c thay th·∫ø. Trong tr∆∞·ªùng h·ª£p n√†y, kh√¥ng c√≥ k√Ω t·ª± n√†o ƒë∆∞·ª£c thay th·∫ø.\n",
    "    ƒê·ªëi s·ªë th·ª© hai (\"\"): X√°c ƒë·ªãnh c√°c k√Ω t·ª± thay th·∫ø. V√¨ kh√¥ng c√≥ k√Ω t·ª± n√†o ƒë∆∞·ª£c thay th·∫ø, ƒë·ªëi s·ªë n√†y c≈©ng l√† m·ªôt chu·ªói tr·ªëng.\n",
    "    ƒê·ªëi s·ªë th·ª© ba (exclude): X√°c ƒë·ªãnh c√°c k√Ω t·ª± s·∫Ω b·ªã lo·∫°i b·ªè. Bi·∫øn exclude ph·∫£i ch·ª©a t·∫•t c·∫£ c√°c d·∫•u c√¢u c·∫ßn lo·∫°i b·ªè.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return text.translate(str.maketrans(\"\",\"\",exclude))\n",
    "\n",
    "text = \"How are you?\"\n",
    "remove_punc(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Word\n",
    " Stop words l√† c√°c t·ª´ ph·ªï bi·∫øn trong m·ªôt ng√¥n ng·ªØ m√† th∆∞·ªùng kh√¥ng mang nhi·ªÅu √Ω nghƒ©a ng·ªØ nghƒ©a v√† ƒë∆∞·ª£c lo·∫°i b·ªè trong qu√° tr√¨nh x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP) v√† khai th√°c vƒÉn b·∫£n. C√°c t·ª´ n√†y bao g·ªìm c√°c t·ª´ nh∆∞ \"a\", \"an\", \"the\", \"in\", \"on\", \"and\", \"is\", \"to\", \"of\" trong ti·∫øng Anh, ho·∫∑c c√°c t·ª´ t∆∞∆°ng t·ª± trong c√°c ng√¥n ng·ªØ kh√°c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Th∆∞ vi·ªán n√†y ch·ªâ h·ªó tr·ª£ stopword cho m·ªôt s·ªë lo·∫°i ng√¥n ng·ªØ:\n",
    "'''\n",
    "available_langs = {\n",
    "        \"catalan\": \"ca\",\n",
    "        \"czech\": \"cs\",\n",
    "        \"german\": \"de\",\n",
    "        \"greek\": \"el\",\n",
    "        \"english\": \"en\",\n",
    "        \"spanish\": \"es\",\n",
    "        \"finnish\": \"fi\",\n",
    "        \"french\": \"fr\",\n",
    "        \"hungarian\": \"hu\",\n",
    "        \"icelandic\": \"is\",\n",
    "        \"italian\": \"it\",\n",
    "        \"latvian\": \"lv\",\n",
    "        \"dutch\": \"nl\",\n",
    "        \"polish\": \"pl\",\n",
    "        \"portuguese\": \"pt\",\n",
    "        \"romanian\": \"ro\",\n",
    "        \"russian\": \"ru\",\n",
    "        \"slovak\": \"sk\",\n",
    "        \"slovenian\": \"sl\",\n",
    "        \"swedish\": \"sv\",\n",
    "        \"tamil\": \"ta\",\n",
    "    }\n",
    "\n",
    "'''    \n",
    "\n",
    "STOP_WORDS = stopwords.words(\"english\")\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  hoan'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopword(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in STOP_WORDS:\n",
    "            new_text.append(\"\")\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)\n",
    "\n",
    "text = \"I am Hoan\"\n",
    "\n",
    "remove_stopword(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming --- Lemmatization\n",
    "<image src = \"https://media.licdn.com/dms/image/C4D12AQEZCHQOHXSEhg/article-cover_image-shrink_600_2000/0/1650689035153?e=2147483647&v=beta&t=uRsPEF-Apt9EvVTcUGR_ZhAs_Dk39de4MFQDo78LHos\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk walk walk walk\n",
      "chang chang chang chang changer\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "def stem_word(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])\n",
    "\n",
    "sample_1 = \"walk walks walking walked\"\n",
    "print(stem_word(sample_1))\n",
    "\n",
    "sample_2 = \"change changing changes changed changer\"\n",
    "print(stem_word(sample_2))  # ·ªü ƒë√¢y changer data c·ªßa h√†m hi·ªÉu l√† ng∆∞·ªùi thay ƒë·ªïi, n√≥ l√† m·ªôt t·ª´ ri√™ng bi·ªát, kh√¥ng ph·∫£i l∆∞u theo ki·ªÉu bi·∫øn th·ªÉ c·ªßa change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "the                 the                 \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "a                   a                   \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# ƒê·∫£m b·∫£o c√°c g√≥i c·∫ßn thi·∫øt ƒë√£ ƒë∆∞·ª£c t·∫£i v·ªÅ\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at the same time. He has a bad habit of swimming after playing long hours in the Sun\"\n",
    "punctuation = string.punctuation\n",
    "\n",
    "# T√°ch c√°c t·ª´ trong c√¢u\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Lo·∫°i b·ªè d·∫•u c√¢u kh·ªèi danh s√°ch c√°c t·ª´\n",
    "sentence_words = [word for word in sentence_words if word not in punctuation]\n",
    "\n",
    "# ƒê·ªãnh d·∫°ng ti√™u ƒë·ªÅ b·∫£ng\n",
    "print(f\"{'Word':<20}{'Lemma':<20}\")\n",
    "\n",
    "# Lemmatize t·ª´ng t·ª´ trong danh s√°ch v√† in k·∫øt qu·∫£\n",
    "for word in sentence_words:\n",
    "    print(f\"{word:<20}{wordnet_lemmatizer.lemmatize(word):<20}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization code chu·∫©n s·ª≠ d·ª•ng nltk v√† spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    " \n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The quick brown foxes are jumping over the lazy dogs.\n",
      "Lemmatized Text: the quick brown fox be jump over the lazy dog .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    " \n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    " \n",
    "# Define a sample text\n",
    "text = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
    " \n",
    "# Process the text using spaCy\n",
    "doc = nlp(text)\n",
    " \n",
    "# Extract lemmatized tokens\n",
    "lemmatized_tokens = [token.lemma_ for token in doc]\n",
    " \n",
    "# Join the lemmatized tokens into a sentence\n",
    "lemmatized_text = ' '.join(lemmatized_tokens)\n",
    " \n",
    "# Print the original and lemmatized text\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Lemmatized Text:\", lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "    Word Embedding is a language modeling technique for mapping words to vectors of real numbers. It represents words or phrases in vector space with several dimensions. Word embeddings can be generated using various methods like neural networks, co-occurrence matrices, probabilistic models, etc.\n",
    "\n",
    "<div style=\"background-color: #fff; padding: 10px;\">\n",
    "    <img src=\"https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg\" alt=\"Linear Relationships\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src = \"https://www.analyticssteps.com/backend/media/uploads/2019/09/06/image-20190906164045-2.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Kh·ªüi t·∫°o c√¥ng c·ª• √°nh x·∫° vecto c·ªßa sklearn \n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ gi·∫£ x·ª≠ ch√∫ng ta c√≥ b·ªô data d·∫°ng b·∫£ng v·ªõi 2 h√†ng, m·ª•c ƒë√≠ch c·ªßa ch√∫ng ta l√† bi·∫øn 2 h√†ng n√†y sang d·∫°ng vecto theo ki·ªÉu Bag of word vectors,\n",
    "+ ƒê·∫ßu ti√™n, ch√∫ng ta s·∫Ω chuy·ªÉn h·∫øt c·∫£ 2 h√†ng d∆∞·ªõi n√†y v·ªÅ c√πng m·ªôt bow, sau ƒë√≥ t·ª´ text n√†y s·∫Ω l·∫•y ra nh·ªØng t·ª´ t·ª´ ƒë·ªçc l·∫≠p duy nh·∫•t, m·ªói t·ª´ s·∫Ω g·∫Øn v·ªõi m·ªôt index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'documentation': 22, 'development': 19, 'may': 42, 'involve': 36, 'document': 21, 'drafting': 23, 'formatting': 27, 'submitting': 61, 'reviewing': 53, 'approving': 3, 'distributing': 20, 'reposting': 52, 'and': 2, 'tracking': 65, 'etc': 25, 'are': 4, 'convened': 15, 'by': 10, 'associated': 5, 'standard': 59, 'operating': 46, 'procedure': 48, 'in': 33, 'regulatory': 51, 'industry': 34, 'it': 38, 'could': 16, 'also': 1, 'creating': 17, 'content': 14, 'from': 28, 'scratch': 54, 'should': 57, 'be': 8, 'easy': 24, 'to': 63, 'read': 50, 'understand': 66, 'if': 31, 'is': 37, 'too': 64, 'long': 40, 'wordy': 69, 'misunderstood': 43, 'or': 47, 'ignored': 32, 'clear': 11, 'concise': 13, 'words': 68, 'used': 67, 'sentences': 55, 'limited': 39, 'maximum': 41, 'of': 45, '15': 0, 'intended': 35, 'for': 26, 'general': 30, 'audience': 6, 'avoid': 7, 'gender': 29, 'specific': 58, 'terms': 62, 'cultural': 18, 'biases': 9, 'series': 56, 'procedures': 49, 'steps': 60, 'clearly': 12, 'numbered': 44}\n",
      "\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "instance_1= '''Documentation development may involve document drafting, \n",
    "formatting, submitting, reviewing, approving, distributing, reposting and tracking, etc., \n",
    "and are convened by associated standard operating procedure in a regulatory industry. \n",
    "'''\n",
    "instance_2 = '''\n",
    "It could also involve creating content from scratch. Documentation should be easy to read and understand. \n",
    "If it is too long and too wordy, it may be misunderstood or ignored. Clear, concise words should be used, \n",
    "and sentences should be limited to a maximum of 15 words. Documentation intended for a general audience should \n",
    "avoid gender-specific terms and cultural biases. In a series of procedures, steps should be clearly numbered.\n",
    "'''\n",
    "bow = cv.fit_transform([instance_1,instance_2])\n",
    "print(cv.vocabulary_)\n",
    "print()\n",
    "print(len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 2 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0\n",
      "  1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0]]\n",
      "[[1 1 4 0 0 0 1 1 5 1 0 1 1 1 1 0 1 1 1 0 0 0 2 0 1 0 1 0 1 1 1 1 1 1 0 1\n",
      "  1 1 3 1 1 1 1 1 1 2 0 1 0 1 1 0 0 0 1 1 1 5 1 0 1 0 1 2 2 0 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "# ch√∫ng ta √°nh x·∫° t·ª´ng h√†ng instance_1,.. l√™n bow, v·ªõi m·ªói t·ª´ trong instance_1, ... xu√¢t shieenj bao nhi√™u l·∫ßn trong bow th√¨ s·∫Ω ƒë√°nh s·ªë v√†o v·ªã tr√≠\n",
    "# c·ªßa vectow cho instance ƒë√≥\n",
    "print(bow[0].toarray())\n",
    "print(bow[1].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# text th·ª≠ v·ªõi m·ªôt v√†i data b·∫•t k√¨ d√πng b·ªô √°nh x·∫° bow\n",
    "print(cv.transform([\"ha khai hoan\"]).toarray())\n",
    "print()\n",
    "print(cv.transform([\"Documentation development may Khai Hoan\"]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thay v√¨ t·∫°o ra vector v·ªõi m·ªói v·ªã tr√≠ t∆∞∆°ng ·ª©ng v·ªõi m·ªôt ch·ªØ, th√¨ \n",
    "# N-grams t·∫°o vector v·ªõi N ch·ªØ li√™n ti·∫øp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src = \"https://www.kdnuggets.com/wp-content/uploads/agarwal_ngram_language_modeling_natural_language_processing_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', '<s>'), ('<s>', 'This'), ('This', 'is'), ('is', 'a'), ('a', 'sample'), ('sample', 'document'), ('document', 'This'), ('This', 'document'), ('document', 'contains'), ('contains', 'several'), ('several', 'sentences'), ('sentences', 'We'), ('We', 'can'), ('can', 'extract'), ('extract', 'bigrams'), ('bigrams', 'from'), ('from', 'it'), ('it', '<e>')]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "# punctuation data\n",
    "PUNC = string.punctuation\n",
    "\n",
    "def processing(document):\n",
    "    \"\"\"\n",
    "    Pre-process the document by removing punctuation and tokenizing.\n",
    "    \n",
    "    Args:\n",
    "        document: Raw document as a string.\n",
    "        \n",
    "    Returns:\n",
    "        A list of words (tokens).\n",
    "    \"\"\"\n",
    "    for punc in PUNC:\n",
    "        if punc in document:\n",
    "            document = document.replace(punc, \"\")\n",
    "    return nltk.word_tokenize(document)\n",
    "\n",
    "def extract_n_grams(data, n, start_token='<s>', end_token='<e>'):\n",
    "    \"\"\"\n",
    "    Extract all n-grams from the data.\n",
    "    \n",
    "    Args:\n",
    "        data: List of words (tokens).\n",
    "        n: Number of words in a sequence (n-gram size).\n",
    "    \n",
    "    Returns:\n",
    "        A list of n-grams.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_grams = []\n",
    "    \n",
    "    # Prepend start token n times, and append the end token one time\n",
    "    sentence = [start_token] * n + data + [end_token]\n",
    "    \n",
    "    # Convert list to tuple for processing n-grams\n",
    "    sentence = tuple(sentence)\n",
    "    \n",
    "    # Extract n-grams\n",
    "    for i in range(len(sentence) if n == 1 else len(sentence) - n + 1):\n",
    "        n_gram = sentence[i:i+n]\n",
    "        n_grams.append(n_gram)\n",
    "    \n",
    "    return n_grams\n",
    "\n",
    "# --------- Main ---------\n",
    "document = \"This is a sample document. This document contains several sentences. We can extract bigrams from it.\"\n",
    "\n",
    "# Extract bi-grams\n",
    "n_grams = extract_n_grams(processing(document), 2)\n",
    "print(n_grams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('<s>', '<s>'): 1, ('<s>', 'This'): 1, ('This', 'is'): 1, ('is', 'a'): 1, ('a', 'sample'): 1, ('sample', 'document'): 1, ('document', 'This'): 1, ('This', 'document'): 1, ('document', 'contains'): 1, ('contains', 'several'): 1, ('several', 'sentences'): 1, ('sentences', 'We'): 1, ('We', 'can'): 1, ('can', 'extract'): 1, ('extract', 'bigrams'): 1, ('bigrams', 'from'): 1, ('from', 'it'): 1, ('it', '<e>'): 1}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "# punctuation data\n",
    "PUNC = string.punctuation\n",
    "\n",
    "def processing(document):\n",
    "    \"\"\"\n",
    "    pre-processing document\n",
    "    \n",
    "    Args:\n",
    "        document: raw document\n",
    "        \n",
    "    Returns:\n",
    "        List of lists of words\n",
    "    \"\"\"\n",
    "    \n",
    "    for punc in PUNC:\n",
    "        if punc in document:\n",
    "            document = document.replace(punc,\"\")\n",
    "\n",
    "    return [nltk.word_tokenize(document)]\n",
    "\n",
    "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
    "    \"\"\"\n",
    "    Count all n-grams in the data\n",
    "    \n",
    "    Args:\n",
    "        data: List of lists of words\n",
    "        n: number of words in a sequence\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps a tuple of n-words to its frequency\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize dictionary of n-grams and their counts\n",
    "    n_grams = {}\n",
    "\n",
    "    # Go through each sentence in the data\n",
    "    for sentence in data: # complete this line\n",
    "        \n",
    "        # prepend start token n times, and  append the end token one time\n",
    "        sentence = [start_token] * n + sentence + [end_token]\n",
    "        \n",
    "        # convert list to tuple\n",
    "        # So that the sequence of words can be used as\n",
    "        # a key in the dictionary\n",
    "        sentence = tuple(sentence)\n",
    "        \n",
    "        # Use 'i' to indicate the start of the n-gram\n",
    "        # from index 0\n",
    "        # to the last index where the end of the n-gram\n",
    "        # is within the sentence.\n",
    "        \n",
    "        for i in range(len(sentence) if n==1 else len(sentence)-n+1): # complete this line\n",
    "\n",
    "            # Get the n-gram from i to i+n\n",
    "            n_gram = sentence[i:i+n]\n",
    "            \n",
    "            # check if the n-gram is in the dictionary\n",
    "            if n_gram in n_grams.keys(): # complete this line with the proper condition\n",
    "            \n",
    "                # Increment the count for this n-gram\n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                # Initialize this n-gram count to 1\n",
    "                n_grams[n_gram] = 1\n",
    "\n",
    "    return n_grams\n",
    "\n",
    "# --------- Main ---------\n",
    "document = \"This is a sample document. This document contains several sentences. We can extract bigrams from it.\"\n",
    "\n",
    "#Use n = 2: bi-grams\n",
    "print(count_n_grams(processing(document),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  CountVectorizer (N-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'documentation development may involve': 29, 'development may involve document': 26, 'may involve document drafting': 54, 'involve document drafting formatting': 45, 'document drafting formatting submitting': 28, 'drafting formatting submitting reviewing': 32, 'formatting submitting reviewing approving': 36, 'submitting reviewing approving distributing': 76, 'reviewing approving distributing reposting': 64, 'approving distributing reposting and': 8, 'distributing reposting and tracking': 27, 'reposting and tracking etc': 63, 'and tracking etc and': 6, 'tracking etc and are': 82, 'etc and are convened': 34, 'and are convened by': 2, 'are convened by associated': 9, 'convened by associated standard': 22, 'by associated standard operating': 18, 'associated standard operating procedure': 10, 'standard operating procedure in': 74, 'operating procedure in regulatory': 58, 'procedure in regulatory industry': 60, 'it could also involve': 47, 'could also involve creating': 23, 'also involve creating content': 1, 'involve creating content from': 44, 'creating content from scratch': 24, 'content from scratch documentation': 21, 'from scratch documentation should': 37, 'scratch documentation should be': 65, 'documentation should be easy': 31, 'should be easy to': 70, 'be easy to read': 13, 'easy to read and': 33, 'to read and understand': 79, 'read and understand if': 62, 'and understand if it': 7, 'understand if it is': 83, 'if it is too': 40, 'it is too long': 48, 'is too long and': 46, 'too long and too': 80, 'long and too wordy': 51, 'and too wordy it': 5, 'too wordy it may': 81, 'wordy it may be': 87, 'it may be misunderstood': 49, 'may be misunderstood or': 53, 'be misunderstood or ignored': 15, 'misunderstood or ignored clear': 55, 'or ignored clear concise': 59, 'ignored clear concise words': 41, 'clear concise words should': 19, 'concise words should be': 20, 'words should be used': 86, 'should be used and': 72, 'be used and sentences': 16, 'used and sentences should': 84, 'and sentences should be': 4, 'sentences should be limited': 66, 'should be limited to': 71, 'be limited to maximum': 14, 'limited to maximum of': 50, 'to maximum of 15': 78, 'maximum of 15 words': 52, 'of 15 words documentation': 56, '15 words documentation intended': 0, 'words documentation intended for': 85, 'documentation intended for general': 30, 'intended for general audience': 43, 'for general audience should': 35, 'general audience should avoid': 39, 'audience should avoid gender': 11, 'should avoid gender specific': 68, 'avoid gender specific terms': 12, 'gender specific terms and': 38, 'specific terms and cultural': 73, 'terms and cultural biases': 77, 'and cultural biases in': 3, 'cultural biases in series': 25, 'biases in series of': 17, 'in series of procedures': 42, 'series of procedures steps': 67, 'of procedures steps should': 57, 'procedures steps should be': 61, 'steps should be clearly': 75, 'should be clearly numbered': 69}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(4,4))\n",
    "bow = cv.fit_transform([instance_1,instance_2])\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0\n",
      "  1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0\n",
      "  0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0]]\n",
      "[[1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1\n",
      "  0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1\n",
      "  1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow[0].toarray())\n",
    "print(bow[1].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Vi·∫øt t·∫Øt c·ªßa thu·∫≠t ng·ªØ ti·∫øng Anh term frequency ‚Äì inverse document frequency, tf-idf l√† tr·ªçng s·ªë c·ªßa m·ªôt t·ª´ trong vƒÉn b·∫£n thu ƒë∆∞·ª£c qua th·ªëng k√™ th·ªÉ hi·ªán m·ª©c ƒë·ªô quan tr·ªçng c·ªßa t·ª´ n√†y trong m·ªôt vƒÉn b·∫£n, m√† b·∫£n th√¢n vƒÉn b·∫£n ƒëang x√©t n·∫±m trong m·ªôt t·∫≠p h·ª£p c√°c vƒÉn b·∫£n.\n",
    "\n",
    "IDF - Inverse Document Frequency: d√πng ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng m·ª©c ƒë·ªô quan tr·ªçng c·ªßa t·ª´ ƒë√≥ nh∆∞ th·∫ø n√†o . Khi t√≠nh t·∫ßn s·ªë xu·∫•t hi·ªán tf th√¨ c√°c t·ª´ ƒë·ªÅu ƒë∆∞·ª£c coi l√† quan tr·ªçng nh∆∞ nhau. Tuy nhi√™n c√≥ m·ªôt s·ªë t·ª´ th∆∞·ªùng ƒë∆∞·ª£c ƒë∆∞·ª£c s·ª≠ d·ª•ng nhi·ªÅu nh∆∞ng kh√¥ng quan tr·ªçng ƒë·ªÉ th·ªÉ hi·ªán √Ω nghƒ©a c·ªßa ƒëo·∫°n vƒÉn.\n",
    "\n",
    "+ T·ª´ n·ªëi: v√†, nh∆∞ng, tuy nhi√™n, v√¨ th·∫ø, v√¨ v·∫≠y, ‚Ä¶\n",
    "+ Gi·ªõi t·ª´: ·ªü, trong, tr√™n, ‚Ä¶\n",
    "+ T·ª´ ch·ªâ ƒë·ªãnh: ·∫•y, ƒë√≥, nh·ªâ, ‚Ä¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src = \"data:image/png;base64,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\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'the': 9, 'sky': 7, 'is': 4, 'blue': 0, 'sun': 8, 'bright': 1, 'in': 3, 'we': 10, 'can': 2, 'see': 5, 'shining': 6}\n",
      "TF-IDF matrix:\n",
      "[[0.65919112 0.         0.         0.         0.42075315 0.\n",
      "  0.         0.51971385 0.         0.34399327 0.        ]\n",
      " [0.         0.52210862 0.         0.         0.52210862 0.\n",
      "  0.         0.         0.52210862 0.42685801 0.        ]\n",
      " [0.         0.3218464  0.         0.50423458 0.3218464  0.\n",
      "  0.         0.39754433 0.3218464  0.52626104 0.        ]\n",
      " [0.         0.23910199 0.37459947 0.         0.         0.37459947\n",
      "  0.37459947 0.         0.47820398 0.39096309 0.37459947]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# B∆∞·ªõc 1: T·∫°o d·ªØ li·ªáu m·∫´u\n",
    "documents = [\n",
    "    \"The sky is blue.\",\n",
    "    \"The sun is bright.\",\n",
    "    \"The sun in the sky is bright.\",\n",
    "    \"We can see the shining sun, the bright sun.\"\n",
    "]\n",
    "\n",
    "# B∆∞·ªõc 2: Kh·ªüi t·∫°o TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# B∆∞·ªõc 3: T√≠nh to√°n TF-IDF\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# B∆∞·ªõc 4: In k·∫øt qu·∫£\n",
    "# In t·ª´ ƒëi·ªÉn (vocabulary)\n",
    "print(\"Vocabulary:\", vectorizer.vocabulary_)\n",
    "\n",
    "# In ma tr·∫≠n TF-IDF\n",
    "print(\"TF-IDF matrix:\")\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "        Word2Vec l√† m·ªôt k·ªπ thu·∫≠t deeplearning ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t·∫°o ra c√°c vector t·ª´ (word embeddings) t·ª´ vƒÉn b·∫£n. N√≥ c√≥ th·ªÉ gi√∫p bi·ªÉu di·ªÖn c√°c t·ª´ trong m·ªôt kh√¥ng gian vector sao cho c√°c t·ª´ c√≥ ng·ªØ nghƒ©a t∆∞∆°ng t·ª± s·∫Ω n·∫±m g·∫ßn nhau. C√≥ hai m√¥ h√¨nh ch√≠nh trong Word2Vec: CBOW (Continuous Bag of Words) v√† Skip-gram.\n",
    "\n",
    "\n",
    "<image src = \"https://swimm.io/wp-content/webp-express/webp-images/uploads/2023/11/word2vec--1024x559.png.webp\">\n",
    "\n",
    "- CBOW\n",
    "\n",
    "        + CBOW nh·∫±m d·ª± ƒëo√°n t·ª´ hi·ªán t·∫°i d·ª±a tr√™n ng·ªØ c·∫£nh xung quanh (context). N√≥ c·ªë g·∫Øng d·ª± ƒëo√°n t·ª´ trung t√¢m (center word) t·ª´ c√°c t·ª´ xung quanh n√≥ trong m·ªôt c·ª≠a s·ªï ng·ªØ c·∫£nh ƒë√£ ch·ªçn.\n",
    "\n",
    "        + V·ªõi CBOW, c√°c t·ª´ ng·ªØ c·∫£nh (context words) ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ d·ª± ƒëo√°n t·ª´ trung t√¢m. C√°c t·ª´ ng·ªØ c·∫£nh n√†y ƒë∆∞·ª£c t·ªïng h·ª£p th√†nh m·ªôt vector v√† ƒë∆∞a v√†o m·ªôt m√¥ h√¨nh neural network ƒë·ªÉ d·ª± ƒëo√°n t·ª´ trung t√¢m.\n",
    "\n",
    "- Skip-gram\n",
    "\n",
    "        + Skip-gram ng∆∞·ª£c l·∫°i, n√≥ c·ªë g·∫Øng d·ª± ƒëo√°n c√°c t·ª´ ng·ªØ c·∫£nh t·ª´ t·ª´ trung t√¢m.\n",
    "        \n",
    "        + V·ªõi Skip-gram, t·ª´ trung t√¢m ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ d·ª± ƒëo√°n c√°c t·ª´ ng·ªØ c·∫£nh trong c√πng m·ªôt c·ª≠a s·ªï ng·ªØ c·∫£nh ƒë√£ ch·ªçn. M·ªói t·ª´ ng·ªØ c·∫£nh ƒë∆∞·ª£c d·ª± ƒëo√°n nh∆∞ l√† m·ªôt ƒë·∫ßu ra c·ªßa m√¥ h√¨nh.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['love', 'machine', 'learning', 'and', 'natural', 'language', 'processing'], ['word', 'vec', 'is', 'technique', 'used', 'to', 'learn', 'word', 'embeddings', 'from', 'text'], ['deep', 'learning', 'models', 'like', 'word', 'vec', 'have', 'revolutionized', 'nlp']]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Data\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"I love machine learning and natural language processing\",\n",
    "    \"Word2Vec is a technique used to learn word embeddings from text\",\n",
    "    \"Deep learning models like Word2Vec have revolutionized NLP\"\n",
    "]\n",
    "\n",
    "# Bi·∫øn ƒë·ªïi b·ªô data th√†nh c√°c c√¢u g·ªìm c√°c token.\n",
    "tokenized_documents = [simple_preprocess(doc) for doc in documents]\n",
    "print(tokenized_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'machine':\n",
      " [ 7.7001113e-03  9.1245426e-03  1.1359043e-03 -8.3226906e-03\n",
      "  8.4260432e-03 -3.6943862e-03  5.7456936e-03  4.3893624e-03\n",
      "  9.6877608e-03 -9.2952261e-03  9.2084585e-03 -9.2804059e-03\n",
      " -6.9048870e-03 -9.1041336e-03 -5.5459035e-03  7.3677199e-03\n",
      "  9.1675948e-03 -3.3266528e-03  3.7216223e-03 -3.6291070e-03\n",
      "  7.8820419e-03  5.8674002e-03  4.7167180e-07 -3.6311133e-03\n",
      " -7.2259647e-03  4.7707637e-03  1.4498112e-03 -2.6142271e-03\n",
      "  7.8376699e-03 -4.0434687e-03 -9.1530606e-03 -2.2557892e-03\n",
      "  1.2613325e-04 -6.6421628e-03 -5.4879123e-03 -8.4960135e-03\n",
      "  9.2218863e-03  7.4243932e-03 -2.9611582e-04  7.3635504e-03\n",
      "  7.9529136e-03 -7.8370131e-04  6.6126566e-03  3.7732164e-03\n",
      "  5.0779628e-03  7.2530461e-03 -4.7419146e-03 -2.1884560e-03\n",
      "  8.7490311e-04  4.2368053e-03  3.3014184e-03  5.0956826e-03\n",
      "  4.5861360e-03 -8.4437206e-03 -3.1825160e-03 -7.2370791e-03\n",
      "  9.6786767e-03  5.0060940e-03  1.7117348e-04  4.1131866e-03\n",
      " -7.6562283e-03 -6.2979194e-03  3.0745452e-03  6.5360777e-03\n",
      "  3.9518555e-03  6.0205087e-03 -1.9859145e-03 -3.3402655e-03\n",
      "  2.0474885e-04 -3.1980751e-03 -5.5169486e-03 -7.7880397e-03\n",
      "  6.5373844e-03 -1.0890692e-03 -1.8930781e-03 -7.8033716e-03\n",
      "  9.3401987e-03  8.6947152e-04  1.7717265e-03  2.4899272e-03\n",
      " -7.3899282e-03  1.6395028e-03  2.9778772e-03 -8.5680820e-03\n",
      "  4.9596955e-03  2.4320416e-03  7.5027482e-03  5.0445190e-03\n",
      " -3.0294782e-03 -7.1676848e-03  7.0995265e-03  1.9028226e-03\n",
      "  5.1974775e-03  6.3840565e-03  1.9099785e-03 -6.1304872e-03\n",
      " -1.0701035e-05  8.2700634e-03 -6.0969014e-03  9.4400095e-03]\n"
     ]
    }
   ],
   "source": [
    "# ƒê√†o t·∫°o m√¥ h√¨nh Word2Vec\n",
    "model = Word2Vec(sentences=tokenized_documents, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "\"\"\"\n",
    "    M·∫∑c ƒë·ªãnh c·ªßa model l√† d√πng C-bow\n",
    "    ----sg : {0, 1}, optional\n",
    "            Training algorithm: 1 for skip-gram; otherwise CBOW.------\n",
    "    # S·ª≠ d·ª•ng Skip-gram\n",
    "    model = Word2Vec(sentences=tokenized_documents, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "    # Ho·∫∑c s·ª≠ d·ª•ng CBOW\n",
    "    model = Word2Vec(sentences=tokenized_documents, vector_size=100, window=5, min_count=1, workers=4, sg=0, cbow_mean=1)\n",
    "\n",
    "    vector_size: \n",
    "        + M√¥ t·∫£: S·ªë chi·ªÅu c·ªßa vector t·ª´ (word embeddings). Gi√° tr·ªã n√†y quy·∫øt ƒë·ªãnh ƒë·ªô d√†i c·ªßa m·ªói vector t·ª´ ƒë∆∞·ª£c t·∫°o ra b·ªüi m√¥ h√¨nh.\n",
    "        + V√≠ d·ª•: 100 c√≥ nghƒ©a l√† m·ªói t·ª´ s·∫Ω ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·ªüi m·ªôt vector c√≥ 100 chi·ªÅu.\n",
    "\n",
    "    window:\n",
    "        + M√¥ t·∫£: K√≠ch th∆∞·ªõc c·ªßa c·ª≠a s·ªï ng·ªØ c·∫£nh, nghƒ©a l√† s·ªë l∆∞·ª£ng t·ª´ xung quanh m·ªôt t·ª´ m·ª•c ti√™u ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ d·ª± ƒëo√°n t·ª´ ƒë√≥. Gi√° tr·ªã n√†y ·∫£nh h∆∞·ªüng ƒë·∫øn ph·∫°m vi ng·ªØ c·∫£nh m√† m√¥ h√¨nh s·∫Ω xem x√©t.\n",
    "        + V√≠ d·ª•: 5 c√≥ nghƒ©a l√† m√¥ h√¨nh s·∫Ω xem x√©t 5 t·ª´ tr∆∞·ªõc v√† 5 t·ª´ sau t·ª´ m·ª•c ti√™u trong c√¢u.\n",
    "    \n",
    "    min_count:\n",
    "\n",
    "        + M√¥ t·∫£: S·ªë l·∫ßn xu·∫•t hi·ªán t·ªëi thi·ªÉu c·ªßa m·ªôt t·ª´ trong to√†n b·ªô t·∫≠p d·ªØ li·ªáu ƒë·ªÉ t·ª´ ƒë√≥ ƒë∆∞·ª£c ƒë∆∞a v√†o m√¥ h√¨nh. C√°c t·ª´ xu·∫•t hi·ªán √≠t h∆°n gi√° tr·ªã n√†y s·∫Ω b·ªã b·ªè qua.\n",
    "        + V√≠ d·ª•: 1 c√≥ nghƒ©a l√† t·∫•t c·∫£ c√°c t·ª´ xu·∫•t hi·ªán √≠t nh·∫•t m·ªôt l·∫ßn s·∫Ω ƒë∆∞·ª£c ƒë∆∞a v√†o m√¥\n",
    "    \n",
    "    workers:\n",
    "        + M√¥ t·∫£: S·ªë l∆∞·ª£ng lu·ªìng x·ª≠ l√Ω (threads) ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒë√†o t·∫°o m√¥ h√¨nh. ƒêi·ªÅu n√†y c√≥ th·ªÉ gi√∫p tƒÉng t·ªëc qu√° tr√¨nh ƒë√†o t·∫°o b·∫±ng c√°ch s·ª≠ d·ª•ng nhi·ªÅu lu·ªìng x·ª≠ l√Ω song song.\n",
    "        + V√≠ d·ª•: 4 c√≥ nghƒ©a l√† s·ª≠ d·ª•ng 4 lu·ªìng x·ª≠ l√Ω ƒë·ªÉ ƒë√†o t·∫°o m√¥ h√¨nh.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "# L·∫•y vector c·ªßa m·ªôt t·ª´\n",
    "word_vector = model.wv['machine']\n",
    "print(\"Vector for 'machine':\\n\", word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'machine':\n",
      "language: 0.2468\n",
      "the: 0.1784\n",
      "technique: 0.1713\n",
      "processing: 0.1624\n",
      "vec: 0.1607\n"
     ]
    }
   ],
   "source": [
    "# T√¨m c√°c t·ª´ t∆∞∆°ng t·ª±\n",
    "similar_words = model.wv.most_similar('machine', topn=5)\n",
    "print(\"Words similar to 'machine':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '.', 'This', 'is', 'an', 'NLTK', 'example', '.']\n",
      "['Hello world.', 'This is an NLTK example.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"Hello world. This is an NLTK example.\"\n",
    "words = nltk.word_tokenize(text)\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(words)    # ['Hello', 'world', '.', 'This', 'is', 'an', 'NLTK', 'example', '.']\n",
    "print(sentences) # ['Hello world.', 'This is an NLTK example.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"needn't\", 'under', 'through', 'haven', 'and', 'into', \"shouldn't\", 'won', 'herself', 'no', 'after', \"don't\", 'him', 'not', 'now', \"wasn't\", 'll', 'nor', 'about', 'such', 'from', \"you're\", 'yourselves', 'out', 'on', 'my', 'isn', 'but', 'did', 'of', \"hasn't\", 'was', 'm', 'i', 'why', 'shan', 'it', 'over', 'them', 'or', 'hadn', 'in', 'its', \"shan't\", 'because', 'y', 'am', 'below', 'don', 'between', 'what', 'at', 'had', 'few', \"you've\", 'other', \"wouldn't\", 'then', 'themselves', \"that'll\", 'wouldn', \"you'll\", 'will', \"won't\", \"didn't\", 'any', 'here', 're', 'needn', \"haven't\", 'by', 'down', 'wasn', 'theirs', 'is', 'who', 'those', 'having', 'ain', 'mustn', 'himself', 'during', 'doing', 'hasn', 'their', 'has', 'above', 'more', 'd', 'there', 'against', \"aren't\", 'an', \"doesn't\", 'this', 'where', 'so', 'each', 'these', 'can', 'for', 'aren', 'if', 'off', 't', \"she's\", 'we', 'up', 'myself', 'mightn', 'me', 'a', \"should've\", 'most', 'he', 'have', 'once', 'doesn', 'your', 'she', 'until', 'again', 'ours', 'both', 'weren', 'our', 'all', 'than', 'yours', 'the', 'only', 'couldn', \"you'd\", 'are', 'didn', 'being', 'ma', 'which', 'same', 'shouldn', 'as', 'when', 'they', \"mightn't\", \"couldn't\", 'her', 'itself', 'be', \"weren't\", 'some', 'to', 'own', 'yourself', 'very', 'his', 've', 'while', \"isn't\", 'should', 'were', 'hers', 'o', 'how', 'ourselves', 'before', \"mustn't\", 'do', 'further', 'you', 'does', 'been', \"hadn't\", 'that', 's', 'with', 'whom', 'too', \"it's\", 'just'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\"\"\"\n",
    "    available_langs = {\n",
    "        \"catalan\": \"ca\",\n",
    "        \"czech\": \"cs\",\n",
    "        \"german\": \"de\",\n",
    "        \"greek\": \"el\",\n",
    "        \"english\": \"en\",\n",
    "        \"spanish\": \"es\",\n",
    "        \"finnish\": \"fi\",\n",
    "        \"french\": \"fr\",\n",
    "        \"hungarian\": \"hu\",\n",
    "        \"icelandic\": \"is\",\n",
    "        \"italian\": \"it\",\n",
    "        \"latvian\": \"lv\",\n",
    "        \"dutch\": \"nl\",\n",
    "        \"polish\": \"pl\",\n",
    "        \"portuguese\": \"pt\",\n",
    "        \"romanian\": \"ro\",\n",
    "        \"russian\": \"ru\",\n",
    "        \"slovak\": \"sk\",\n",
    "        \"slovenian\": \"sl\",\n",
    "        \"swedish\": \"sv\",\n",
    "        \"tamil\": \"ta\",\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    "print(stop_words) # {'a', 'an', 'the', ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "fli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('running'))  # run\n",
    "print(stemmer.stem('flies'))    # fli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "fly\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('running', pos='v'))  # run\n",
    "print(lemmatizer.lemmatize('flies', pos='n'))    # fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Part-of-Speech Tagging (POS Tagging)\n",
    "\n",
    "Part-of-Speech Tagging (POS Tagging) l√† m·ªôt k·ªπ thu·∫≠t trong x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP) d√πng ƒë·ªÉ g√°n nh√£n t·ª´ lo·∫°i cho c√°c t·ª´ trong m·ªôt vƒÉn b·∫£n. M·ªói t·ª´ trong c√¢u s·∫Ω ƒë∆∞·ª£c g√°n m·ªôt nh√£n t∆∞∆°ng ·ª©ng v·ªõi lo·∫°i t·ª´ c·ªßa n√≥, ch·∫≥ng h·∫°n nh∆∞ danh t·ª´ (noun), ƒë·ªông t·ª´ (verb), t√≠nh t·ª´ (adjective), tr·∫°ng t·ª´ (adverb), v.v.\n",
    "\n",
    "<image src = \"https://images.shiksha.com/mediadata/ugcDocuments/images/wordpressImages/2022_12_POS-Tagging.jpg\">\n",
    "\n",
    "`M·ª•c ƒë√≠ch c·ªßa POS Tagging`\n",
    "+ X√°c ƒë·ªãnh vai tr√≤ t·ª´ trong c√¢u: Gi√∫p hi·ªÉu c√°ch c√°c t·ª´ k·∫øt h·ª£p v·ªõi nhau ƒë·ªÉ t·∫°o th√†nh c√¢u c√≥ nghƒ©a.\n",
    "+ C·∫£i thi·ªán c√°c t√°c v·ª• NLP kh√°c: Cung c·∫•p th√¥ng tin v·ªÅ c·∫•u tr√∫c c√¢u gi√∫p c·∫£i thi·ªán c√°c t√°c v·ª• kh√°c nh∆∞ ph√¢n t√≠ch c√∫ ph√°p, nh·∫≠n di·ªán th·ª±c th·ªÉ, v√† d·ªãch m√°y.\n",
    "\n",
    "`C√°c nh√£n t·ª´ lo·∫°i ph·ªï bi·∫øn`\n",
    "+ `NN` (Noun, singular or mass): Danh t·ª´ s·ªë √≠t ho·∫∑c danh t·ª´ kh√¥ng ƒë·∫øm ƒë∆∞·ª£c.\n",
    "+ `NNS` (Noun, plural): Danh t·ª´ s·ªë nhi·ªÅu.\n",
    "+ `VB` (Verb, base form): ƒê·ªông t·ª´ ·ªü d·∫°ng c∆° b·∫£n.\n",
    "+ `VBD` (Verb, past tense): ƒê·ªông t·ª´ th√¨ qu√° kh·ª©.\n",
    "+ `VBG` (Verb, gerund/present participle): ƒê·ªông t·ª´ d·∫°ng hi·ªán t·∫°i.\n",
    "+ `VBN` (Verb, past participle): ƒê·ªông t·ª´ d·∫°ng qu√° kh·ª© ph√¢n t·ª´.\n",
    "+ `JJ` (Adjective): T√≠nh t·ª´.\n",
    "+ `RB` (Adverb): Tr·∫°ng t·ª´.\n",
    "+ `PRP` (Personal pronoun): ƒê·∫°i t·ª´ c√° nh√¢n (v√≠ d·ª•: he, she, it).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('simple', 'JJ'), ('example', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "text = \"This is a simple example.\"\n",
    "tokens = word_tokenize(text)\n",
    "tagged = pos_tag(tokens)\n",
    "print(tagged)  # [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('simple', 'JJ'), ('example', 'NN')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Named Entity Recognition (NER)\n",
    "\n",
    "Named Entity Recognition (NER) l√† m·ªôt k·ªπ thu·∫≠t trong x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP) d√πng ƒë·ªÉ nh·∫≠n di·ªán c√°c th·ª±c th·ªÉ c·ª• th·ªÉ trong vƒÉn b·∫£n, ch·∫≥ng h·∫°n nh∆∞ t√™n ng∆∞·ªùi, ƒë·ªãa ƒëi·ªÉm, t·ªï ch·ª©c, v√† c√°c th·ª±c th·ªÉ kh√°c.\n",
    "\n",
    "<image src = \"https://www.shaip.com/wp-content/uploads/2022/02/Blog_Named-Entity-Recognition-%E2%80%93-The-Concept-Types-Applications.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  (GPE Hawaii/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "text = \"Barack Obama was born in Hawaii.\"\n",
    "tokens = word_tokenize(text)\n",
    "tagged = pos_tag(tokens)\n",
    "entities = ne_chunk(tagged)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('apple', 3), ('banana', 2), ('orange', 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "text = \"apple banana apple apple orange banana\"\n",
    "tokens = word_tokenize(text)\n",
    "fdist = FreqDist(tokens)\n",
    "print(fdist.most_common())  # [('apple', 3), ('banana', 2), ('orange', 1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Collocations\n",
    "\n",
    "Collocations l√† c√°c c·∫∑p t·ª´ th∆∞·ªùng xu·∫•t hi·ªán c√πng nhau trong vƒÉn b·∫£n, v√† vi·ªác nh·∫≠n di·ªán ch√∫ng c√≥ th·ªÉ gi√∫p hi·ªÉu s√¢u h∆°n v·ªÅ ng·ªØ nghƒ©a v√† c·∫•u tr√∫c c·ªßa vƒÉn b·∫£n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'like'), ('and', 'basketball'), ('basketball', '.'), ('football', 'and'), ('like', 'to'), ('play', 'football'), ('to', 'play')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "text = \"I like to play football and basketball.\"\n",
    "tokens = word_tokenize(text)\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "bigrams = finder.nbest(BigramAssocMeasures.likelihood_ratio, 10)\n",
    "print(bigrams)  # [('', 'football'), ('football', 'and'), ...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Parsing\n",
    "\n",
    "Parsing, trong ng·ªØ nghƒ©a c·ªßa x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP) v√† khoa h·ªçc m√°y t√≠nh, l√† qu√° tr√¨nh ph√¢n t√≠ch m·ªôt chu·ªói t·ª´ (ho·∫∑c c√¢u) ƒë·ªÉ x√°c ƒë·ªãnh c·∫•u tr√∫c ng·ªØ ph√°p c·ªßa n√≥ theo m·ªôt ng·ªØ ph√°p c·ª• th·ªÉ. M·ª•c ti√™u c·ªßa parsing l√† hi·ªÉu c√°ch c√°c th√†nh ph·∫ßn c·ªßa c√¢u k·∫øt h·ª£p v·ªõi nhau ƒë·ªÉ t·∫°o th√†nh m·ªôt c√¢u h·ª£p l·ªá.\n",
    "\n",
    "`Ng·ªØ ph√°p (Grammar): L√† m·ªôt t·∫≠p h·ª£p c√°c quy t·∫Øc x√°c ƒë·ªãnh c√°ch c√°c t·ª´ v√† c·ª•m t·ª´ c√≥ th·ªÉ k·∫øt h·ª£p ƒë·ªÉ t·∫°o th√†nh c√¢u h·ª£p l·ªá. Trong ng·ªØ ph√°p Context-Free Grammar (CFG), c√°c quy t·∫Øc ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a d∆∞·ªõi d·∫°ng c√°c quy t·∫Øc s·∫£n sinh (production rules).`\n",
    "\n",
    "```bash\n",
    "S -> NP VP\n",
    "NP -> DT NN\n",
    "VP -> V NP\n",
    "DT -> 'the' | 'a'\n",
    "NN -> 'man' | 'ball'\n",
    "V -> 'hit'\n",
    "```\n",
    "\n",
    "+ S l√† c√¢u.\n",
    "+ NP l√† c·ª•m danh t·ª´.\n",
    "+ VP l√† c·ª•m ƒë·ªông t·ª´.\n",
    "+ DT l√† t·ª´ ƒë·ªãnh nghƒ©a (article).\n",
    "+ NN l√† danh t·ª´.\n",
    "+ V l√† ƒë·ªông t·ª´.\n",
    "\n",
    "`C√¢y Ph√¢n T√≠ch (Parse Tree): L√† c·∫•u tr√∫c ph√¢n c·∫•p hi·ªÉn th·ªã c√°ch c√°c t·ª´ trong c√¢u ƒë∆∞·ª£c k·∫øt h·ª£p v·ªõi nhau theo c√°c quy t·∫Øc ng·ªØ ph√°p. N√≥ cho th·∫•y c√°c th√†nh ph·∫ßn c√¢u (nh∆∞ c·ª•m danh t·ª´, c·ª•m ƒë·ªông t·ª´) v√† c√°ch ch√∫ng li√™n k·∫øt v·ªõi nhau.`\n",
    "\n",
    "```bash\n",
    "(S\n",
    "  (NP (DT the) (NN man))\n",
    "  (VP (V hit) (NP (DT the) (NN ball))))\n",
    "```\n",
    "\n",
    "`Ph√¢n T√≠ch (Parsing): Qu√° tr√¨nh x√°c ƒë·ªãnh c·∫•u tr√∫c ng·ªØ ph√°p c·ªßa m·ªôt c√¢u b·∫±ng c√°ch √°p d·ª•ng c√°c quy t·∫Øc ng·ªØ ph√°p ƒë·ªÉ t·∫°o ra c√¢y ph√¢n t√≠ch.` C√≥ nhi·ªÅu ph∆∞∆°ng ph√°p parsing kh√°c nhau, bao g·ªìm:\n",
    "\n",
    "+ Top-Down Parsing: B·∫Øt ƒë·∫ßu t·ª´ n√∫t g·ªëc (nh∆∞ S) v√† c·ªë g·∫Øng ph√¢n t√≠ch t·ª´ tr√™n xu·ªëng d∆∞·ªõi b·∫±ng c√°ch √°p d·ª•ng c√°c quy t·∫Øc ng·ªØ ph√°p.\n",
    "+ Bottom-Up Parsing: B·∫Øt ƒë·∫ßu t·ª´ c√°c t·ª´ trong c√¢u v√† c·ªë g·∫Øng x√¢y d·ª±ng c√¢y ph√¢n t√≠ch t·ª´ d∆∞·ªõi l√™n b·∫±ng c√°ch k·∫øt h·ª£p c√°c th√†nh ph·∫ßn nh·ªè th√†nh c√°c th√†nh ph·∫ßn l·ªõn h∆°n.\n",
    "+ Chart Parsing: S·ª≠ d·ª•ng c·∫•u tr√∫c d·ªØ li·ªáu g·ªçi l√† chart ƒë·ªÉ l∆∞u tr·ªØ v√† t√°i s·ª≠ d·ª•ng c√°c ph√¢n t√≠ch trung gian, gi√∫p x·ª≠ l√Ω hi·ªáu qu·∫£ h∆°n.\n",
    "+ ChartParser trong NLTK: NLTK (Natural Language Toolkit) cung c·∫•p m·ªôt c√¥ng c·ª• parsing m·∫°nh m·∫Ω g·ªçi l√† ChartParser ƒë·ªÉ th·ª±c hi·ªán parsing d·ª±a tr√™n ng·ªØ ph√°p CFG.\n",
    "\n",
    "`ChartParser: X√¢y d·ª±ng m·ªôt c·∫•u tr√∫c d·ªØ li·ªáu ƒë·ªÉ l∆∞u tr·ªØ c√°c ph√¢n t√≠ch trung gian v√† tr√°nh vi·ªác ph√¢n t√≠ch l·∫°i c√πng m·ªôt ph·∫ßn c·ªßa c√¢u. ƒêi·ªÅu n√†y gi√∫p c·∫£i thi·ªán hi·ªáu su·∫•t v√† x·ª≠ l√Ω c√°c c√¢u ph·ª©c t·∫°p h∆°n.`\n",
    "+ ChartParser: X√¢y d·ª±ng m·ªôt c·∫•u tr√∫c d·ªØ li·ªáu ƒë·ªÉ l∆∞u tr·ªØ c√°c ph√¢n t√≠ch trung gian v√† tr√°nh vi·ªác ph√¢n t√≠ch l·∫°i c√πng m·ªôt ph·∫ßn c·ªßa c√¢u. ƒêi·ªÅu n√†y gi√∫p c·∫£i thi·ªán hi·ªáu su·∫•t v√† x·ª≠ l√Ω c√°c c√¢u ph·ª©c t·∫°p h∆°n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (DT the) (NN man)) (VP (V hit) (NP (DT the) (NN ball))))\n"
     ]
    }
   ],
   "source": [
    "from nltk import CFG, ChartParser\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  NP -> DT NN\n",
    "  VP -> V NP\n",
    "  DT -> 'the' | 'a'\n",
    "  NN -> 'man' | 'ball'\n",
    "  V -> 'hit'\n",
    "\"\"\")\n",
    "parser = ChartParser(grammar)\n",
    "sentence = 'the man hit the ball'.split()\n",
    "for tree in parser.parse(sentence):\n",
    "    print(tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. WordNet Interface\n",
    "\n",
    "+ WordNet l√† m·ªôt c∆° s·ªü d·ªØ li·ªáu t·ª´ v·ª±ng m·∫°nh m·∫Ω cho ti·∫øng Anh, ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i trong x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n ƒë·ªÉ tra c·ª©u t·ª´ ƒë·ªìng nghƒ©a, t·ª´ tr√°i nghƒ©a, v√† c√°c m·ªëi quan h·ªá ng·ªØ nghƒ©a kh√°c. Th∆∞ vi·ªán NLTK cung c·∫•p m·ªôt giao di·ªán ƒë·ªÉ l√†m vi·ªác v·ªõi WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('good.n.01'), Synset('good.n.02'), Synset('good.n.03'), Synset('commodity.n.01'), Synset('good.a.01'), Synset('full.s.06'), Synset('good.a.03'), Synset('estimable.s.02'), Synset('beneficial.s.01'), Synset('good.s.06'), Synset('good.s.07'), Synset('adept.s.01'), Synset('good.s.09'), Synset('dear.s.02'), Synset('dependable.s.04'), Synset('good.s.12'), Synset('good.s.13'), Synset('effective.s.04'), Synset('good.s.15'), Synset('good.s.16'), Synset('good.s.17'), Synset('good.s.18'), Synset('good.s.19'), Synset('good.s.20'), Synset('good.s.21'), Synset('well.r.01'), Synset('thoroughly.r.02')]\n",
      "[Lemma('good.n.01.good')]\n",
      "benefit\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonyms = wordnet.synsets('good')\n",
    "print(synonyms)  # [Synset('good.n.01'), Synset('good.a.01'), ...]\n",
    "print(synonyms[0].lemmas())  # [Lemma('good.n.01.good'), ...]\n",
    "print(synonyms[0].definition())  # 'that which is good'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
