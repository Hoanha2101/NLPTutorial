{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGEX - REGular EXpression\n",
    "\n",
    "Link youtube giới thiệu và hướng dẫn khá hay về lí thuyết: https://www.youtube.com/watch?v=YGpWSC8Kxvk&list=LL&index=7&t=1s\n",
    "\n",
    "Link trang web thực hành: https://regex101.com/\n",
    "\n",
    "Trang web python:\n",
    "https://www.geeksforgeeks.org/regular-expression-python-examples/\n",
    "\n",
    "### Tóm tắt\n",
    "\n",
    "Regex bao gồm 2 phần chính:\n",
    "\n",
    "/ab + c/g      (- Ngăn cách giữa 2 phần của regex là 2 kí tự gạch chéo)\n",
    "\n",
    "Trong đó: \n",
    "+ ab + c: pattern\n",
    "+ g: flag\n",
    "\n",
    "    Với flag:\n",
    "    \n",
    "        . /g: Cho phép tìm kiếm ở phạm vi toàn cục (global search)\n",
    "        . /i: cho phép tìm kiếm mà không cần phân biệt chữ hoa hay chữ thường. (case-insensitive search)\n",
    "        . /m: cho phép tìm kiếm trên nhiều dòng (multi-line search)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nguồn của các bài của Regex ví dụ:** https://www.geeksforgeeks.org/regular-expression-python-examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(34, 40), match='portal'>\n",
      "Start Index: 34\n",
      "End Index: 40\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "s = 'GeeksforGeeks: A computer science portal for geeks'\n",
    "\n",
    "# tìm kiếm từ \"portal\"\n",
    "\n",
    "match = re.search(r'portal', s) \n",
    "print(match)\n",
    "print('Start Index:', match.start()) \n",
    "print('End Index:', match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='g'>\n",
      "<re.Match object; span=(5, 6), match='.'>\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "  \n",
    "s = 'geeks.forgeeks'\n",
    "  \n",
    "# without using \\ \n",
    "match = re.search(r'.', s) \n",
    "print(match)\n",
    "\n",
    "# using \\ \n",
    "match = re.search(r'\\.', s) \n",
    "print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  [] – Dấu ngoặc vuông\n",
    "\n",
    "+ Dấu ngoặc vuông ([]) đại diện cho một lớp ký tự bao gồm một tập hợp các ký tự mà chúng ta muốn khớp. Ví dụ: lớp ký tự [abc] sẽ khớp với bất kỳ a, b hoặc c nào. \n",
    "\n",
    "+ Chúng ta cũng có thể chỉ định một phạm vi ký tự bằng cách sử dụng – bên trong dấu ngoặc vuông. Ví dụ, \n",
    "\n",
    "[0, 3] là mẫu dưới dạng [0123]\n",
    "[ac] giống như [abc]\n",
    "+ Chúng ta cũng có thể đảo ngược lớp ký tự bằng cách sử dụng ký hiệu dấu mũ(^). Ví dụ, \n",
    "\n",
    "[^0-3] có nghĩa là bất kỳ số nào ngoại trừ 0, 1, 2 hoặc 3\n",
    "[^ac] có nghĩa là bất kỳ ký tự nào ngoại trừ a, b hoặc c\n",
    "+ Ví dụ:\n",
    "    Trong mã này, bạn đang sử dụng biểu thức chính quy để tìm tất cả các ký tự trong chuỗi nằm trong phạm vi từ 'a' đến 'm'. Hàm re.findall()trả về danh sách tất cả các ký tự như vậy. Trong chuỗi đã cho, các ký tự khớp với mẫu này là: 'c', 'k', 'b', 'f', 'j', 'e', ​​'h', 'l', 'd', ' g'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(1, 2), match='h'>\n",
      "\n",
      "['h', 'e', 'i', 'c', 'k', 'b', 'f', 'j', 'm', 'e', 'h', 'e', 'l', 'a', 'd', 'g']\n",
      "['T', ' ', 'q', 'u', ' ', 'r', 'o', 'w', 'n', ' ', 'o', 'x', ' ', 'u', 'p', 's', ' ', 'o', 'v', 'r', ' ', 't', ' ', 'z', 'y', ' ', 'o']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "  \n",
    "string = \"The quick brown fox jumps over the lazy dog\"\n",
    "pattern = \"[a-m]\"\n",
    "result = re.search(pattern, string) # search: trong trường hợp này sẽ trả lại index kết quả tìm kiếm của kí tự đầu tiên của kết quả findall\n",
    "print(result)\n",
    "print()\n",
    "result = re.findall(pattern, string) # trả ra tất cả các kí tự nằm trong khoảng [a - m]\n",
    "print(result)\n",
    "\n",
    "pattern_sub= \"[^a-m]\" # Dùng kí tự ^ trong pattern\n",
    "result = re.findall(pattern_sub, string) # trả ra kết quả nằm ngoài [a-m]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ^ – Dấu mũ\n",
    "Biểu tượng dấu mũ (^) khớp với phần đầu của chuỗi, tức là kiểm tra xem chuỗi có bắt đầu bằng (các) ký tự đã cho hay không. Ví dụ -  \n",
    "\n",
    "+ ^g sẽ kiểm tra xem chuỗi có bắt đầu bằng g hay không, chẳng hạn như geeks, Globe, girl, g, v.v.\n",
    "\n",
    "+ ^ge sẽ kiểm tra xem chuỗi có bắt đầu bằng ge hay không, chẳng hạn như geeks, geeksforgeeks, v.v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: The quick brown fox\n",
      "Matched: The lazy dog\n",
      "Not matched: A quick brown fox\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "regex = r'^The'\n",
    "strings = ['The quick brown fox', 'The lazy dog', 'A quick brown fox'] \n",
    "for string in strings: \n",
    "    if re.match(regex, string):  # Sử dụng match trong re để thực hiện việc so khớp\n",
    "        print(f'Matched: {string}') \n",
    "    else: \n",
    "        print(f'Not matched: {string}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ – Đô la\n",
    "Ký hiệu Dollar($) khớp với phần cuối của chuỗi, tức là kiểm tra xem chuỗi có kết thúc bằng (các) ký tự đã cho hay không. Ví dụ-\n",
    "\n",
    "+ s$ sẽ kiểm tra chuỗi kết thúc bằng a chẳng hạn như computer geek, end, s, v.v.\n",
    "\n",
    "+ ks$ sẽ kiểm tra chuỗi kết thúc bằng ks như geeks, geeksforgeeks, ks, v.v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(6, 12), match='World!'>\n",
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "# Không dùng match cho $, dùng search\n",
    "\n",
    "string = \"Hello World!\"\n",
    "pattern = r\"World!$\"\n",
    "  \n",
    "match = re.search(pattern, string) \n",
    "print(match)\n",
    "if match: \n",
    "    print(\"Match found!\") \n",
    "else: \n",
    "    print(\"Match not found.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## . – Chấm\n",
    "\n",
    "Biểu tượng dấu chấm(.) chỉ khớp với một ký tự duy nhất ngoại trừ ký tự dòng mới (\\n). Ví dụ -  \n",
    "\n",
    "+ ab sẽ kiểm tra chuỗi có chứa bất kỳ ký tự nào ở vị trí dấu chấm như acb, acbd, abbb, v.v.\n",
    "\n",
    "+ .. sẽ kiểm tra xem chuỗi có chứa ít nhất 2 ký tự không"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "  \n",
    "string = \"The quick brown fox jumps over the lazy dog.\"\n",
    "pattern = r\"brown.fox\"\n",
    "\n",
    "# Giải thích: \n",
    "\"\"\"\n",
    "    ...brown fox...\n",
    "    ...brown0fox...\n",
    "    ...brown-fox...\n",
    "        ...\n",
    "    \n",
    "    Biểu thức pattern = r\"brown.fox\" có dấu chấm, tức nó chấp nhận bất kì\n",
    "                            lí tự nào nằm giữ brown và fox ngoại từ kí tự xuống hàng /n (dấu xuống dòng)\n",
    "\"\"\"\n",
    "\n",
    "match = re.search(pattern, string) \n",
    "if match: \n",
    "    print(\"Match found!\") \n",
    "else: \n",
    "    print(\"Match not found.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## | - Hoặc\n",
    "\n",
    "Ký hiệu hoặc hoạt động như toán tử hoặc nghĩa là nó kiểm tra xem mẫu trước hay sau ký hiệu hoặc có trong chuỗi hay không. Ví dụ -  \n",
    "\n",
    "+ a|b sẽ khớp với bất kỳ chuỗi nào chứa a hoặc b, chẳng hạn như acd, bcd, abcd, v.v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "  \n",
    "string = \"The quick brown fox jumps over the lazy dog.\"\n",
    "pattern = r\"brown|mmmm\" \n",
    "\n",
    "match = re.search(pattern, string) \n",
    "if match: \n",
    "    print(\"Match found!\") \n",
    "else: \n",
    "    print(\"Match not found.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ? - Dấu chấm hỏi\n",
    "\n",
    "Dấu chấm hỏi (?) là một bộ định lượng trong các biểu thức chính quy cho biết phần tử trước đó phải được so khớp bằng 0 hoặc một lần. Nó cho phép bạn chỉ định rằng phần tử này là tùy chọn, nghĩa là nó có thể xảy ra một lần hoặc không xảy ra lần nào. Ví dụ,\n",
    "\n",
    "+ ab?c sẽ khớp với chuỗi ac, acb, dabc nhưng sẽ không khớp với chuỗi abbc vì có hai chuỗi b. Tương tự, nó sẽ không khớp với abdc vì b không được theo sau bởi c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match not found.\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "string = \"The quick brown fox jumps over the lazy dog.\"\n",
    "pattern = r\"br?wn\" \n",
    "\n",
    "match = re.search(pattern, string) \n",
    "\n",
    "if match: \n",
    "    print(\"Match found!\") \n",
    "else: \n",
    "    print(\"Match not found.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giải thích nguyên nhân không tìm thấy:\n",
    "\n",
    "### Phân tích mẫu regex r\"br?wn\"\n",
    "    b: Ký tự b.\n",
    "    r?: Ký tự r là tùy chọn (có thể xuất hiện hoặc không xuất hiện).\n",
    "    wn: Ký tự w tiếp theo là n.\n",
    "\n",
    "### Cách regex hoạt động\n",
    "+ Ký tự ? trong regex có nghĩa là ký tự trước nó (trong trường hợp này là r) là tùy chọn, nghĩa là có thể xuất hiện hoặc không xuất hiện. Vì vậy, mẫu r\"br?wn\" sẽ khớp với hai mẫu con:\n",
    "\n",
    "    bwn (nếu r không xuất hiện)\n",
    "    brwn (nếu r xuất hiện)\n",
    "\n",
    "### Chuỗi \"The quick brown fox jumps over the lazy dog.\"\n",
    "+ Trong chuỗi này, chúng ta có từ \"brown\", nhưng từ này không khớp với mẫu r\"br?wn\":\n",
    "\n",
    "    brown: Có r, không khớp với bwn\n",
    "    brown: Khớp với brwn, nhưng chúng ta đang kiểm tra sự xuất hiện của r.\n",
    "\n",
    ">=> Như vậy, từ \"brown\" không khớp với mẫu r\"br?wn\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "string = \"The quick brown fox jumps over the lazy dog.\"\n",
    "pattern = r\"r?w\" # có r hoặc không có r\n",
    "\n",
    "match = re.search(pattern, string) \n",
    "\n",
    "if match: \n",
    "    print(\"Match found!\") \n",
    "else: \n",
    "    print(\"Match not found.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * – Ngôi sao\n",
    "\n",
    "Biểu tượng dấu sao (*) khớp với 0 hoặc nhiều lần xuất hiện của biểu thức chính quy trước ký hiệu *. Ví dụ -  \n",
    "\n",
    "+ ab*c sẽ được so khớp với chuỗi ac, abc, abbbc, dabc, v.v. nhưng sẽ không khớp với chuỗi abdc vì b không được theo sau bởi c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "string = \"The quick brown fox jumps over the lazy dog.\"\n",
    "pattern = r\"r*wn\" # số lần xuất hiện của r là 0 hoặc lớn hơn 0 và theo sau bởi wn\n",
    "\n",
    "match = re.search(pattern, string) \n",
    "\n",
    "if match: \n",
    "    print(\"Match found!\") \n",
    "else: \n",
    "    print(\"Match not found.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + – Cộng\n",
    "Biểu tượng dấu cộng (+) khớp với một hoặc nhiều lần xuất hiện của biểu thức chính quy trước biểu tượng +. Ví dụ -  \n",
    "\n",
    "+ ab+c sẽ khớp với chuỗi abc, abbc, dabc, nhưng sẽ không khớp với chuỗi ac, abdc, vì không có b trong ac và b, không có c trong abdc theo sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "string = \"The quick brown fox jummmmmps over the lazy dog.\"\n",
    "pattern = r\"um+p\" # số lần xuất hiện của r là 0 hoặc lớn hơn 0 và theo sau bởi wn\n",
    "\n",
    "match = re.search(pattern, string) \n",
    "\n",
    "if match: \n",
    "    print(\"Match found!\") \n",
    "else: \n",
    "    print(\"Match not found.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## {m, n} – Braces\n",
    "\n",
    "Các dấu ngoặc nhọn khớp với mọi lần lặp lại trước biểu thức chính quy từ m đến n, bao gồm cả hai. Ví dụ -  \n",
    "\n",
    "+ a{2, 4} sẽ khớp với chuỗi aaab, baaaac, gaad, nhưng sẽ không khớp với các chuỗi như abc, bc vì chỉ có một a hoặc không có a trong cả hai trường hợp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(1, 9), match='oooooooo'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "string = \"goooooooogle in Vietnam\"\n",
    "\n",
    "pattern = r\"o{2,10}\"\n",
    "\n",
    "FIND = re.search(pattern, string)\n",
    "print(FIND)\n",
    "\n",
    "pattern = r\"V{2,10}\"\n",
    "\n",
    "FIND = re.search(pattern, string)\n",
    "print(FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (\\<regex>) – Nhóm\n",
    "Ký hiệu nhóm được sử dụng để nhóm các mẫu phụ. Ví dụ -  \n",
    "\n",
    "+ (a|b)cd sẽ khớp với các chuỗi như acd, abcd, gacd, v.v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(16, 19), match='Vie'>\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "string = \"goooooooogle in Vietnam\"\n",
    "\n",
    "pattern = r\"(V|W)ie\"\n",
    "\n",
    "FIND = re.search(pattern, string)\n",
    "print(FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\A - Matches if the string begins with the given character\n",
    "Khớp nếu chuỗi bắt đầu bằng ký tự đã cho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n",
      "Match not found.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"\\Afor\"\n",
    "text1 = \"for geeks\"\n",
    "text2 = \"geeks for\"\n",
    "\n",
    "match1 = re.search(pattern, text1)\n",
    "match2 = re.search(pattern, text2)\n",
    "\n",
    "print(\"Match found!\" if match1 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match2 else \"Match not found.\")  # Match not found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\b - Matches if the word begins or ends with the given character\n",
    "\n",
    "Khớp nếu từ bắt đầu hoặc kết thúc bằng ký tự đã cho. \n",
    "+ \\b(string) sẽ kiểm tra phần đầu của từ.\n",
    "+ (string)\\b sẽ kiểm tra phần cuối của từ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 2), match='ge'>\n",
      "<re.Match object; span=(0, 2), match='ge'>\n",
      "<re.Match object; span=(3, 5), match='ge'>\n",
      "Match found!\n",
      "Match found!\n",
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern1 = r\"\\bge\"\n",
    "pattern2 = r\"ge\\b\"\n",
    "text1 = \"geeks\"\n",
    "text2 = \"get\"\n",
    "text3 = \"forge\"\n",
    "\n",
    "match1 = re.search(pattern1, text1)\n",
    "print(match1)\n",
    "match2 = re.search(pattern1, text2)\n",
    "print(match2)\n",
    "match3 = re.search(pattern2, text3)\n",
    "print(match3)\n",
    "\n",
    "print(\"Match found!\" if match1 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match2 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match3 else \"Match not found.\")  # Match found!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\B - Opposite of \\b, should not start or end with the given regex\n",
    "Nó ngược lại với \\b tức là chuỗi không được bắt đầu hoặc kết thúc bằng biểu thức chính quy đã cho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n",
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"\\Bge\"\n",
    "text1 = \"together\"\n",
    "text2 = \"forge\"\n",
    "\n",
    "match1 = re.search(pattern, text1)\n",
    "match2 = re.search(pattern, text2)\n",
    "\n",
    "print(\"Match found!\" if match1 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match2 else \"Match not found.\")  # Match not found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\d - Matches any decimal digit\n",
    "Khớp với bất kỳ chữ số thập phân nào, điều này tương đương với lớp đã đặt [0-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n",
      "Match found!\n",
      "Match not found.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"\\d\"\n",
    "text1 = \"123\"\n",
    "text2 = \"gee1\"\n",
    "text3 = \"gee\"\n",
    "\n",
    "match1 = re.search(pattern, text1)\n",
    "match2 = re.search(pattern, text2)\n",
    "match3 = re.search(pattern, text3)\n",
    "\n",
    "print(\"Match found!\" if match1 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match2 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match3 else \"Match not found.\")  # Match not found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\D - Matches any non-digit character\n",
    "Khớp với bất kỳ ký tự không có chữ số nào, điều này tương đương với lớp đã đặt [^0-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n",
      "Match found!\n",
      "Match not found.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"\\D\"\n",
    "text1 = \"geeks\"\n",
    "text2 = \"geek1131\"\n",
    "text3 = \"1131\"\n",
    "\n",
    "match1 = re.search(pattern, text1)\n",
    "match2 = re.search(pattern, text2)\n",
    "match3 = re.search(pattern, text3)\n",
    "\n",
    "print(\"Match found!\" if match1 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match2 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match3 else \"Match not found.\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\s - Matches any whitespace character\n",
    "Khớp với bất kỳ ký tự khoảng trắng nào."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n",
      "Match found!\n",
      "Match not found.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"\\s\"\n",
    "text1 = \"gee ks\"\n",
    "text2 = \"a bc a\"\n",
    "text3 = \"msmsmdm\"\n",
    "\n",
    "match1 = re.search(pattern, text1)\n",
    "match2 = re.search(pattern, text2)\n",
    "match3 = re.search(pattern, text3)\n",
    "\n",
    "print(\"Match found!\" if match1 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match2 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match3 else \"Match not found.\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\S - Matches any non-whitespace character\n",
    "Khớp với bất kỳ ký tự không phải khoảng trắng nào"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n",
      "Match found!\n",
      "Match not found.\n",
      "Match not found.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"\\S\"\n",
    "text1 = \"a bd\"\n",
    "text2 = \"abcd\"\n",
    "text3 = \"\"\n",
    "text4 = \" \"\n",
    "\n",
    "match1 = re.search(pattern, text1)\n",
    "match2 = re.search(pattern, text2)\n",
    "match3 = re.search(pattern, text3)\n",
    "match4 = re.search(pattern, text4)\n",
    "\n",
    "print(\"Match found!\" if match1 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match2 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match3 else \"Match not found.\")\n",
    "print(\"Match found!\" if match4 else \"Match not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\w - Matches any alphanumeric character\n",
    "Khớp với bất kỳ ký tự chữ và số nào, ký tự này tương đương với lớp [a-zA-Z0-9_].\n",
    "\n",
    "Tức là chuỗi có thể bao gồm `chữ thường`, `chữ hoa`, và `số`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n",
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"\\w\"\n",
    "text1 = \"123\"\n",
    "text2 = \"geeKs4\"\n",
    "\n",
    "match1 = re.search(pattern, text1)\n",
    "match2 = re.search(pattern, text2)\n",
    "\n",
    "print(\"Match found!\" if match1 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match2 else \"Match not found.\")  # Match found!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\W - Matches any non-alphanumeric character\n",
    "Khớp với bất kỳ ký tự không phải chữ và số nào."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n",
      "Match found!\n",
      "Match not found.\n",
      "Match not found.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"\\W\"\n",
    "text1 = \">$\"\n",
    "text2 = \"gee<>\"\n",
    "text3 = \"123\"\n",
    "text4 = \"gee\"\n",
    "\n",
    "match1 = re.search(pattern, text1)\n",
    "match2 = re.search(pattern, text2)\n",
    "match3 = re.search(pattern, text3)\n",
    "match4 = re.search(pattern, text4)\n",
    "\n",
    "print(\"Match found!\" if match1 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match2 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match3 else \"Match not found.\")\n",
    "print(\"Match found!\" if match4 else \"Match not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\Z - Matches if the string ends with the given regex\n",
    "Khớp nếu chuỗi kết thúc bằng biểu thức chính quy đã cho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n",
      "Match found!\n",
      "Match not found.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"ab\\Z\"\n",
    "text1 = \"abcdab\"\n",
    "text2 = \"abababab\"\n",
    "text3 = \"abababcm\"\n",
    "\n",
    "match1 = re.search(pattern, text1)\n",
    "match2 = re.search(pattern, text2)\n",
    "match3 = re.search(pattern, text3)\n",
    "\n",
    "print(\"Match found!\" if match1 else \"Match not found.\")  # Match found!\n",
    "print(\"Match found!\" if match2 else \"Match not found.\")  # Match not found.\n",
    "print(\"Match found!\" if match3 else \"Match not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re.findall()\n",
    "Trả về tất cả các mẫu khớp không trùng nhau trong chuỗi dưới dạng danh sách các chuỗi. Chuỗi được quét từ trái sang phải và kết quả khớp được trả về theo thứ tự tìm thấy.\n",
    "\n",
    "Tìm tất cả các lần xuất hiện của một mẫu \n",
    "\n",
    "Mã này sử dụng regular expression (\\d+) để tìm tất cả các chuỗi của một hoặc nhiều chữ số trong chuỗi đã cho. Nó tìm kiếm các giá trị số và lưu trữ chúng trong một danh sách. Trong ví dụ này, nó tìm và in các số “123456789” và “987654321” từ chuỗi đầu vào.\\d+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['123456789', '987654321']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re \n",
    "string = \"\"\"Hello my Number is 123456789 and \n",
    "            my friend's number is 987654321\"\"\"\n",
    "regex = '\\d+'\n",
    "  \n",
    "match = re.findall(regex, string) \n",
    "print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re.compile()\n",
    "\n",
    "Biểu thức chính quy được biên dịch thành các đối tượng mẫu, có các phương thức cho nhiều thao tác khác nhau như tìm kiếm kết quả khớp mẫu hoặc thực hiện thay thế chuỗi. \n",
    "\n",
    "Ví dụ 1:\n",
    "\n",
    "Mã này sử dụng mẫu biểu thức chính quy để tìm và liệt kê tất cả các chữ cái viết thường từ 'a' đến 'e' trong chuỗi đầu vào “Đúng vậy, ông Gibenson Stark nói”. Đầu ra sẽ là , là các ký tự phù hợp. [a-e]['e', 'a', 'd', 'b', 'e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'a', 'd', 'b', 'e', 'a']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "p = re.compile('[a-e]') \n",
    "\n",
    "print(p.findall(\"Aye, said Mr. Gibenson Stark\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiểu đầu ra: \n",
    "\n",
    "+ Lần xuất hiện đầu tiên là 'e' trong \"Aye\" chứ không phải 'A', vì nó phân biệt chữ hoa chữ thường.\n",
    "\n",
    "+ Lần xuất hiện tiếp theo là 'a' trong \"said\", sau đó là 'd' trong \"said\", tiếp theo là 'b' và 'e' trong \"Gibenson\", 'a' cuối cùng trùng khớp với \"Stark\".\n",
    "\n",
    "+ Dấu gạch chéo ngược siêu ký tự '\\' có vai trò rất quan trọng vì nó báo hiệu các chuỗi khác nhau. Nếu dấu gạch chéo ngược được sử dụng mà không có ý nghĩa đặc biệt như siêu ký tự, hãy sử dụng'\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ 2: Đặt lớp [\\s,.] sẽ khớp với bất kỳ ký tự khoảng trắng nào, ',' hoặc, '.' . \n",
    "\n",
    "+ Mã này sử dụng các biểu thức chính quy để tìm và liệt kê tất cả các chữ số đơn và chuỗi chữ số trong chuỗi đầu vào đã cho. Nó tìm thấy các chữ số đơn với và chuỗi các chữ số với .\\d \\d+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '1', '4', '1', '8', '8', '6']\n",
      "['11', '4', '1886']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "p = re.compile('\\d') \n",
    "print(p.findall(\"I went to him at 11 A.M. on 4th July 1886\")) \n",
    "  \n",
    "p = re.compile('\\d+') \n",
    "print(p.findall(\"I went to him at 11 A.M. on 4th July 1886\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ 3:\n",
    "\n",
    "Mã này sử dụng các biểu thức chính quy để tìm và liệt kê các ký tự từ, chuỗi ký tự từ và ký tự không phải từ trong chuỗi đầu vào. Nó cung cấp danh sách các ký tự hoặc chuỗi phù hợp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 's', 'a', 'i', 'd', 'i', 'n', 's', 'o', 'm', 'e', '_', 'l', 'a', 'n', 'g']\n",
      "\n",
      "['I', 'went', 'to', 'him', 'at', '11', 'A', 'M', 'he', 'said', 'in', 'some_language']\n",
      "\n",
      "[' ', ' ', '*', '*', '*', ' ', ' ', '.']\n",
      "\n",
      "[' ', ' *** ', ' ', '.']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "  \n",
    "p = re.compile('\\w') \n",
    "print(p.findall(\"He said * in some_lang.\")) \n",
    "\n",
    "print()\n",
    "p = re.compile('\\w+') \n",
    "print(p.findall(\"I went to him at 11 A.M., he said *** in some_language.\")) \n",
    "\n",
    "print()\n",
    "p = re.compile('\\W') \n",
    "print(p.findall(\"he said *** in some_language.\")) \n",
    "\n",
    "print()\n",
    "p = re.compile('\\W+') \n",
    "print(p.findall(\"he said *** in some_language.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ 4:\n",
    "\n",
    "Mã này sử dụng mẫu biểu thức chính quy 'ab*' để tìm và liệt kê tất cả các lần xuất hiện của 'ab', theo sau là 0 hoặc nhiều ký tự 'b' trong chuỗi đầu vào “ababbaabbb”. Nó trả về danh sách trùng khớp sau: ['ab', 'abb', 'abbb']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'abb', 'a', 'abbb']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nHiểu đầu ra: \\n\\nRE của chúng tôi là ab*, mà 'a' đi kèm với bất kỳ số nào. của 'b', bắt đầu từ 0.\\nĐầu ra 'ab', hợp lệ vì có một 'a' đi kèm với một 'b'.\\nĐầu ra 'abb', hợp lệ vì có một 'a' kèm theo 2 'b'.\\nĐầu ra 'a', hợp lệ vì có một 'a' kèm theo 0 'b'.\\nĐầu ra 'abbb', hợp lệ vì có một 'a' kèm theo 3 '\\n\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "p = re.compile('ab*') \n",
    "print(p.findall(\"ababbaabbb\")) \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Hiểu đầu ra: \n",
    "\n",
    "RE của chúng tôi là ab*, mà 'a' đi kèm với bất kỳ số nào. của 'b', bắt đầu từ 0.\n",
    "Đầu ra 'ab', hợp lệ vì có một 'a' đi kèm với một 'b'.\n",
    "Đầu ra 'abb', hợp lệ vì có một 'a' kèm theo 2 'b'.\n",
    "Đầu ra 'a', hợp lệ vì có một 'a' kèm theo 0 'b'.\n",
    "Đầu ra 'abbb', hợp lệ vì có một 'a' kèm theo 3 '\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re.split() \n",
    "Chia chuỗi theo số lần xuất hiện của một ký tự hoặc mẫu, khi tìm thấy mẫu đó, các ký tự còn lại trong chuỗi sẽ được trả về như một phần của danh sách kết quả. \n",
    "\n",
    "+ re.split(mẫu, chuỗi, maxsplit=0, flags=0)\n",
    "\n",
    "+ Tham số đầu tiên, mẫu biểu thị biểu thức chính quy, chuỗi là chuỗi đã cho trong đó mẫu sẽ được tìm kiếm và trong đó xảy ra sự phân tách, maxsplit nếu không được cung cấp sẽ được coi là bằng 0 '0' và nếu bất kỳ giá trị nào khác 0 được cung cấp thì nhiều nhất là có nhiều sự chia rẽ xảy ra. Nếu maxsplit = 1 thì chuỗi sẽ chỉ phân tách một lần, dẫn đến danh sách có độ dài 2. Các cờ rất hữu ích và có thể giúp rút ngắn mã, chúng không phải là tham số cần thiết, ví dụ: flags = re.IGNORECASE, trong phần phân chia này , trường hợp, tức là chữ thường hoặc chữ hoa sẽ bị bỏ qua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Words', 'words', 'Words']\n",
      "['Word', 's', 'words', 'Words']\n",
      "['On', '12th', 'Jan', '2016', 'at', '11', '02', 'AM']\n",
      "['On ', 'th Jan ', ', at ', ':', ' AM']\n"
     ]
    }
   ],
   "source": [
    "from re import split \n",
    "  \n",
    "print(split('\\W+', 'Words, words , Words')) \n",
    "print(split('\\W+', \"Word's words Words\")) \n",
    "print(split('\\W+', 'On 12th Jan 2016, at 11:02 AM')) \n",
    "print(split('\\d+', 'On 12th Jan 2016, at 11:02 AM')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ 2:\n",
    "\n",
    "Câu lệnh đầu tiên chia chuỗi ở lần xuất hiện đầu tiên của một hoặc nhiều chữ số: ['On ', 'th Jan 2016, at 11:02 AM']. thứ hai phân tách chuỗi bằng cách sử dụng các chữ cái viết thường từ a đến f làm dấu phân cách, không phân biệt chữ hoa chữ thường: . Thứ ba chia chuỗi bằng cách sử dụng các chữ cái viết thường từ a đến f làm dấu phân cách, phân biệt chữ hoa chữ thường: . ['', 'y, ', 'oy oh ', 'oy, ', 'ome here']['', 'ey, Boy oh ', 'oy, ', 'ome here']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On ', 'th Jan 2016, at 11:02 AM']\n",
      "['', 'y, ', 'oy oh ', 'oy, ', 'om', ' h', 'r', '']\n",
      "['A', 'y, Boy oh ', 'oy, ', 'om', ' h', 'r', '']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "print(re.split('\\d+', 'On 12th Jan 2016, at 11:02 AM', 1)) \n",
    "print(re.split('[a-f]+', 'Aey, Boy oh boy, come here', flags=re.IGNORECASE)) \n",
    "print(re.split('[a-f]+', 'Aey, Boy oh boy, come here')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re.sub() \n",
    "‘sub’ trong hàm là viết tắt của SubString, một mẫu biểu thức chính quy nhất định được tìm kiếm trong chuỗi đã cho (tham số thứ 3) và khi tìm thấy mẫu chuỗi con được thay thế bằng repl (tham số thứ 2), kiểm tra số lần và duy trì số lần điều này xảy ra. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ 1:\n",
    "\n",
    "+ Câu lệnh đầu tiên thay thế tất cả các lần xuất hiện của 'ub' bằng '~*' (không phân biệt chữ hoa chữ thường): 'S~*ject has ~*er booked already'.\n",
    "+ Câu lệnh thứ hai thay thế tất cả các lần xuất hiện của 'ub' bằng '~*' (phân biệt chữ hoa chữ thường): 'S~*ject has Uber booked already'.\n",
    "+ Câu lệnh thứ ba thay thế lần xuất hiện đầu tiên của 'ub' bằng '~*' (không phân biệt chữ hoa chữ thường): 'S~*ject has Uber booked already'.\n",
    "+ Thứ tư thay thế 'AND' bằng ' & ' (không phân biệt chữ hoa chữ thường): .'Baked Beans & Spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S~*ject has ~*er booked already\n",
      "S~*ject has Uber booked already\n",
      "S~*ject has Uber booked already\n",
      "Baked Beans & Spam\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "print(re.sub('ub', '~*', 'Subject has Uber booked already',  \n",
    "             flags=re.IGNORECASE)) \n",
    "print(re.sub('ub', '~*', 'Subject has Uber booked already')) \n",
    "print(re.sub('ub', '~*', 'Subject has Uber booked already', \n",
    "             count=1, flags=re.IGNORECASE)) \n",
    "print(re.sub(r'\\sAND\\s', ' & ', 'Baked Beans And Spam',  \n",
    "             flags=re.IGNORECASE)) # Trong câu lệnh này r'\\sAND\\s' có thể ko cần dùng \\s, nhưng ' & ', phải xóa các dấu trống đi, để khi thay thế không bị dư dấu trống"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re.subn() \n",
    "subn() tương tự như sub() về mọi mặt, ngoại trừ cách cung cấp đầu ra. Nó trả về một bộ dữ liệu có tổng số thay thế và chuỗi mới thay vì chỉ chuỗi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ:\n",
    "\n",
    "re.subn() thay thế tất cả các lần xuất hiện của một mẫu trong chuỗi và trả về một bộ dữ liệu có chuỗi đã sửa đổi và số lần thay thế được thực hiện. Nó hữu ích cho cả sự thay thế phân biệt chữ hoa chữ thường và không phân biệt chữ hoa chữ thường."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('S~*ject has Uber booked already', 1)\n",
      "('S~*ject has ~*er booked already', 2)\n",
      "2\n",
      "S~*ject has ~*er booked already\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "  \n",
    "print(re.subn('ub', '~*', 'Subject has Uber booked already')) \n",
    "  \n",
    "t = re.subn('ub', '~*', 'Subject has Uber booked already', \n",
    "            flags=re.IGNORECASE) \n",
    "print(t) \n",
    "print(len(t)) \n",
    "print(t[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re.escape()\n",
    "Trả về chuỗi có tất cả các ký tự không phải chữ và số bị gạch chéo ngược, điều này rất hữu ích nếu bạn muốn khớp một chuỗi ký tự tùy ý có thể chứa các siêu ký tự biểu thức chính quy trong đó."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ:\n",
    "\n",
    "re.escape()được sử dụng để thoát các ký tự đặc biệt trong một chuỗi, giúp an toàn khi sử dụng làm mẫu trong biểu thức chính quy. Nó đảm bảo rằng bất kỳ ký tự nào có ý nghĩa đặc biệt trong biểu thức chính quy đều được coi là ký tự chữ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\\ is\\ Awesome\\ even\\ 1\\ AM\n",
      "I\\ Asked\\ what\\ is\\ \\\n",
      "\\ this\\ \\[a\\-9\\],\\ he\\ said\\ \\\t\\ \\^WoW\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "print(re.escape(\"This is Awesome even 1 AM\")) \n",
    "print(re.escape(\"I Asked what is \\n this [a-9], he said \\t ^WoW\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re.search()\n",
    "Phương thức này trả về Không có (nếu mẫu không khớp) hoặc re.MatchObject chứa thông tin về phần khớp của chuỗi. Phương pháp này dừng sau lần so khớp đầu tiên, vì vậy phương pháp này phù hợp nhất để kiểm tra biểu thức chính quy hơn là trích xuất dữ liệu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ: Tìm kiếm sự xuất hiện của mẫu\n",
    "\n",
    "Mã này sử dụng biểu thức chính quy để tìm kiếm mẫu trong chuỗi đã cho. Nếu tìm thấy kết quả khớp, nó sẽ trích xuất và in các phần phù hợp của chuỗi.\n",
    "\n",
    "Trong ví dụ cụ thể này, nó tìm kiếm một mẫu bao gồm một tháng (chữ cái) theo sau là một ngày (chữ số) trong chuỗi đầu vào “Tôi sinh ngày 24 tháng 6”. Nếu tìm thấy kết quả khớp, nó sẽ in kết quả khớp đầy đủ, tháng và ngày."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match at index 14, 21\n",
      "Full match: June 24\n",
      "Month: June\n",
      "Day: 24\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "regex = r\"([a-zA-Z]+) (\\d+)\"\n",
    "  \n",
    "match = re.search(regex, \"I was born on June 24\") \n",
    "if match != None: \n",
    "    print (\"Match at index %s, %s\" % (match.start(), match.end())) \n",
    "    print (\"Full match: %s\" % (match.group(0))) \n",
    "    print (\"Month: %s\" % (match.group(1))) \n",
    "    print (\"Day: %s\" % (match.group(2))) \n",
    "  \n",
    "else: \n",
    "    print (\"The regex pattern does not match.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the string and the regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(11, 12), match='G'>\n",
      "re.compile('\\\\bG')\n",
      "Welcome to GeeksForGeeks\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "s = \"Welcome to GeeksForGeeks\"\n",
    "res = re.search(r\"\\bG\", s) \n",
    "\n",
    "print(res)\n",
    "print(res.re) \n",
    "print(res.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Getting index of matched object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "14\n",
      "(11, 14)\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "  \n",
    "s = \"Welcome to GeeksForGeeks\"\n",
    "  \n",
    "res = re.search(r\"\\bGee\", s) \n",
    "  \n",
    "print(res.start()) \n",
    "print(res.end()) \n",
    "print(res.span())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Getting matched substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(5, 9), match='me t'>\n",
      "me t\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "s = \"Welcome to GeeksForGeeks\"\n",
    "res = re.search(r\"\\D{2} t\", s) \n",
    "print(res)\n",
    "print(res.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "    Word Embedding is a language modeling technique for mapping words to vectors of real numbers. It represents words or phrases in vector space with several dimensions. Word embeddings can be generated using various methods like neural networks, co-occurrence matrices, probabilistic models, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'any communicable material that is used to describe, explain or instruct regarding some attributes of an object, system or procedure'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"<b>any communicable material that is used to describe, explain or instruct regarding some attributes of an object, system or procedure</b>\"\n",
    "# Sử dụng chính quy hóa\n",
    "import re\n",
    "def stringhtml(data):\n",
    "    '''    \n",
    "        + r\"\": Ký tự r trước dấu ngoặc kép biến chuỗi thành \"raw string\" (chuỗi thô), giúp xử lý các ký tự đặc biệt bên trong chuỗi một cách chính xác mà không cần dùng ký tự escape (\\).\n",
    "        + <: Ký tự < tượng trưng cho việc bắt đầu một thẻ HTML.\n",
    "        + .*?: Dấu chấm . đại diện cho bất kỳ ký tự nào, và *? là một biểu thức lười biếng (non-greedy), có nghĩa là nó sẽ khớp với càng ít ký tự càng tốt cho đến khi gặp ký tự khớp tiếp theo.\n",
    "        + >: Ký tự > tượng trưng cho việc kết thúc một thẻ HTML.\n",
    "       =>  Do đó, r\"<.*?>\" có ý nghĩa là tìm và khớp với mọi thẻ HTML trong chuỗi, từ < đến >, bất kể nội dung bên trong thẻ là gì.\n",
    "    '''\n",
    "    \n",
    "    p = re.compile(r\"<.*?>\")\n",
    "    return p.sub(\"\",data)\n",
    "\n",
    "stringhtml(sample_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNIcode Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"\\nToday is a great day! \\xf0\\x9f\\x98\\x84\\nI finished my work early and can now write\\nRelax with a cup of coffee \\xe2\\x98\\x95\\xef\\xb8\\x8f. The weather is beautiful,\\nIt's sunny and warm \\xf0\\x9f\\x8c\\x9e. I'm so happy to be able to spend time with my family \\xf0\\x9f\\x91\\xa8\\xe2\\x80\\x8d\\xf0\\x9f\\x91\\xa9\\xe2\\x80\\x8d\\xf0\\x9f\\x91\\xa7\\xe2\\x80\\x8d\\xf0\\x9f\\x91\\xa6\\nand friends \\xf0\\x9f\\x91\\xaf\\xe2\\x80\\x8d\\xe2\\x99\\x82\\xef\\xb8\\x8f this weekend. Wishing everyone a wonderful day! \\xf0\\x9f\\x8c\\x9f\\n\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "Today is a great day! 😄\n",
    "I finished my work early and can now write\n",
    "Relax with a cup of coffee ☕️. The weather is beautiful,\n",
    "It's sunny and warm 🌞. I'm so happy to be able to spend time with my family 👨‍👩‍👧‍👦\n",
    "and friends 👯‍♂️ this weekend. Wishing everyone a wonderful day! 🌟\n",
    "\"\"\"\n",
    "\n",
    "sample_text.encode(\"utf-8\")\n",
    "\n",
    "# Những emoji trong văn bản bản chất vẫn là những chuỗi kí tự được mã hóa, cần xử lí phù hợp ở các trường hợp khác nhau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TextBlob' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m TextBlob\u001b[38;5;241m.\u001b[39mcorrect()  \u001b[38;5;66;03m# TextBlob(\"how are you?\")\u001b[39;00m\n\u001b[0;32m     13\u001b[0m incorrect_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhhow ar yor?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 15\u001b[0m TextBlob \u001b[38;5;241m=\u001b[39m \u001b[43mTextBlob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mincorrect_text\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     16\u001b[0m TextBlob\u001b[38;5;241m.\u001b[39mcorrect() \u001b[38;5;66;03m# TextBlob(\"how ar for?\")\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'TextBlob' object is not callable"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "#Với những từ tiếng Anh bị lặp kí tự nếu form đã hoàn chỉnh thì có thể dùng textblob đẻ khắc phục lỗi,\n",
    "# Nhưng thư viên này cũng còn rất hạn chế, không thể giải quyết hết được, có thể dùng cách sửa thủ công\n",
    "# hoặc dùng các thư viện có bộ ngôn ngữ từ vựng cấu trúc, câu lớn hơn để sửa => có một bộ data đẹp, vì việc \n",
    "# model học các câu sai ảnh hưởng lớn đến chất lượng.\n",
    "\n",
    "incorrect_text = \"hhow are yoou?\"\n",
    "\n",
    "TextBlob = TextBlob(incorrect_text) \n",
    "TextBlob.correct()  # TextBlob(\"how are you?\")\n",
    "\n",
    "incorrect_text = \"hhow ar yor?\"\n",
    "\n",
    "TextBlob = TextBlob(incorrect_text) \n",
    "TextBlob.correct() # TextBlob(\"how ar for?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# nltk là một thư viện xử lí ngon ngữ được xử dụng rất nhiều,\n",
    "#nó support hầu hết các hàm phan tích, xử lí data dạng văn bản.\n",
    "\n",
    "# sent_tokenize : Tách câu thành các phần tử của list\n",
    "# word_tokenize : tách chữ thành các phân tử của list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The most important person in the life of a person is his or her mother.',\n",
       " 'There is also a super character in our real lives and that is the mother.',\n",
       " 'So for me, that special character is my mom as she is a superwoman.',\n",
       " 'Regardless of day and night, my mother is always there for me.',\n",
       " 'Like everyone’s mother, my mother is a hard-working woman.',\n",
       " 'She worked day and night for my whole house.',\n",
       " 'I learn almost everything \\n    from my mother, as she’s my first teacher.',\n",
       " 'My mother taught me manners \\n    in everything, like manners of eating, manners of talking, and so on.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = \"\"\"The most important person in the life of a person is his or her mother. \n",
    "    There is also a super character in our real lives and that is the mother. \n",
    "    So for me, that special character is my mom as she is a superwoman. \n",
    "    Regardless of day and night, my mother is always there for me.\n",
    "\n",
    "    Like everyone’s mother, my mother is a hard-working woman. \n",
    "    She worked day and night for my whole house. I learn almost everything \n",
    "    from my mother, as she’s my first teacher. My mother taught me manners \n",
    "    in everything, like manners of eating, manners of talking, and so on.\n",
    "\"\"\"\n",
    "sents = sent_tokenize(dummy)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['most', 'important', 'person', 'in']\n",
      "['The', 'most', 'important', 'person', 'in', 'the', 'life', 'of', 'a', 'person', 'is', 'his', 'or', 'her', 'mother', '.']\n",
      "['There', 'is', 'also', 'a', 'super', 'character', 'in', 'our', 'real', 'lives', 'and', 'that', 'is', 'the', 'mother', '.']\n",
      "['So', 'for', 'me', ',', 'that', 'special', 'character', 'is', 'my', 'mom', 'as', 'she', 'is', 'a', 'superwoman', '.']\n",
      "['Regardless', 'of', 'day', 'and', 'night', ',', 'my', 'mother', 'is', 'always', 'there', 'for', 'me', '.']\n",
      "['Like', 'everyone', '’', 's', 'mother', ',', 'my', 'mother', 'is', 'a', 'hard-working', 'woman', '.']\n",
      "['She', 'worked', 'day', 'and', 'night', 'for', 'my', 'whole', 'house', '.']\n",
      "['I', 'learn', 'almost', 'everything', 'from', 'my', 'mother', ',', 'as', 'she', '’', 's', 'my', 'first', 'teacher', '.']\n",
      "['My', 'mother', 'taught', 'me', 'manners', 'in', 'everything', ',', 'like', 'manners', 'of', 'eating', ',', 'manners', 'of', 'talking', ',', 'and', 'so', 'on', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(dummy)\n",
    "print(words[1:5])  # ['most', 'important', 'person', 'in']\n",
    "\n",
    "#or \n",
    "for sent in sents:\n",
    "    print(word_tokenize(sent)) # phân tách thành từng chữ riêng lẻ trong từng câu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the most important person in the life of a person is his or her mother. \\n    there is also a super character in our real lives and that is the mother. \\n    so for me, that special character is my mom as she is a superwoman. \\n    regardless of day and night, my mother is always there for me.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Đưa bộ data về chung một kiểu viết -> viết thường lower\n",
    "\n",
    "sentence = \"\"\"The most important person in the life of a person is his or her mother. \n",
    "    There is also a super character in our real lives and that is the mother. \n",
    "    So for me, that special character is my mom as she is a superwoman. \n",
    "    Regardless of day and night, my mother is always there for me.\"\"\"\n",
    "    \n",
    "sentence.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check out my notebook '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_url(text):\n",
    "    \"\"\"\n",
    "    \n",
    "    r\"\": Ký tự r trước dấu ngoặc kép biến chuỗi thành \"raw string\" (chuỗi thô), giúp xử lý các ký tự đặc biệt bên trong chuỗi một cách chính xác mà không cần dùng ký tự escape (\\).\n",
    "\n",
    "    https?://\\S+: Đây là một phần của biểu thức chính quy và có ý nghĩa như sau:\n",
    "\n",
    "    http: Khớp chính xác với chuỗi http.\n",
    "    s?: Ký tự s có thể xuất hiện hoặc không (khớp với cả http và https).\n",
    "    ://: Khớp chính xác với chuỗi ://.\n",
    "    \\S+: Khớp với một hoặc nhiều ký tự không phải khoảng trắng. \\S đại diện cho bất kỳ ký tự nào không phải là khoảng trắng, và + chỉ ra rằng phải có ít nhất một ký tự không phải khoảng trắng.\n",
    "    |: Dấu gạch đứng (|) có nghĩa là \"hoặc\". Nó phân tách hai phần của biểu thức chính quy và cho phép khớp với một trong hai phần.\n",
    "\n",
    "    www\\.\\S+: Đây là phần còn lại của biểu thức chính quy và có ý nghĩa như sau:\n",
    "\n",
    "    www: Khớp chính xác với chuỗi www.\n",
    "    \\.: Khớp với ký tự dấu chấm (.). Dấu chấm là ký tự đặc biệt trong regex, nên cần phải có ký tự escape (\\) để khớp với dấu chấm thực tế.\n",
    "    \\S+: Khớp với một hoặc nhiều ký tự không phải khoảng trắng, tương tự như phần trước.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r\"\", text)\n",
    "\n",
    "text = \"Check out my notebook https://englishinsane.com/paragraph-on-my-mother\"\n",
    "remove_url(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import time\n",
    "string.punctuation   # Danh sách các punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo biến để dễ quản lí\n",
    "exclude = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How are you'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cách 1\n",
    "\n",
    "def remove_punc(text):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        text : string lúc chưa bỏ những punctuation\n",
    "\n",
    "    Returns:\n",
    "        text: string đã xử lí punctuatio\n",
    "    \"\"\"\n",
    "    \n",
    "    for char in exclude:\n",
    "        text = text.replace(char,\"\")\n",
    "    return text\n",
    "\n",
    "text = \"How are you?\"\n",
    "\n",
    "remove_punc(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How are you'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cách 2\n",
    "def remove_punc_v2(text):\n",
    "    \"\"\"\n",
    "    str.maketrans(\"\", \"\", exclude): Tạo một bảng dịch (translation table) mà sẽ được sử dụng bởi phương thức translate. Bảng dịch này xác định các ký tự nào sẽ bị thay thế hoặc loại bỏ.\n",
    "    Ba đối số trong str.maketrans là:\n",
    "    Đối số thứ nhất (\"\"): Xác định các ký tự sẽ được thay thế. Trong trường hợp này, không có ký tự nào được thay thế.\n",
    "    Đối số thứ hai (\"\"): Xác định các ký tự thay thế. Vì không có ký tự nào được thay thế, đối số này cũng là một chuỗi trống.\n",
    "    Đối số thứ ba (exclude): Xác định các ký tự sẽ bị loại bỏ. Biến exclude phải chứa tất cả các dấu câu cần loại bỏ.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return text.translate(str.maketrans(\"\",\"\",exclude))\n",
    "\n",
    "text = \"How are you?\"\n",
    "remove_punc(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Word\n",
    " Stop words là các từ phổ biến trong một ngôn ngữ mà thường không mang nhiều ý nghĩa ngữ nghĩa và được loại bỏ trong quá trình xử lý ngôn ngữ tự nhiên (NLP) và khai thác văn bản. Các từ này bao gồm các từ như \"a\", \"an\", \"the\", \"in\", \"on\", \"and\", \"is\", \"to\", \"of\" trong tiếng Anh, hoặc các từ tương tự trong các ngôn ngữ khác."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Thư viện này chỉ hỗ trợ stopword cho một số loại ngôn ngữ:\n",
    "'''\n",
    "available_langs = {\n",
    "        \"catalan\": \"ca\",\n",
    "        \"czech\": \"cs\",\n",
    "        \"german\": \"de\",\n",
    "        \"greek\": \"el\",\n",
    "        \"english\": \"en\",\n",
    "        \"spanish\": \"es\",\n",
    "        \"finnish\": \"fi\",\n",
    "        \"french\": \"fr\",\n",
    "        \"hungarian\": \"hu\",\n",
    "        \"icelandic\": \"is\",\n",
    "        \"italian\": \"it\",\n",
    "        \"latvian\": \"lv\",\n",
    "        \"dutch\": \"nl\",\n",
    "        \"polish\": \"pl\",\n",
    "        \"portuguese\": \"pt\",\n",
    "        \"romanian\": \"ro\",\n",
    "        \"russian\": \"ru\",\n",
    "        \"slovak\": \"sk\",\n",
    "        \"slovenian\": \"sl\",\n",
    "        \"swedish\": \"sv\",\n",
    "        \"tamil\": \"ta\",\n",
    "    }\n",
    "\n",
    "'''    \n",
    "\n",
    "STOP_WORDS = stopwords.words(\"english\")\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  hoan'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopword(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in STOP_WORDS:\n",
    "            new_text.append(\"\")\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)\n",
    "\n",
    "text = \"I am Hoan\"\n",
    "\n",
    "remove_stopword(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming --- Lemmatization\n",
    "<image src = \"https://media.licdn.com/dms/image/C4D12AQEZCHQOHXSEhg/article-cover_image-shrink_600_2000/0/1650689035153?e=2147483647&v=beta&t=uRsPEF-Apt9EvVTcUGR_ZhAs_Dk39de4MFQDo78LHos\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk walk walk walk\n",
      "chang chang chang chang changer\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "def stem_word(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])\n",
    "\n",
    "sample_1 = \"walk walks walking walked\"\n",
    "print(stem_word(sample_1))\n",
    "\n",
    "sample_2 = \"change changing changes changed changer\"\n",
    "print(stem_word(sample_2))  # ở đây changer data của hàm hiểu là người thay đổi, nó là một từ riêng biệt, không phải lưu theo kiểu biến thể của change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "the                 the                 \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "a                   a                   \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Đảm bảo các gói cần thiết đã được tải về\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at the same time. He has a bad habit of swimming after playing long hours in the Sun\"\n",
    "punctuation = string.punctuation\n",
    "\n",
    "# Tách các từ trong câu\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Loại bỏ dấu câu khỏi danh sách các từ\n",
    "sentence_words = [word for word in sentence_words if word not in punctuation]\n",
    "\n",
    "# Định dạng tiêu đề bảng\n",
    "print(f\"{'Word':<20}{'Lemma':<20}\")\n",
    "\n",
    "# Lemmatize từng từ trong danh sách và in kết quả\n",
    "for word in sentence_words:\n",
    "    print(f\"{word:<20}{wordnet_lemmatizer.lemmatize(word):<20}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization code chuẩn sử dụng nltk và spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    " \n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The quick brown foxes are jumping over the lazy dogs.\n",
      "Lemmatized Text: the quick brown fox be jump over the lazy dog .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    " \n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    " \n",
    "# Define a sample text\n",
    "text = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
    " \n",
    "# Process the text using spaCy\n",
    "doc = nlp(text)\n",
    " \n",
    "# Extract lemmatized tokens\n",
    "lemmatized_tokens = [token.lemma_ for token in doc]\n",
    " \n",
    "# Join the lemmatized tokens into a sentence\n",
    "lemmatized_text = ' '.join(lemmatized_tokens)\n",
    " \n",
    "# Print the original and lemmatized text\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Lemmatized Text:\", lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "    Word Embedding is a language modeling technique for mapping words to vectors of real numbers. It represents words or phrases in vector space with several dimensions. Word embeddings can be generated using various methods like neural networks, co-occurrence matrices, probabilistic models, etc.\n",
    "\n",
    "<div style=\"background-color: #fff; padding: 10px;\">\n",
    "    <img src=\"https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg\" alt=\"Linear Relationships\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src = \"https://www.analyticssteps.com/backend/media/uploads/2019/09/06/image-20190906164045-2.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Khởi tạo công cụ ánh xạ vecto của sklearn \n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ giả xử chúng ta có bộ data dạng bảng với 2 hàng, mục đích của chúng ta là biến 2 hàng này sang dạng vecto theo kiểu Bag of word vectors,\n",
    "+ Đầu tiên, chúng ta sẽ chuyển hết cả 2 hàng dưới này về cùng một bow, sau đó từ text này sẽ lấy ra những từ từ đọc lập duy nhất, mỗi từ sẽ gắn với một index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'documentation': 22, 'development': 19, 'may': 42, 'involve': 36, 'document': 21, 'drafting': 23, 'formatting': 27, 'submitting': 61, 'reviewing': 53, 'approving': 3, 'distributing': 20, 'reposting': 52, 'and': 2, 'tracking': 65, 'etc': 25, 'are': 4, 'convened': 15, 'by': 10, 'associated': 5, 'standard': 59, 'operating': 46, 'procedure': 48, 'in': 33, 'regulatory': 51, 'industry': 34, 'it': 38, 'could': 16, 'also': 1, 'creating': 17, 'content': 14, 'from': 28, 'scratch': 54, 'should': 57, 'be': 8, 'easy': 24, 'to': 63, 'read': 50, 'understand': 66, 'if': 31, 'is': 37, 'too': 64, 'long': 40, 'wordy': 69, 'misunderstood': 43, 'or': 47, 'ignored': 32, 'clear': 11, 'concise': 13, 'words': 68, 'used': 67, 'sentences': 55, 'limited': 39, 'maximum': 41, 'of': 45, '15': 0, 'intended': 35, 'for': 26, 'general': 30, 'audience': 6, 'avoid': 7, 'gender': 29, 'specific': 58, 'terms': 62, 'cultural': 18, 'biases': 9, 'series': 56, 'procedures': 49, 'steps': 60, 'clearly': 12, 'numbered': 44}\n",
      "\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "instance_1= '''Documentation development may involve document drafting, \n",
    "formatting, submitting, reviewing, approving, distributing, reposting and tracking, etc., \n",
    "and are convened by associated standard operating procedure in a regulatory industry. \n",
    "'''\n",
    "instance_2 = '''\n",
    "It could also involve creating content from scratch. Documentation should be easy to read and understand. \n",
    "If it is too long and too wordy, it may be misunderstood or ignored. Clear, concise words should be used, \n",
    "and sentences should be limited to a maximum of 15 words. Documentation intended for a general audience should \n",
    "avoid gender-specific terms and cultural biases. In a series of procedures, steps should be clearly numbered.\n",
    "'''\n",
    "bow = cv.fit_transform([instance_1,instance_2])\n",
    "print(cv.vocabulary_)\n",
    "print()\n",
    "print(len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 2 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0\n",
      "  1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0]]\n",
      "[[1 1 4 0 0 0 1 1 5 1 0 1 1 1 1 0 1 1 1 0 0 0 2 0 1 0 1 0 1 1 1 1 1 1 0 1\n",
      "  1 1 3 1 1 1 1 1 1 2 0 1 0 1 1 0 0 0 1 1 1 5 1 0 1 0 1 2 2 0 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "# chúng ta ánh xạ từng hàng instance_1,.. lên bow, với mỗi từ trong instance_1, ... xuât shieenj bao nhiêu lần trong bow thì sẽ đánh số vào vị trí\n",
    "# của vectow cho instance đó\n",
    "print(bow[0].toarray())\n",
    "print(bow[1].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# text thử với một vài data bất kì dùng bộ ánh xạ bow\n",
    "print(cv.transform([\"ha khai hoan\"]).toarray())\n",
    "print()\n",
    "print(cv.transform([\"Documentation development may Khai Hoan\"]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thay vì tạo ra vector với mỗi vị trí tương ứng với một chữ, thì \n",
    "# N-grams tạo vector với N chữ liên tiếp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src = \"https://www.kdnuggets.com/wp-content/uploads/agarwal_ngram_language_modeling_natural_language_processing_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', '<s>'), ('<s>', 'This'), ('This', 'is'), ('is', 'a'), ('a', 'sample'), ('sample', 'document'), ('document', 'This'), ('This', 'document'), ('document', 'contains'), ('contains', 'several'), ('several', 'sentences'), ('sentences', 'We'), ('We', 'can'), ('can', 'extract'), ('extract', 'bigrams'), ('bigrams', 'from'), ('from', 'it'), ('it', '<e>')]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "# punctuation data\n",
    "PUNC = string.punctuation\n",
    "\n",
    "def processing(document):\n",
    "    \"\"\"\n",
    "    Pre-process the document by removing punctuation and tokenizing.\n",
    "    \n",
    "    Args:\n",
    "        document: Raw document as a string.\n",
    "        \n",
    "    Returns:\n",
    "        A list of words (tokens).\n",
    "    \"\"\"\n",
    "    for punc in PUNC:\n",
    "        if punc in document:\n",
    "            document = document.replace(punc, \"\")\n",
    "    return nltk.word_tokenize(document)\n",
    "\n",
    "def extract_n_grams(data, n, start_token='<s>', end_token='<e>'):\n",
    "    \"\"\"\n",
    "    Extract all n-grams from the data.\n",
    "    \n",
    "    Args:\n",
    "        data: List of words (tokens).\n",
    "        n: Number of words in a sequence (n-gram size).\n",
    "    \n",
    "    Returns:\n",
    "        A list of n-grams.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_grams = []\n",
    "    \n",
    "    # Prepend start token n times, and append the end token one time\n",
    "    sentence = [start_token] * n + data + [end_token]\n",
    "    \n",
    "    # Convert list to tuple for processing n-grams\n",
    "    sentence = tuple(sentence)\n",
    "    \n",
    "    # Extract n-grams\n",
    "    for i in range(len(sentence) if n == 1 else len(sentence) - n + 1):\n",
    "        n_gram = sentence[i:i+n]\n",
    "        n_grams.append(n_gram)\n",
    "    \n",
    "    return n_grams\n",
    "\n",
    "# --------- Main ---------\n",
    "document = \"This is a sample document. This document contains several sentences. We can extract bigrams from it.\"\n",
    "\n",
    "# Extract bi-grams\n",
    "n_grams = extract_n_grams(processing(document), 2)\n",
    "print(n_grams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('<s>', '<s>'): 1, ('<s>', 'This'): 1, ('This', 'is'): 1, ('is', 'a'): 1, ('a', 'sample'): 1, ('sample', 'document'): 1, ('document', 'This'): 1, ('This', 'document'): 1, ('document', 'contains'): 1, ('contains', 'several'): 1, ('several', 'sentences'): 1, ('sentences', 'We'): 1, ('We', 'can'): 1, ('can', 'extract'): 1, ('extract', 'bigrams'): 1, ('bigrams', 'from'): 1, ('from', 'it'): 1, ('it', '<e>'): 1}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "# punctuation data\n",
    "PUNC = string.punctuation\n",
    "\n",
    "def processing(document):\n",
    "    \"\"\"\n",
    "    pre-processing document\n",
    "    \n",
    "    Args:\n",
    "        document: raw document\n",
    "        \n",
    "    Returns:\n",
    "        List of lists of words\n",
    "    \"\"\"\n",
    "    \n",
    "    for punc in PUNC:\n",
    "        if punc in document:\n",
    "            document = document.replace(punc,\"\")\n",
    "\n",
    "    return [nltk.word_tokenize(document)]\n",
    "\n",
    "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
    "    \"\"\"\n",
    "    Count all n-grams in the data\n",
    "    \n",
    "    Args:\n",
    "        data: List of lists of words\n",
    "        n: number of words in a sequence\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps a tuple of n-words to its frequency\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize dictionary of n-grams and their counts\n",
    "    n_grams = {}\n",
    "\n",
    "    # Go through each sentence in the data\n",
    "    for sentence in data: # complete this line\n",
    "        \n",
    "        # prepend start token n times, and  append the end token one time\n",
    "        sentence = [start_token] * n + sentence + [end_token]\n",
    "        \n",
    "        # convert list to tuple\n",
    "        # So that the sequence of words can be used as\n",
    "        # a key in the dictionary\n",
    "        sentence = tuple(sentence)\n",
    "        \n",
    "        # Use 'i' to indicate the start of the n-gram\n",
    "        # from index 0\n",
    "        # to the last index where the end of the n-gram\n",
    "        # is within the sentence.\n",
    "        \n",
    "        for i in range(len(sentence) if n==1 else len(sentence)-n+1): # complete this line\n",
    "\n",
    "            # Get the n-gram from i to i+n\n",
    "            n_gram = sentence[i:i+n]\n",
    "            \n",
    "            # check if the n-gram is in the dictionary\n",
    "            if n_gram in n_grams.keys(): # complete this line with the proper condition\n",
    "            \n",
    "                # Increment the count for this n-gram\n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                # Initialize this n-gram count to 1\n",
    "                n_grams[n_gram] = 1\n",
    "\n",
    "    return n_grams\n",
    "\n",
    "# --------- Main ---------\n",
    "document = \"This is a sample document. This document contains several sentences. We can extract bigrams from it.\"\n",
    "\n",
    "#Use n = 2: bi-grams\n",
    "print(count_n_grams(processing(document),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  CountVectorizer (N-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'documentation development may involve': 29, 'development may involve document': 26, 'may involve document drafting': 54, 'involve document drafting formatting': 45, 'document drafting formatting submitting': 28, 'drafting formatting submitting reviewing': 32, 'formatting submitting reviewing approving': 36, 'submitting reviewing approving distributing': 76, 'reviewing approving distributing reposting': 64, 'approving distributing reposting and': 8, 'distributing reposting and tracking': 27, 'reposting and tracking etc': 63, 'and tracking etc and': 6, 'tracking etc and are': 82, 'etc and are convened': 34, 'and are convened by': 2, 'are convened by associated': 9, 'convened by associated standard': 22, 'by associated standard operating': 18, 'associated standard operating procedure': 10, 'standard operating procedure in': 74, 'operating procedure in regulatory': 58, 'procedure in regulatory industry': 60, 'it could also involve': 47, 'could also involve creating': 23, 'also involve creating content': 1, 'involve creating content from': 44, 'creating content from scratch': 24, 'content from scratch documentation': 21, 'from scratch documentation should': 37, 'scratch documentation should be': 65, 'documentation should be easy': 31, 'should be easy to': 70, 'be easy to read': 13, 'easy to read and': 33, 'to read and understand': 79, 'read and understand if': 62, 'and understand if it': 7, 'understand if it is': 83, 'if it is too': 40, 'it is too long': 48, 'is too long and': 46, 'too long and too': 80, 'long and too wordy': 51, 'and too wordy it': 5, 'too wordy it may': 81, 'wordy it may be': 87, 'it may be misunderstood': 49, 'may be misunderstood or': 53, 'be misunderstood or ignored': 15, 'misunderstood or ignored clear': 55, 'or ignored clear concise': 59, 'ignored clear concise words': 41, 'clear concise words should': 19, 'concise words should be': 20, 'words should be used': 86, 'should be used and': 72, 'be used and sentences': 16, 'used and sentences should': 84, 'and sentences should be': 4, 'sentences should be limited': 66, 'should be limited to': 71, 'be limited to maximum': 14, 'limited to maximum of': 50, 'to maximum of 15': 78, 'maximum of 15 words': 52, 'of 15 words documentation': 56, '15 words documentation intended': 0, 'words documentation intended for': 85, 'documentation intended for general': 30, 'intended for general audience': 43, 'for general audience should': 35, 'general audience should avoid': 39, 'audience should avoid gender': 11, 'should avoid gender specific': 68, 'avoid gender specific terms': 12, 'gender specific terms and': 38, 'specific terms and cultural': 73, 'terms and cultural biases': 77, 'and cultural biases in': 3, 'cultural biases in series': 25, 'biases in series of': 17, 'in series of procedures': 42, 'series of procedures steps': 67, 'of procedures steps should': 57, 'procedures steps should be': 61, 'steps should be clearly': 75, 'should be clearly numbered': 69}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(4,4))\n",
    "bow = cv.fit_transform([instance_1,instance_2])\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0\n",
      "  1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0\n",
      "  0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0]]\n",
      "[[1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1\n",
      "  0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1\n",
      "  1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow[0].toarray())\n",
    "print(bow[1].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Viết tắt của thuật ngữ tiếng Anh term frequency – inverse document frequency, tf-idf là trọng số của một từ trong văn bản thu được qua thống kê thể hiện mức độ quan trọng của từ này trong một văn bản, mà bản thân văn bản đang xét nằm trong một tập hợp các văn bản.\n",
    "\n",
    "IDF - Inverse Document Frequency: dùng để ước lượng mức độ quan trọng của từ đó như thế nào . Khi tính tần số xuất hiện tf thì các từ đều được coi là quan trọng như nhau. Tuy nhiên có một số từ thường được được sử dụng nhiều nhưng không quan trọng để thể hiện ý nghĩa của đoạn văn.\n",
    "\n",
    "+ Từ nối: và, nhưng, tuy nhiên, vì thế, vì vậy, …\n",
    "+ Giới từ: ở, trong, trên, …\n",
    "+ Từ chỉ định: ấy, đó, nhỉ, …"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src = \"data:image/png;base64,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\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'the': 9, 'sky': 7, 'is': 4, 'blue': 0, 'sun': 8, 'bright': 1, 'in': 3, 'we': 10, 'can': 2, 'see': 5, 'shining': 6}\n",
      "TF-IDF matrix:\n",
      "[[0.65919112 0.         0.         0.         0.42075315 0.\n",
      "  0.         0.51971385 0.         0.34399327 0.        ]\n",
      " [0.         0.52210862 0.         0.         0.52210862 0.\n",
      "  0.         0.         0.52210862 0.42685801 0.        ]\n",
      " [0.         0.3218464  0.         0.50423458 0.3218464  0.\n",
      "  0.         0.39754433 0.3218464  0.52626104 0.        ]\n",
      " [0.         0.23910199 0.37459947 0.         0.         0.37459947\n",
      "  0.37459947 0.         0.47820398 0.39096309 0.37459947]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Bước 1: Tạo dữ liệu mẫu\n",
    "documents = [\n",
    "    \"The sky is blue.\",\n",
    "    \"The sun is bright.\",\n",
    "    \"The sun in the sky is bright.\",\n",
    "    \"We can see the shining sun, the bright sun.\"\n",
    "]\n",
    "\n",
    "# Bước 2: Khởi tạo TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Bước 3: Tính toán TF-IDF\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Bước 4: In kết quả\n",
    "# In từ điển (vocabulary)\n",
    "print(\"Vocabulary:\", vectorizer.vocabulary_)\n",
    "\n",
    "# In ma trận TF-IDF\n",
    "print(\"TF-IDF matrix:\")\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "        Word2Vec là một kỹ thuật deeplearning được sử dụng để tạo ra các vector từ (word embeddings) từ văn bản. Nó có thể giúp biểu diễn các từ trong một không gian vector sao cho các từ có ngữ nghĩa tương tự sẽ nằm gần nhau. Có hai mô hình chính trong Word2Vec: CBOW (Continuous Bag of Words) và Skip-gram.\n",
    "\n",
    "\n",
    "<image src = \"https://swimm.io/wp-content/webp-express/webp-images/uploads/2023/11/word2vec--1024x559.png.webp\">\n",
    "\n",
    "- CBOW\n",
    "\n",
    "        + CBOW nhằm dự đoán từ hiện tại dựa trên ngữ cảnh xung quanh (context). Nó cố gắng dự đoán từ trung tâm (center word) từ các từ xung quanh nó trong một cửa sổ ngữ cảnh đã chọn.\n",
    "\n",
    "        + Với CBOW, các từ ngữ cảnh (context words) được sử dụng để dự đoán từ trung tâm. Các từ ngữ cảnh này được tổng hợp thành một vector và đưa vào một mô hình neural network để dự đoán từ trung tâm.\n",
    "\n",
    "- Skip-gram\n",
    "\n",
    "        + Skip-gram ngược lại, nó cố gắng dự đoán các từ ngữ cảnh từ từ trung tâm.\n",
    "        \n",
    "        + Với Skip-gram, từ trung tâm được sử dụng để dự đoán các từ ngữ cảnh trong cùng một cửa sổ ngữ cảnh đã chọn. Mỗi từ ngữ cảnh được dự đoán như là một đầu ra của mô hình.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['love', 'machine', 'learning', 'and', 'natural', 'language', 'processing'], ['word', 'vec', 'is', 'technique', 'used', 'to', 'learn', 'word', 'embeddings', 'from', 'text'], ['deep', 'learning', 'models', 'like', 'word', 'vec', 'have', 'revolutionized', 'nlp']]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Data\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"I love machine learning and natural language processing\",\n",
    "    \"Word2Vec is a technique used to learn word embeddings from text\",\n",
    "    \"Deep learning models like Word2Vec have revolutionized NLP\"\n",
    "]\n",
    "\n",
    "# Biến đổi bộ data thành các câu gồm các token.\n",
    "tokenized_documents = [simple_preprocess(doc) for doc in documents]\n",
    "print(tokenized_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'machine':\n",
      " [ 7.7001113e-03  9.1245426e-03  1.1359043e-03 -8.3226906e-03\n",
      "  8.4260432e-03 -3.6943862e-03  5.7456936e-03  4.3893624e-03\n",
      "  9.6877608e-03 -9.2952261e-03  9.2084585e-03 -9.2804059e-03\n",
      " -6.9048870e-03 -9.1041336e-03 -5.5459035e-03  7.3677199e-03\n",
      "  9.1675948e-03 -3.3266528e-03  3.7216223e-03 -3.6291070e-03\n",
      "  7.8820419e-03  5.8674002e-03  4.7167180e-07 -3.6311133e-03\n",
      " -7.2259647e-03  4.7707637e-03  1.4498112e-03 -2.6142271e-03\n",
      "  7.8376699e-03 -4.0434687e-03 -9.1530606e-03 -2.2557892e-03\n",
      "  1.2613325e-04 -6.6421628e-03 -5.4879123e-03 -8.4960135e-03\n",
      "  9.2218863e-03  7.4243932e-03 -2.9611582e-04  7.3635504e-03\n",
      "  7.9529136e-03 -7.8370131e-04  6.6126566e-03  3.7732164e-03\n",
      "  5.0779628e-03  7.2530461e-03 -4.7419146e-03 -2.1884560e-03\n",
      "  8.7490311e-04  4.2368053e-03  3.3014184e-03  5.0956826e-03\n",
      "  4.5861360e-03 -8.4437206e-03 -3.1825160e-03 -7.2370791e-03\n",
      "  9.6786767e-03  5.0060940e-03  1.7117348e-04  4.1131866e-03\n",
      " -7.6562283e-03 -6.2979194e-03  3.0745452e-03  6.5360777e-03\n",
      "  3.9518555e-03  6.0205087e-03 -1.9859145e-03 -3.3402655e-03\n",
      "  2.0474885e-04 -3.1980751e-03 -5.5169486e-03 -7.7880397e-03\n",
      "  6.5373844e-03 -1.0890692e-03 -1.8930781e-03 -7.8033716e-03\n",
      "  9.3401987e-03  8.6947152e-04  1.7717265e-03  2.4899272e-03\n",
      " -7.3899282e-03  1.6395028e-03  2.9778772e-03 -8.5680820e-03\n",
      "  4.9596955e-03  2.4320416e-03  7.5027482e-03  5.0445190e-03\n",
      " -3.0294782e-03 -7.1676848e-03  7.0995265e-03  1.9028226e-03\n",
      "  5.1974775e-03  6.3840565e-03  1.9099785e-03 -6.1304872e-03\n",
      " -1.0701035e-05  8.2700634e-03 -6.0969014e-03  9.4400095e-03]\n"
     ]
    }
   ],
   "source": [
    "# Đào tạo mô hình Word2Vec\n",
    "model = Word2Vec(sentences=tokenized_documents, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "\"\"\"\n",
    "    Mặc định của model là dùng C-bow\n",
    "    ----sg : {0, 1}, optional\n",
    "            Training algorithm: 1 for skip-gram; otherwise CBOW.------\n",
    "    # Sử dụng Skip-gram\n",
    "    model = Word2Vec(sentences=tokenized_documents, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "    # Hoặc sử dụng CBOW\n",
    "    model = Word2Vec(sentences=tokenized_documents, vector_size=100, window=5, min_count=1, workers=4, sg=0, cbow_mean=1)\n",
    "\n",
    "    vector_size: \n",
    "        + Mô tả: Số chiều của vector từ (word embeddings). Giá trị này quyết định độ dài của mỗi vector từ được tạo ra bởi mô hình.\n",
    "        + Ví dụ: 100 có nghĩa là mỗi từ sẽ được biểu diễn bởi một vector có 100 chiều.\n",
    "\n",
    "    window:\n",
    "        + Mô tả: Kích thước của cửa sổ ngữ cảnh, nghĩa là số lượng từ xung quanh một từ mục tiêu được sử dụng để dự đoán từ đó. Giá trị này ảnh hưởng đến phạm vi ngữ cảnh mà mô hình sẽ xem xét.\n",
    "        + Ví dụ: 5 có nghĩa là mô hình sẽ xem xét 5 từ trước và 5 từ sau từ mục tiêu trong câu.\n",
    "    \n",
    "    min_count:\n",
    "\n",
    "        + Mô tả: Số lần xuất hiện tối thiểu của một từ trong toàn bộ tập dữ liệu để từ đó được đưa vào mô hình. Các từ xuất hiện ít hơn giá trị này sẽ bị bỏ qua.\n",
    "        + Ví dụ: 1 có nghĩa là tất cả các từ xuất hiện ít nhất một lần sẽ được đưa vào mô\n",
    "    \n",
    "    workers:\n",
    "        + Mô tả: Số lượng luồng xử lý (threads) được sử dụng để đào tạo mô hình. Điều này có thể giúp tăng tốc quá trình đào tạo bằng cách sử dụng nhiều luồng xử lý song song.\n",
    "        + Ví dụ: 4 có nghĩa là sử dụng 4 luồng xử lý để đào tạo mô hình.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "# Lấy vector của một từ\n",
    "word_vector = model.wv['machine']\n",
    "print(\"Vector for 'machine':\\n\", word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'machine':\n",
      "language: 0.2468\n",
      "the: 0.1784\n",
      "technique: 0.1713\n",
      "processing: 0.1624\n",
      "vec: 0.1607\n"
     ]
    }
   ],
   "source": [
    "# Tìm các từ tương tự\n",
    "similar_words = model.wv.most_similar('machine', topn=5)\n",
    "print(\"Words similar to 'machine':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '.', 'This', 'is', 'an', 'NLTK', 'example', '.']\n",
      "['Hello world.', 'This is an NLTK example.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"Hello world. This is an NLTK example.\"\n",
    "words = nltk.word_tokenize(text)\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(words)    # ['Hello', 'world', '.', 'This', 'is', 'an', 'NLTK', 'example', '.']\n",
    "print(sentences) # ['Hello world.', 'This is an NLTK example.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"needn't\", 'under', 'through', 'haven', 'and', 'into', \"shouldn't\", 'won', 'herself', 'no', 'after', \"don't\", 'him', 'not', 'now', \"wasn't\", 'll', 'nor', 'about', 'such', 'from', \"you're\", 'yourselves', 'out', 'on', 'my', 'isn', 'but', 'did', 'of', \"hasn't\", 'was', 'm', 'i', 'why', 'shan', 'it', 'over', 'them', 'or', 'hadn', 'in', 'its', \"shan't\", 'because', 'y', 'am', 'below', 'don', 'between', 'what', 'at', 'had', 'few', \"you've\", 'other', \"wouldn't\", 'then', 'themselves', \"that'll\", 'wouldn', \"you'll\", 'will', \"won't\", \"didn't\", 'any', 'here', 're', 'needn', \"haven't\", 'by', 'down', 'wasn', 'theirs', 'is', 'who', 'those', 'having', 'ain', 'mustn', 'himself', 'during', 'doing', 'hasn', 'their', 'has', 'above', 'more', 'd', 'there', 'against', \"aren't\", 'an', \"doesn't\", 'this', 'where', 'so', 'each', 'these', 'can', 'for', 'aren', 'if', 'off', 't', \"she's\", 'we', 'up', 'myself', 'mightn', 'me', 'a', \"should've\", 'most', 'he', 'have', 'once', 'doesn', 'your', 'she', 'until', 'again', 'ours', 'both', 'weren', 'our', 'all', 'than', 'yours', 'the', 'only', 'couldn', \"you'd\", 'are', 'didn', 'being', 'ma', 'which', 'same', 'shouldn', 'as', 'when', 'they', \"mightn't\", \"couldn't\", 'her', 'itself', 'be', \"weren't\", 'some', 'to', 'own', 'yourself', 'very', 'his', 've', 'while', \"isn't\", 'should', 'were', 'hers', 'o', 'how', 'ourselves', 'before', \"mustn't\", 'do', 'further', 'you', 'does', 'been', \"hadn't\", 'that', 's', 'with', 'whom', 'too', \"it's\", 'just'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\"\"\"\n",
    "    available_langs = {\n",
    "        \"catalan\": \"ca\",\n",
    "        \"czech\": \"cs\",\n",
    "        \"german\": \"de\",\n",
    "        \"greek\": \"el\",\n",
    "        \"english\": \"en\",\n",
    "        \"spanish\": \"es\",\n",
    "        \"finnish\": \"fi\",\n",
    "        \"french\": \"fr\",\n",
    "        \"hungarian\": \"hu\",\n",
    "        \"icelandic\": \"is\",\n",
    "        \"italian\": \"it\",\n",
    "        \"latvian\": \"lv\",\n",
    "        \"dutch\": \"nl\",\n",
    "        \"polish\": \"pl\",\n",
    "        \"portuguese\": \"pt\",\n",
    "        \"romanian\": \"ro\",\n",
    "        \"russian\": \"ru\",\n",
    "        \"slovak\": \"sk\",\n",
    "        \"slovenian\": \"sl\",\n",
    "        \"swedish\": \"sv\",\n",
    "        \"tamil\": \"ta\",\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    "print(stop_words) # {'a', 'an', 'the', ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "fli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('running'))  # run\n",
    "print(stemmer.stem('flies'))    # fli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "fly\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('running', pos='v'))  # run\n",
    "print(lemmatizer.lemmatize('flies', pos='n'))    # fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Part-of-Speech Tagging (POS Tagging)\n",
    "\n",
    "Part-of-Speech Tagging (POS Tagging) là một kỹ thuật trong xử lý ngôn ngữ tự nhiên (NLP) dùng để gán nhãn từ loại cho các từ trong một văn bản. Mỗi từ trong câu sẽ được gán một nhãn tương ứng với loại từ của nó, chẳng hạn như danh từ (noun), động từ (verb), tính từ (adjective), trạng từ (adverb), v.v.\n",
    "\n",
    "<image src = \"https://images.shiksha.com/mediadata/ugcDocuments/images/wordpressImages/2022_12_POS-Tagging.jpg\">\n",
    "\n",
    "`Mục đích của POS Tagging`\n",
    "+ Xác định vai trò từ trong câu: Giúp hiểu cách các từ kết hợp với nhau để tạo thành câu có nghĩa.\n",
    "+ Cải thiện các tác vụ NLP khác: Cung cấp thông tin về cấu trúc câu giúp cải thiện các tác vụ khác như phân tích cú pháp, nhận diện thực thể, và dịch máy.\n",
    "\n",
    "`Các nhãn từ loại phổ biến`\n",
    "+ `NN` (Noun, singular or mass): Danh từ số ít hoặc danh từ không đếm được.\n",
    "+ `NNS` (Noun, plural): Danh từ số nhiều.\n",
    "+ `VB` (Verb, base form): Động từ ở dạng cơ bản.\n",
    "+ `VBD` (Verb, past tense): Động từ thì quá khứ.\n",
    "+ `VBG` (Verb, gerund/present participle): Động từ dạng hiện tại.\n",
    "+ `VBN` (Verb, past participle): Động từ dạng quá khứ phân từ.\n",
    "+ `JJ` (Adjective): Tính từ.\n",
    "+ `RB` (Adverb): Trạng từ.\n",
    "+ `PRP` (Personal pronoun): Đại từ cá nhân (ví dụ: he, she, it).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('simple', 'JJ'), ('example', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "text = \"This is a simple example.\"\n",
    "tokens = word_tokenize(text)\n",
    "tagged = pos_tag(tokens)\n",
    "print(tagged)  # [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('simple', 'JJ'), ('example', 'NN')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Named Entity Recognition (NER)\n",
    "\n",
    "Named Entity Recognition (NER) là một kỹ thuật trong xử lý ngôn ngữ tự nhiên (NLP) dùng để nhận diện các thực thể cụ thể trong văn bản, chẳng hạn như tên người, địa điểm, tổ chức, và các thực thể khác.\n",
    "\n",
    "<image src = \"https://www.shaip.com/wp-content/uploads/2022/02/Blog_Named-Entity-Recognition-%E2%80%93-The-Concept-Types-Applications.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  (GPE Hawaii/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "text = \"Barack Obama was born in Hawaii.\"\n",
    "tokens = word_tokenize(text)\n",
    "tagged = pos_tag(tokens)\n",
    "entities = ne_chunk(tagged)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('apple', 3), ('banana', 2), ('orange', 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "text = \"apple banana apple apple orange banana\"\n",
    "tokens = word_tokenize(text)\n",
    "fdist = FreqDist(tokens)\n",
    "print(fdist.most_common())  # [('apple', 3), ('banana', 2), ('orange', 1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Collocations\n",
    "\n",
    "Collocations là các cặp từ thường xuất hiện cùng nhau trong văn bản, và việc nhận diện chúng có thể giúp hiểu sâu hơn về ngữ nghĩa và cấu trúc của văn bản."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'like'), ('and', 'basketball'), ('basketball', '.'), ('football', 'and'), ('like', 'to'), ('play', 'football'), ('to', 'play')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "text = \"I like to play football and basketball.\"\n",
    "tokens = word_tokenize(text)\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "bigrams = finder.nbest(BigramAssocMeasures.likelihood_ratio, 10)\n",
    "print(bigrams)  # [('', 'football'), ('football', 'and'), ...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Parsing\n",
    "\n",
    "Parsing, trong ngữ nghĩa của xử lý ngôn ngữ tự nhiên (NLP) và khoa học máy tính, là quá trình phân tích một chuỗi từ (hoặc câu) để xác định cấu trúc ngữ pháp của nó theo một ngữ pháp cụ thể. Mục tiêu của parsing là hiểu cách các thành phần của câu kết hợp với nhau để tạo thành một câu hợp lệ.\n",
    "\n",
    "`Ngữ pháp (Grammar): Là một tập hợp các quy tắc xác định cách các từ và cụm từ có thể kết hợp để tạo thành câu hợp lệ. Trong ngữ pháp Context-Free Grammar (CFG), các quy tắc được định nghĩa dưới dạng các quy tắc sản sinh (production rules).`\n",
    "\n",
    "```bash\n",
    "S -> NP VP\n",
    "NP -> DT NN\n",
    "VP -> V NP\n",
    "DT -> 'the' | 'a'\n",
    "NN -> 'man' | 'ball'\n",
    "V -> 'hit'\n",
    "```\n",
    "\n",
    "+ S là câu.\n",
    "+ NP là cụm danh từ.\n",
    "+ VP là cụm động từ.\n",
    "+ DT là từ định nghĩa (article).\n",
    "+ NN là danh từ.\n",
    "+ V là động từ.\n",
    "\n",
    "`Cây Phân Tích (Parse Tree): Là cấu trúc phân cấp hiển thị cách các từ trong câu được kết hợp với nhau theo các quy tắc ngữ pháp. Nó cho thấy các thành phần câu (như cụm danh từ, cụm động từ) và cách chúng liên kết với nhau.`\n",
    "\n",
    "```bash\n",
    "(S\n",
    "  (NP (DT the) (NN man))\n",
    "  (VP (V hit) (NP (DT the) (NN ball))))\n",
    "```\n",
    "\n",
    "`Phân Tích (Parsing): Quá trình xác định cấu trúc ngữ pháp của một câu bằng cách áp dụng các quy tắc ngữ pháp để tạo ra cây phân tích.` Có nhiều phương pháp parsing khác nhau, bao gồm:\n",
    "\n",
    "+ Top-Down Parsing: Bắt đầu từ nút gốc (như S) và cố gắng phân tích từ trên xuống dưới bằng cách áp dụng các quy tắc ngữ pháp.\n",
    "+ Bottom-Up Parsing: Bắt đầu từ các từ trong câu và cố gắng xây dựng cây phân tích từ dưới lên bằng cách kết hợp các thành phần nhỏ thành các thành phần lớn hơn.\n",
    "+ Chart Parsing: Sử dụng cấu trúc dữ liệu gọi là chart để lưu trữ và tái sử dụng các phân tích trung gian, giúp xử lý hiệu quả hơn.\n",
    "+ ChartParser trong NLTK: NLTK (Natural Language Toolkit) cung cấp một công cụ parsing mạnh mẽ gọi là ChartParser để thực hiện parsing dựa trên ngữ pháp CFG.\n",
    "\n",
    "`ChartParser: Xây dựng một cấu trúc dữ liệu để lưu trữ các phân tích trung gian và tránh việc phân tích lại cùng một phần của câu. Điều này giúp cải thiện hiệu suất và xử lý các câu phức tạp hơn.`\n",
    "+ ChartParser: Xây dựng một cấu trúc dữ liệu để lưu trữ các phân tích trung gian và tránh việc phân tích lại cùng một phần của câu. Điều này giúp cải thiện hiệu suất và xử lý các câu phức tạp hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (DT the) (NN man)) (VP (V hit) (NP (DT the) (NN ball))))\n"
     ]
    }
   ],
   "source": [
    "from nltk import CFG, ChartParser\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  NP -> DT NN\n",
    "  VP -> V NP\n",
    "  DT -> 'the' | 'a'\n",
    "  NN -> 'man' | 'ball'\n",
    "  V -> 'hit'\n",
    "\"\"\")\n",
    "parser = ChartParser(grammar)\n",
    "sentence = 'the man hit the ball'.split()\n",
    "for tree in parser.parse(sentence):\n",
    "    print(tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. WordNet Interface\n",
    "\n",
    "+ WordNet là một cơ sở dữ liệu từ vựng mạnh mẽ cho tiếng Anh, được sử dụng rộng rãi trong xử lý ngôn ngữ tự nhiên để tra cứu từ đồng nghĩa, từ trái nghĩa, và các mối quan hệ ngữ nghĩa khác. Thư viện NLTK cung cấp một giao diện để làm việc với WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('good.n.01'), Synset('good.n.02'), Synset('good.n.03'), Synset('commodity.n.01'), Synset('good.a.01'), Synset('full.s.06'), Synset('good.a.03'), Synset('estimable.s.02'), Synset('beneficial.s.01'), Synset('good.s.06'), Synset('good.s.07'), Synset('adept.s.01'), Synset('good.s.09'), Synset('dear.s.02'), Synset('dependable.s.04'), Synset('good.s.12'), Synset('good.s.13'), Synset('effective.s.04'), Synset('good.s.15'), Synset('good.s.16'), Synset('good.s.17'), Synset('good.s.18'), Synset('good.s.19'), Synset('good.s.20'), Synset('good.s.21'), Synset('well.r.01'), Synset('thoroughly.r.02')]\n",
      "[Lemma('good.n.01.good')]\n",
      "benefit\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonyms = wordnet.synsets('good')\n",
    "print(synonyms)  # [Synset('good.n.01'), Synset('good.a.01'), ...]\n",
    "print(synonyms[0].lemmas())  # [Lemma('good.n.01.good'), ...]\n",
    "print(synonyms[0].definition())  # 'that which is good'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
